<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>blog</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2021-10-25T09:27:13.136Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>祎熵</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>pytorch深度学习一</title>
    <link href="http://example.com/post/d9099bcf.html"/>
    <id>http://example.com/post/d9099bcf.html</id>
    <published>2021-10-25T07:00:58.000Z</published>
    <updated>2021-10-25T09:27:13.136Z</updated>
    
    <content type="html"><![CDATA[<h1 id="pytorch"><a href="#pytorch" class="headerlink" title="pytorch"></a>pytorch</h1><h3 id="1、线性模型"><a href="#1、线性模型" class="headerlink" title="1、线性模型"></a>1、线性模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x * w</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    y_pred = forward(x)</span><br><span class="line">    <span class="keyword">return</span> (y_pred - y) * (y_pred - y)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 保存权重</span></span><br><span class="line">w_list = []</span><br><span class="line"><span class="comment"># 保存权重的损失函数值</span></span><br><span class="line">loss_list = []</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 损失函数MSE</span></span><br><span class="line"><span class="keyword">for</span> w <span class="keyword">in</span> np.arange(<span class="number">0.0</span>, <span class="number">4.1</span>, <span class="number">0.1</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;w = &#x27;</span>, w)</span><br><span class="line">    loss_sum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x_val, y_val <span class="keyword">in</span> <span class="built_in">zip</span>(x_data, y_data):</span><br><span class="line">        <span class="comment"># 为了打印y预测值，其实loss里也计算了</span></span><br><span class="line">        y_pred_val = forward(x_val)</span><br><span class="line">        loss_val = loss(x_val, y_val)</span><br><span class="line">        loss_sum += loss_val</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;\t&#x27;</span>, x_val, y_val,</span><br><span class="line">              y_pred_val, loss_val)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;loss = &#x27;</span>, loss_sum / <span class="number">3</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;☆&#x27;</span>*<span class="number">55</span>)</span><br><span class="line">    w_list.append(w)</span><br><span class="line">    loss_list.append(loss_sum / <span class="number">3</span>)</span><br></pre></td></tr></table></figure><p>结果：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">w =  0.0</span><br><span class="line"> 1.0 2.0 0.0 4.0</span><br><span class="line"> 2.0 4.0 0.0 16.0</span><br><span class="line"> 3.0 6.0 0.0 36.0</span><br><span class="line">loss =  18.666666666666668</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  0.1</span><br><span class="line"> 1.0 2.0 0.1 3.61</span><br><span class="line"> 2.0 4.0 0.2 14.44</span><br><span class="line"> 3.0 6.0 0.30000000000000004 32.49</span><br><span class="line">loss =  16.846666666666668</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  0.2</span><br><span class="line"> 1.0 2.0 0.2 3.24</span><br><span class="line"> 2.0 4.0 0.4 12.96</span><br><span class="line"> 3.0 6.0 0.6000000000000001 29.160000000000004</span><br><span class="line">loss =  15.120000000000003</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  0.30000000000000004</span><br><span class="line"> 1.0 2.0 0.30000000000000004 2.8899999999999997</span><br><span class="line"> 2.0 4.0 0.6000000000000001 11.559999999999999</span><br><span class="line"> 3.0 6.0 0.9000000000000001 26.009999999999998</span><br><span class="line">loss =  13.486666666666665</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  0.4</span><br><span class="line"> 1.0 2.0 0.4 2.5600000000000005</span><br><span class="line"> 2.0 4.0 0.8 10.240000000000002</span><br><span class="line"> 3.0 6.0 1.2000000000000002 23.04</span><br><span class="line">loss =  11.946666666666667</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  0.5</span><br><span class="line"> 1.0 2.0 0.5 2.25</span><br><span class="line"> 2.0 4.0 1.0 9.0</span><br><span class="line"> 3.0 6.0 1.5 20.25</span><br><span class="line">loss =  10.5</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  0.6000000000000001</span><br><span class="line"> 1.0 2.0 0.6000000000000001 1.9599999999999997</span><br><span class="line"> 2.0 4.0 1.2000000000000002 7.839999999999999</span><br><span class="line"> 3.0 6.0 1.8000000000000003 17.639999999999993</span><br><span class="line">loss =  9.146666666666663</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  0.7000000000000001</span><br><span class="line"> 1.0 2.0 0.7000000000000001 1.6899999999999995</span><br><span class="line"> 2.0 4.0 1.4000000000000001 6.759999999999998</span><br><span class="line"> 3.0 6.0 2.1 15.209999999999999</span><br><span class="line">loss =  7.886666666666666</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  0.8</span><br><span class="line"> 1.0 2.0 0.8 1.44</span><br><span class="line"> 2.0 4.0 1.6 5.76</span><br><span class="line"> 3.0 6.0 2.4000000000000004 12.959999999999997</span><br><span class="line">loss =  6.719999999999999</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  0.9</span><br><span class="line"> 1.0 2.0 0.9 1.2100000000000002</span><br><span class="line"> 2.0 4.0 1.8 4.840000000000001</span><br><span class="line"> 3.0 6.0 2.7 10.889999999999999</span><br><span class="line">loss =  5.646666666666666</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  1.0</span><br><span class="line"> 1.0 2.0 1.0 1.0</span><br><span class="line"> 2.0 4.0 2.0 4.0</span><br><span class="line"> 3.0 6.0 3.0 9.0</span><br><span class="line">loss =  4.666666666666667</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  1.1</span><br><span class="line"> 1.0 2.0 1.1 0.8099999999999998</span><br><span class="line"> 2.0 4.0 2.2 3.2399999999999993</span><br><span class="line"> 3.0 6.0 3.3000000000000003 7.289999999999998</span><br><span class="line">loss =  3.779999999999999</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  1.2000000000000002</span><br><span class="line"> 1.0 2.0 1.2000000000000002 0.6399999999999997</span><br><span class="line"> 2.0 4.0 2.4000000000000004 2.5599999999999987</span><br><span class="line"> 3.0 6.0 3.6000000000000005 5.759999999999997</span><br><span class="line">loss =  2.986666666666665</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  1.3</span><br><span class="line"> 1.0 2.0 1.3 0.48999999999999994</span><br><span class="line"> 2.0 4.0 2.6 1.9599999999999997</span><br><span class="line"> 3.0 6.0 3.9000000000000004 4.409999999999998</span><br><span class="line">loss =  2.2866666666666657</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  1.4000000000000001</span><br><span class="line"> 1.0 2.0 1.4000000000000001 0.3599999999999998</span><br><span class="line"> 2.0 4.0 2.8000000000000003 1.4399999999999993</span><br><span class="line"> 3.0 6.0 4.2 3.2399999999999993</span><br><span class="line">loss =  1.6799999999999995</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  1.5</span><br><span class="line"> 1.0 2.0 1.5 0.25</span><br><span class="line"> 2.0 4.0 3.0 1.0</span><br><span class="line"> 3.0 6.0 4.5 2.25</span><br><span class="line">loss =  1.1666666666666667</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  1.6</span><br><span class="line"> 1.0 2.0 1.6 0.15999999999999992</span><br><span class="line"> 2.0 4.0 3.2 0.6399999999999997</span><br><span class="line"> 3.0 6.0 4.800000000000001 1.4399999999999984</span><br><span class="line">loss =  0.746666666666666</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  1.7000000000000002</span><br><span class="line"> 1.0 2.0 1.7000000000000002 0.0899999999999999</span><br><span class="line"> 2.0 4.0 3.4000000000000004 0.3599999999999996</span><br><span class="line"> 3.0 6.0 5.1000000000000005 0.809999999999999</span><br><span class="line">loss =  0.4199999999999995</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  1.8</span><br><span class="line"> 1.0 2.0 1.8 0.03999999999999998</span><br><span class="line"> 2.0 4.0 3.6 0.15999999999999992</span><br><span class="line"> 3.0 6.0 5.4 0.3599999999999996</span><br><span class="line">loss =  0.1866666666666665</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  1.9000000000000001</span><br><span class="line"> 1.0 2.0 1.9000000000000001 0.009999999999999974</span><br><span class="line"> 2.0 4.0 3.8000000000000003 0.0399999999999999</span><br><span class="line"> 3.0 6.0 5.7 0.0899999999999999</span><br><span class="line">loss =  0.046666666666666586</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  2.0</span><br><span class="line"> 1.0 2.0 2.0 0.0</span><br><span class="line"> 2.0 4.0 4.0 0.0</span><br><span class="line"> 3.0 6.0 6.0 0.0</span><br><span class="line">loss =  0.0</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  2.1</span><br><span class="line"> 1.0 2.0 2.1 0.010000000000000018</span><br><span class="line"> 2.0 4.0 4.2 0.04000000000000007</span><br><span class="line"> 3.0 6.0 6.300000000000001 0.09000000000000043</span><br><span class="line">loss =  0.046666666666666835</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  2.2</span><br><span class="line"> 1.0 2.0 2.2 0.04000000000000007</span><br><span class="line"> 2.0 4.0 4.4 0.16000000000000028</span><br><span class="line"> 3.0 6.0 6.6000000000000005 0.36000000000000065</span><br><span class="line">loss =  0.18666666666666698</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  2.3000000000000003</span><br><span class="line"> 1.0 2.0 2.3000000000000003 0.09000000000000016</span><br><span class="line"> 2.0 4.0 4.6000000000000005 0.36000000000000065</span><br><span class="line"> 3.0 6.0 6.9 0.8100000000000006</span><br><span class="line">loss =  0.42000000000000054</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  2.4000000000000004</span><br><span class="line"> 1.0 2.0 2.4000000000000004 0.16000000000000028</span><br><span class="line"> 2.0 4.0 4.800000000000001 0.6400000000000011</span><br><span class="line"> 3.0 6.0 7.200000000000001 1.4400000000000026</span><br><span class="line">loss =  0.7466666666666679</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  2.5</span><br><span class="line"> 1.0 2.0 2.5 0.25</span><br><span class="line"> 2.0 4.0 5.0 1.0</span><br><span class="line"> 3.0 6.0 7.5 2.25</span><br><span class="line">loss =  1.1666666666666667</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  2.6</span><br><span class="line"> 1.0 2.0 2.6 0.3600000000000001</span><br><span class="line"> 2.0 4.0 5.2 1.4400000000000004</span><br><span class="line"> 3.0 6.0 7.800000000000001 3.2400000000000024</span><br><span class="line">loss =  1.6800000000000008</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  2.7</span><br><span class="line"> 1.0 2.0 2.7 0.49000000000000027</span><br><span class="line"> 2.0 4.0 5.4 1.960000000000001</span><br><span class="line"> 3.0 6.0 8.100000000000001 4.410000000000006</span><br><span class="line">loss =  2.2866666666666693</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  2.8000000000000003</span><br><span class="line"> 1.0 2.0 2.8000000000000003 0.6400000000000005</span><br><span class="line"> 2.0 4.0 5.6000000000000005 2.560000000000002</span><br><span class="line"> 3.0 6.0 8.4 5.760000000000002</span><br><span class="line">loss =  2.986666666666668</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  2.9000000000000004</span><br><span class="line"> 1.0 2.0 2.9000000000000004 0.8100000000000006</span><br><span class="line"> 2.0 4.0 5.800000000000001 3.2400000000000024</span><br><span class="line"> 3.0 6.0 8.700000000000001 7.290000000000005</span><br><span class="line">loss =  3.780000000000003</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  3.0</span><br><span class="line"> 1.0 2.0 3.0 1.0</span><br><span class="line"> 2.0 4.0 6.0 4.0</span><br><span class="line"> 3.0 6.0 9.0 9.0</span><br><span class="line">loss =  4.666666666666667</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  3.1</span><br><span class="line"> 1.0 2.0 3.1 1.2100000000000002</span><br><span class="line"> 2.0 4.0 6.2 4.840000000000001</span><br><span class="line"> 3.0 6.0 9.3 10.890000000000004</span><br><span class="line">loss =  5.646666666666668</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  3.2</span><br><span class="line"> 1.0 2.0 3.2 1.4400000000000004</span><br><span class="line"> 2.0 4.0 6.4 5.760000000000002</span><br><span class="line"> 3.0 6.0 9.600000000000001 12.96000000000001</span><br><span class="line">loss =  6.720000000000003</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  3.3000000000000003</span><br><span class="line"> 1.0 2.0 3.3000000000000003 1.6900000000000006</span><br><span class="line"> 2.0 4.0 6.6000000000000005 6.7600000000000025</span><br><span class="line"> 3.0 6.0 9.9 15.210000000000003</span><br><span class="line">loss =  7.886666666666668</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  3.4000000000000004</span><br><span class="line"> 1.0 2.0 3.4000000000000004 1.960000000000001</span><br><span class="line"> 2.0 4.0 6.800000000000001 7.840000000000004</span><br><span class="line"> 3.0 6.0 10.200000000000001 17.640000000000008</span><br><span class="line">loss =  9.14666666666667</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  3.5</span><br><span class="line"> 1.0 2.0 3.5 2.25</span><br><span class="line"> 2.0 4.0 7.0 9.0</span><br><span class="line"> 3.0 6.0 10.5 20.25</span><br><span class="line">loss =  10.5</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  3.6</span><br><span class="line"> 1.0 2.0 3.6 2.5600000000000005</span><br><span class="line"> 2.0 4.0 7.2 10.240000000000002</span><br><span class="line"> 3.0 6.0 10.8 23.040000000000006</span><br><span class="line">loss =  11.94666666666667</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  3.7</span><br><span class="line"> 1.0 2.0 3.7 2.8900000000000006</span><br><span class="line"> 2.0 4.0 7.4 11.560000000000002</span><br><span class="line"> 3.0 6.0 11.100000000000001 26.010000000000016</span><br><span class="line">loss =  13.486666666666673</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  3.8000000000000003</span><br><span class="line"> 1.0 2.0 3.8000000000000003 3.240000000000001</span><br><span class="line"> 2.0 4.0 7.6000000000000005 12.960000000000004</span><br><span class="line"> 3.0 6.0 11.4 29.160000000000004</span><br><span class="line">loss =  15.120000000000005</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  3.9000000000000004</span><br><span class="line"> 1.0 2.0 3.9000000000000004 3.610000000000001</span><br><span class="line"> 2.0 4.0 7.800000000000001 14.440000000000005</span><br><span class="line"> 3.0 6.0 11.700000000000001 32.49000000000001</span><br><span class="line">loss =  16.84666666666667</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  4.0</span><br><span class="line"> 1.0 2.0 4.0 4.0</span><br><span class="line"> 2.0 4.0 8.0 16.0</span><br><span class="line"> 3.0 6.0 12.0 36.0</span><br><span class="line">loss =  18.666666666666668</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"> <span class="comment"># 绘loss变化图，横坐标是w，纵坐标是loss</span></span><br><span class="line">plt.plot(w_list, loss_list)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;w&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/10/25/ANq6IBCF9OEZnov.png" alt="image-20211025164754953"></p><p>预测</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 当w为2时间，损失最小</span></span><br><span class="line">y_pers = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> x_data:</span><br><span class="line">    y_per =<span class="number">2</span> * i</span><br><span class="line">    y_pers.append(y_per)</span><br><span class="line">    </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;预测结果:&quot;</span>,<span class="built_in">str</span>(y_pers))    </span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">预测结果: [<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.plot(x_data,y_data,<span class="string">&#x27;o&#x27;</span>,color=<span class="string">&#x27;red&#x27;</span> )</span><br><span class="line">plt.plot(x_data,y_pers)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;predict&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/10/25/XjHdxEn95cB1U84.png" alt="image-20211025164651345"></p><p><img src="https://i.loli.net/2021/10/25/dv7NYHnSXxLjVCK.jpg"></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">实现线性模型（y = w x  + b）并输出loss的3D图像</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> cm</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 线性模型 加入一个偏置b</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">x,w,b</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x * w + b</span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失函数 mse损失</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">x, y, w, b</span>):</span></span><br><span class="line">    y_pred = forward(x, w, b)</span><br><span class="line">    <span class="keyword">return</span> (y_pred - y) * (y_pred - y)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#计算每个x，b对应的损失loss</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_loss</span>(<span class="params">w,b</span>):</span></span><br><span class="line">    l_sum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x_val, y_val <span class="keyword">in</span> <span class="built_in">zip</span>(x_data, y_data):</span><br><span class="line">        y_pred_val = forward(x_val,w,b)</span><br><span class="line">        loss_val = loss(x_val, y_val,w,b)</span><br><span class="line">        l_sum += loss_val</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;\t&#x27;</span>, x_val, y_val, y_pred_val, loss_val)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;loss=&#x27;</span>, l_sum / <span class="number">3</span>)</span><br><span class="line">    <span class="keyword">return</span>  l_sum/<span class="number">3</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.定义网格化数据</span></span><br><span class="line">b_list=np.arange(-<span class="number">30</span>,<span class="number">30</span>,<span class="number">0.1</span>)</span><br><span class="line">w_list=np.arange(-<span class="number">30</span>,<span class="number">30</span>,<span class="number">0.1</span>);</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 2.生成网格化数据</span></span><br><span class="line">xx, yy = np.meshgrid(b_list, w_list, sparse=<span class="literal">False</span>, indexing=<span class="string">&#x27;xy&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.每个点的对应高度 loss</span></span><br><span class="line">zz=get_loss(xx,yy)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"> 1.0 2.0 [[-6.00000000e+01 -5.99000000e+01 -5.98000000e+01 ... -3.00000000e-01</span><br><span class="line">  -2.00000000e-01 -1.00000000e-01]</span><br><span class="line"> [-5.99000000e+01 -5.98000000e+01 -5.97000000e+01 ... -2.00000000e-01</span><br><span class="line">  -1.00000000e-01  8.52651283e-13]</span><br><span class="line"> [-5.98000000e+01 -5.97000000e+01 -5.96000000e+01 ... -1.00000000e-01</span><br><span class="line">   8.52651283e-13  1.00000000e-01]</span><br><span class="line"> ...</span><br><span class="line"> [-3.00000000e-01 -2.00000000e-01 -1.00000000e-01 ...  5.94000000e+01</span><br><span class="line">   5.95000000e+01  5.96000000e+01]</span><br><span class="line"> [-2.00000000e-01 -1.00000000e-01  8.52651283e-13 ...  5.95000000e+01</span><br><span class="line">   5.96000000e+01  5.97000000e+01]</span><br><span class="line"> [-1.00000000e-01  8.52651283e-13  1.00000000e-01 ...  5.96000000e+01</span><br><span class="line">   5.97000000e+01  5.98000000e+01]] [[3.84400e+03 3.83161e+03 3.81924e+03 ... 5.29000e+00 4.84000e+00</span><br><span class="line">  4.41000e+00]</span><br><span class="line"> [3.83161e+03 3.81924e+03 3.80689e+03 ... 4.84000e+00 4.41000e+00</span><br><span class="line">  4.00000e+00]</span><br><span class="line"> [3.81924e+03 3.80689e+03 3.79456e+03 ... 4.41000e+00 4.00000e+00</span><br><span class="line">  3.61000e+00]</span><br><span class="line"> ...</span><br><span class="line"> [5.29000e+00 4.84000e+00 4.41000e+00 ... 3.29476e+03 3.30625e+03</span><br><span class="line">  3.31776e+03]</span><br><span class="line"> [4.84000e+00 4.41000e+00 4.00000e+00 ... 3.30625e+03 3.31776e+03</span><br><span class="line">  3.32929e+03]</span><br><span class="line"> [4.41000e+00 4.00000e+00 3.61000e+00 ... 3.31776e+03 3.32929e+03</span><br><span class="line">  3.34084e+03]]</span><br><span class="line"> 2.0 4.0 [[-90.  -89.8 -89.6 ...  29.4  29.6  29.8]</span><br><span class="line"> [-89.9 -89.7 -89.5 ...  29.5  29.7  29.9]</span><br><span class="line"> [-89.8 -89.6 -89.4 ...  29.6  29.8  30. ]</span><br><span class="line"> ...</span><br><span class="line"> [-30.3 -30.1 -29.9 ...  89.1  89.3  89.5]</span><br><span class="line"> [-30.2 -30.  -29.8 ...  89.2  89.4  89.6]</span><br><span class="line"> [-30.1 -29.9 -29.7 ...  89.3  89.5  89.7]] [[8836.   8798.44 8760.96 ...  645.16  655.36  665.64]</span><br><span class="line"> [8817.21 8779.69 8742.25 ...  650.25  660.49  670.81]</span><br><span class="line"> [8798.44 8760.96 8723.56 ...  655.36  665.64  676.  ]</span><br><span class="line"> ...</span><br><span class="line"> [1176.49 1162.81 1149.21 ... 7242.01 7276.09 7310.25]</span><br><span class="line"> [1169.64 1156.   1142.44 ... 7259.04 7293.16 7327.36]</span><br><span class="line"> [1162.81 1149.21 1135.69 ... 7276.09 7310.25 7344.49]]</span><br><span class="line"> 3.0 6.0 [[-120.  -119.7 -119.4 ...   59.1   59.4   59.7]</span><br><span class="line"> [-119.9 -119.6 -119.3 ...   59.2   59.5   59.8]</span><br><span class="line"> [-119.8 -119.5 -119.2 ...   59.3   59.6   59.9]</span><br><span class="line"> ...</span><br><span class="line"> [ -60.3  -60.   -59.7 ...  118.8  119.1  119.4]</span><br><span class="line"> [ -60.2  -59.9  -59.6 ...  118.9  119.2  119.5]</span><br><span class="line"> [ -60.1  -59.8  -59.5 ...  119.   119.3  119.6]] [[15876.   15800.49 15725.16 ...  2819.61  2851.56  2883.69]</span><br><span class="line"> [15850.81 15775.36 15700.09 ...  2830.24  2862.25  2894.44]</span><br><span class="line"> [15825.64 15750.25 15675.04 ...  2840.89  2872.96  2905.21]</span><br><span class="line"> ...</span><br><span class="line"> [ 4395.69  4356.    4316.49 ... 12723.84 12791.61 12859.56]</span><br><span class="line"> [ 4382.44  4342.81  4303.36 ... 12746.41 12814.24 12882.25]</span><br><span class="line"> [ 4369.21  4329.64  4290.25 ... 12769.   12836.89 12904.96]]</span><br><span class="line">loss= [[9518.66666667 9476.84666667 9435.12       ... 1156.68666667</span><br><span class="line">  1170.58666667 1184.58      ]</span><br><span class="line"> [9499.87666667 9458.09666667 9416.41       ... 1161.77666667</span><br><span class="line">  1175.71666667 1189.75      ]</span><br><span class="line"> [9481.10666667 9439.36666667 9397.72       ... 1166.88666667</span><br><span class="line">  1180.86666667 1194.94      ]</span><br><span class="line"> ...</span><br><span class="line"> [1859.15666667 1841.21666667 1823.37       ... 7753.53666667</span><br><span class="line">  7791.31666667 7829.19      ]</span><br><span class="line"> [1852.30666667 1834.40666667 1816.6        ... 7770.56666667</span><br><span class="line">  7808.38666667 7846.3       ]</span><br><span class="line"> [1845.47666667 1827.61666667 1809.85       ... 7787.61666667</span><br><span class="line">  7825.47666667 7863.43      ]]</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">fig = plt.figure()</span><br><span class="line">ax = Axes3D(fig)</span><br><span class="line">plt.title(&quot;3D&quot;)</span><br><span class="line">ax.plot_surface(xx, yy, zz, cmap=plt.get_cmap(&#x27;rainbow&#x27;)) # 设置曲面的颜色</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/10/25/gj2lr6tU3s9qZhk.png" alt="image-20211025172320588"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;pytorch&quot;&gt;&lt;a href=&quot;#pytorch&quot; class=&quot;headerlink&quot; title=&quot;pytorch&quot;&gt;&lt;/a&gt;pytorch&lt;/h1&gt;&lt;h3 id=&quot;1、线性模型&quot;&gt;&lt;a href=&quot;#1、线性模型&quot; class=&quot;headerlink&quot; </summary>
      
    
    
    
    <category term="pytorch" scheme="http://example.com/categories/pytorch/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="pytorch" scheme="http://example.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>注意力机制</title>
    <link href="http://example.com/post/141d1667.html"/>
    <id>http://example.com/post/141d1667.html</id>
    <published>2021-10-24T07:06:02.000Z</published>
    <updated>2021-10-24T17:41:34.111Z</updated>
    
    <content type="html"><![CDATA[<h1 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">注意力机制的核心在于通过计算一个注意力map，来强调最相关的特征，并避免不相关特征的干扰。</span><br><span class="line">获取注意力map的方法可分为两类：无参数、有参数，如图6所示，主要的区别在于注意图中的重要性权重是否可学习：</span><br><span class="line">注意力机制为深度网络提供了突出给定图像中最重要区域的能力</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1D 通道注意力(任务)</span><br><span class="line">2D 空间注意力</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">对于1D数据来说，在注意力方面，SE仅关注了通道注意力，没考虑空间方面的注意力。</span><br></pre></td></tr></table></figure><h3 id="Encoder-Decoder"><a href="#Encoder-Decoder" class="headerlink" title="Encoder-Decoder"></a>Encoder-Decoder</h3><h3 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h3><h3 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h3>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;注意力机制&quot;&gt;&lt;a href=&quot;#注意力机制&quot; class=&quot;headerlink&quot; title=&quot;注意力机制&quot;&gt;&lt;/a&gt;注意力机制&lt;/h1&gt;&lt;figure class=&quot;highlight plaintext&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;cod</summary>
      
    
    
    
    <category term="深度学习" scheme="http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="人工智能" scheme="http://example.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="注意力机制" scheme="http://example.com/tags/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"/>
    
  </entry>
  
  <entry>
    <title>人脸识别一</title>
    <link href="http://example.com/post/937cd58.html"/>
    <id>http://example.com/post/937cd58.html</id>
    <published>2021-10-24T05:57:51.000Z</published>
    <updated>2021-10-24T17:41:34.109Z</updated>
    
    
    
    
    
  </entry>
  
  <entry>
    <title>目标检测一</title>
    <link href="http://example.com/post/f9b95e7d.html"/>
    <id>http://example.com/post/f9b95e7d.html</id>
    <published>2021-10-24T05:57:22.000Z</published>
    <updated>2021-10-24T17:41:34.112Z</updated>
    
    <content type="html"><![CDATA[<h3 id="YOLO"><a href="#YOLO" class="headerlink" title="YOLO"></a>YOLO</h3><h3 id="YOLO的改进"><a href="#YOLO的改进" class="headerlink" title="YOLO的改进"></a>YOLO的改进</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1、去掉FPN</span><br><span class="line">yolo-f</span><br><span class="line"></span><br><span class="line">2、anchor Free</span><br><span class="line">yolo-x</span><br><span class="line">将每个位置预测个数从3减少到1，并直接预测四个值（即：到网格的左上角的偏移量和box的高宽）；同时，将中心点设为正样本，并预设了一个尺度范围为每个对象指定FPN级别。</span><br><span class="line"></span><br><span class="line">3、NMS-Free</span><br><span class="line">OneNet</span><br><span class="line">4、</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;YOLO&quot;&gt;&lt;a href=&quot;#YOLO&quot; class=&quot;headerlink&quot; title=&quot;YOLO&quot;&gt;&lt;/a&gt;YOLO&lt;/h3&gt;&lt;h3 id=&quot;YOLO的改进&quot;&gt;&lt;a href=&quot;#YOLO的改进&quot; class=&quot;headerlink&quot; title=&quot;YOL</summary>
      
    
    
    
    <category term="目标检测" scheme="http://example.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
    
    <category term="人工智能" scheme="http://example.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="目标检测" scheme="http://example.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>深度学习之生成网络cyclegan</title>
    <link href="http://example.com/post/8b28088e.html"/>
    <id>http://example.com/post/8b28088e.html</id>
    <published>2021-10-23T16:43:40.000Z</published>
    <updated>2021-10-23T19:39:41.301Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Cyclegan"><a href="#Cyclegan" class="headerlink" title="Cyclegan"></a>Cyclegan</h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">贡献：</span><br><span class="line">提出了循环一致性损失</span><br><span class="line">使用非对称的数据就可以进行风格转换</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">总共使用了两队生成器和判别器。</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/10/24/Bdo5CKXvTqup4jJ.png"></p><p><img src="https://i.loli.net/2021/10/24/XCteayNqGjv46ZW.png"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">循环一致性损失：相当于一张图片风格为A，通过G_AB 生成风格为B的假图 -&gt; fake_B,</span><br><span class="line">然后通过 G_BA 生成风格为A假图 fake_A,最终得到的fake_A能够骗过D_A，反之亦然。</span><br><span class="line">A -G_AB&gt; fake_B -G_BA&gt; fake_A</span><br><span class="line">为什么循环一致性损失能够保证在非对称的数据下，图像的内容信息不会改变？</span><br><span class="line"></span><br><span class="line">简单来说：判别器是判断图像风格的，如果没有循环一致性损失时，从源域（风格A）到目标域（风格B）中任意一个或许是最简单的方法，</span><br><span class="line">但是当加入了循环一致性损失后，从源域到目标域的变化中，最简单的方法是只变化风格，而不变化内容，从而约束了生成器的生成。</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Cyclegan&quot;&gt;&lt;a href=&quot;#Cyclegan&quot; class=&quot;headerlink&quot; title=&quot;Cyclegan&quot;&gt;&lt;/a&gt;Cyclegan&lt;/h1&gt;&lt;figure class=&quot;highlight plaintext&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;t</summary>
      
    
    
    
    <category term="生成网络" scheme="http://example.com/categories/%E7%94%9F%E6%88%90%E7%BD%91%E7%BB%9C/"/>
    
    
    <category term="人工智能" scheme="http://example.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="生成网络" scheme="http://example.com/tags/%E7%94%9F%E6%88%90%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>深度学习之生成网络gan</title>
    <link href="http://example.com/post/f5c1817e.html"/>
    <id>http://example.com/post/f5c1817e.html</id>
    <published>2021-10-23T16:43:16.000Z</published>
    <updated>2021-10-23T17:04:46.423Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Gan"><a href="#Gan" class="headerlink" title="Gan"></a>Gan</h1><h1 id="Gan的改进"><a href="#Gan的改进" class="headerlink" title="Gan的改进"></a>Gan的改进</h1>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Gan&quot;&gt;&lt;a href=&quot;#Gan&quot; class=&quot;headerlink&quot; title=&quot;Gan&quot;&gt;&lt;/a&gt;Gan&lt;/h1&gt;&lt;h1 id=&quot;Gan的改进&quot;&gt;&lt;a href=&quot;#Gan的改进&quot; class=&quot;headerlink&quot; title=&quot;Gan的改进&quot;&gt;&lt;</summary>
      
    
    
    
    <category term="生成网络" scheme="http://example.com/categories/%E7%94%9F%E6%88%90%E7%BD%91%E7%BB%9C/"/>
    
    
    <category term="人工智能" scheme="http://example.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="生成网络" scheme="http://example.com/tags/%E7%94%9F%E6%88%90%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>深度学习基础一</title>
    <link href="http://example.com/post/9f799b3c.html"/>
    <id>http://example.com/post/9f799b3c.html</id>
    <published>2021-10-23T16:20:02.000Z</published>
    <updated>2021-10-24T17:41:10.521Z</updated>
    
    <content type="html"><![CDATA[<h1 id="卷积-cnn"><a href="#卷积-cnn" class="headerlink" title="卷积(cnn)"></a>卷积(cnn)</h1><p><img src="https://i.loli.net/2021/10/24/YqCZOxnU51ELi8Q.png"></p><p><img src="https://i.loli.net/2021/10/24/7n9Si4u1d6l3oIN.gif"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">卷积核：简单来说就可以看做一个特征提取器。</span><br><span class="line"></span><br><span class="line">步长stride：每次卷积核在特征图上滑动的步长，默认只滑动一个像素。</span><br><span class="line">当滑动的步长越大，输出的特征图(feature map)就越小，相当于下面说的，池化操作。</span><br><span class="line"></span><br><span class="line">padding操作：填充，对于图像边缘的像素来说，只进行了一次卷积操作，而内部的卷积则进行了至少两次卷积操作，</span><br><span class="line">作用有两个：</span><br><span class="line">1、通过填充后，可以使得卷积后的feature map 和原来的feature map一样的大小,</span><br><span class="line">可以用来控制卷积层输出的特征图的大小。</span><br><span class="line">2、通过对边缘向外填充像素点，来使得边缘像素点，也来充分提取特征。</span><br><span class="line"></span><br><span class="line">感受野：由于卷积操作后，假如是3*3的卷积核，对应于原来的feature map，3*3区域大小的像素</span><br><span class="line">就变为了一个像素区域，因此输出的feature map就相当于原来的3*3的大小，随着卷积层的</span><br><span class="line">加深，感受野也会越来越大。</span><br></pre></td></tr></table></figure><p>当只有一个卷积核时学习到的特征有限，因此需要多个卷积核：</p><p><img src="https://i.loli.net/2021/10/24/upofgIP5DrBdbXe.png"></p><p><img src="https://i.loli.net/2021/10/24/in7tUKYS4jWOywx.png" alt="0005"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">一组多个卷积核就能学到多个特征。</span><br><span class="line">对于k个卷积核中的一个卷积核，每张特征图上对应一个卷积核，</span><br><span class="line">在卷积的时候，每张特征图上卷积核不是同一个卷积核，</span><br><span class="line">也就是说对于3通道来说，卷积核就变为了3D，也就是3个卷积核。</span><br><span class="line">卷积完后，然后相加，就变为了一个feature map</span><br><span class="line">对于k个卷积核，最终会得到k个feature map。</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/10/24/qO12soTD6GRlpc3.png"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">通过上面的例子再说一下：</span><br><span class="line">1、上面图上有两个卷积核，都是3*3的，每个卷积核上的值叫做权重  Bias b是偏置，可以不用管</span><br><span class="line">2、3*3的卷积核，因为上一层feature map是3个，也就是通道数是3，卷积核就会变为3*3*3</span><br><span class="line">3、且每个卷积核不是同一个卷积核，然后每个卷积核和对应的卷积层进行卷积，得到的值，</span><br><span class="line">然后相加，最终变为一个值，图中就是5.</span><br></pre></td></tr></table></figure><h3 id="1、输入层"><a href="#1、输入层" class="headerlink" title="1、输入层"></a>1、输入层</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">一般是一张图像 H*W*C</span><br><span class="line">H：图像高 W ：图像宽 c：图像通道数，刚开始图像通道数为3，RGB</span><br></pre></td></tr></table></figure><h3 id="2、卷积层"><a href="#2、卷积层" class="headerlink" title="2、卷积层"></a>2、卷积层</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">就是我们刚才说的卷积操作。</span><br><span class="line"></span><br><span class="line">有一点需要注意，每层的卷积核是权重共享的。</span><br><span class="line"></span><br><span class="line">为什么要多层卷积？？？？</span><br><span class="line">卷积一直在做的一件事就是对特征提取。</span><br><span class="line">多层卷积相当于做了多次特征提取，随着网络的加深提取到的特征越来越特殊，</span><br><span class="line">越来越能代表这种物体的特征。</span><br></pre></td></tr></table></figure><h3 id="3、池化层"><a href="#3、池化层" class="headerlink" title="3、池化层"></a>3、池化层</h3><p><img src="https://i.loli.net/2021/10/24/uGerRDV4a5oOZv2.png" alt="0006"></p><p><img src="https://i.loli.net/2021/10/24/SJ1AoQqvflEc7eZ.png" alt="0007"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">特征选择作用：采样层实际上就是一个特征选择的过程，挑选出对我们最有用的信息，去除多余的，冗余的信息</span><br><span class="line">1、最大池化</span><br><span class="line">和卷积核大小类似，加入说我们选2*2的大小，进行池化</span><br><span class="line">最大池化就是说从这2*2 ，也就是4个点中，选择最大的那个，其他舍弃</span><br><span class="line">池化可以一定程度提高空间不变性，比如旋转后，和旋转前，池化是一样的，但是局部有限区域内。</span><br><span class="line"></span><br><span class="line">2、平均池化</span><br><span class="line">就是把四个数加起来除4，当做特征值</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">可以看到，池化说到底还是一个特征选择，信息过滤的过程，我们会损失了一部分信息，来减小参数，计算量</span><br><span class="line">但是现在有的地方也不用池化层了。</span><br></pre></td></tr></table></figure><h3 id="4、激活函数-加入非线性"><a href="#4、激活函数-加入非线性" class="headerlink" title="4、激活函数(加入非线性)"></a>4、激活函数(加入非线性)</h3><p><img src="https://i.loli.net/2021/10/24/hLZuNxep4nrMyzf.png" alt="000"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">对于原有的卷积层，只能处理线性问题，对于非线性分类问题解决不了。</span><br><span class="line">那么无论神经网络的层数有多少还是在解决线性函数问题，因为两个线性函数的组合还是线性的。</span><br><span class="line"></span><br><span class="line">因此需要引入非线性函数。</span><br><span class="line">sigmoid函数：</span><br><span class="line">在反向传播时，容易出现梯度消失</span><br><span class="line">计算量大</span><br><span class="line">tanh函数：</span><br><span class="line">在反向传播时，容易出现梯度消失</span><br><span class="line">计算量大</span><br><span class="line">Relu函数：</span><br><span class="line">优点：避免梯度消失的问题，通过使得部分神经元为0，抑制作用，避免过拟合问题</span><br><span class="line">缺点：当值为负数的时候，就会变为0，神经元完全不起作用。</span><br><span class="line"></span><br><span class="line">注：现在一般都会使用Relu函数和其改进版本，如ELU、PRelu等，从某种程度上避免了使部分神经元死掉的问题。</span><br></pre></td></tr></table></figure><h3 id="5、标准化层"><a href="#5、标准化层" class="headerlink" title="5、标准化层"></a>5、标准化层</h3><p><img src="https://i.loli.net/2021/10/24/uBCUmf89LDoShkN.png"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">什么是标准化？？？</span><br><span class="line">数据的标准化是指将数据按照比例缩放,使之落入一个特定的区间.</span><br><span class="line"></span><br><span class="line">为什么要做标准化？？？</span><br><span class="line">1. 加快网络的训练和收敛的速度</span><br><span class="line">将数据分布在均值为零，方差为1状态下，使得梯度稳定，容易收敛。</span><br><span class="line"></span><br><span class="line">2. 控制梯度爆炸和防止梯度消失</span><br><span class="line">控制数据集中分布在0值附近，有两个好处，</span><br><span class="line">例如sigmoid函数：</span><br><span class="line">1、在经过sigmoid函数将值约束到0附近，梯度不会消失。</span><br><span class="line">因此通常会加在全连接和激励函数之间。</span><br><span class="line">2、如果不使用标准化可能初始loss过大，梯度反向传播就会积累，前面几层会变得非常大，产生梯度爆炸的问题。</span><br><span class="line">而使用标准化后权值就不会很大了。</span><br><span class="line"></span><br><span class="line">3. 防止过拟合</span><br><span class="line">标准化层在使用过程中，通常会以考虑整个batch的数据进行标准化，</span><br><span class="line">考虑整体比只考虑单个肯定能够一定程度上解决过拟合问题</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="6、全连接层"><a href="#6、全连接层" class="headerlink" title="6、全连接层"></a>6、全连接层</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">为什么需要全连接层？</span><br><span class="line">卷积层我们会发现，有一个天生致命的缺点，那就是没有全局，</span><br><span class="line">因此我们需要一个来把握全局的，那就是全连接层。</span><br><span class="line">相当于把前面，每个学习到的特征进行一个汇总操作，使得我们能够把握到整体。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">全连接层如果加深层数，增加神经元输量，网络能力会得到提升，但是也有可能出现过拟合问题。</span><br></pre></td></tr></table></figure><h3 id="7、输出层"><a href="#7、输出层" class="headerlink" title="7、输出层"></a>7、输出层</h3><p><img src="https://i.loli.net/2021/10/25/hDmXqrZFkHx2lBe.png"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">对于分类任务来说：</span><br><span class="line">FC+Softmax+Cross-entropy loss</span><br><span class="line">1、当输入为X, 预测类别为j 的概率为P</span><br><span class="line">2、所有预测类别概率和为1</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">一句话说就是：卷积层负责提取特征，池化负责特征选择，激活函数增加非线性能力，标准化层用来约束数据分布，全连接层负责分类</span><br></pre></td></tr></table></figure><h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><h3 id="分类损失"><a href="#分类损失" class="headerlink" title="分类损失"></a>分类损失</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1、交叉熵损失CE</span><br><span class="line">用于度量两个函数的分布.</span><br><span class="line"></span><br><span class="line">信息熵:去掉冗余信息后的平均信息量。衡量不确定性，不确定性大，信息熵就越大。</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">相对熵(KL散度): 下图</span><br><span class="line">KL散度用于估计两个分布的相似性</span><br><span class="line">其中l(p,p)是分布p的熵，而l(p,q)就是p和q的交叉熵。</span><br><span class="line">假如p是一个已知的分布，则熵是一个常数。</span><br><span class="line">此时KL与l(p,q)也就是交叉熵只有一个常数的差异，两者是等价的。</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/10/24/ae6Rd5IFBGs7Myw.jpg"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">交叉熵CE:</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/10/24/kSmxX8PlvJoji9E.jpg"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">由于交叉熵正负样本平衡，对于正负样本不平衡网络就不能很好的学习</span><br><span class="line">2、平衡交叉熵损失（正负样本不平衡问题）</span><br><span class="line">3、专注难样本 Focal loss（难易样本）</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">在工程上，经常和softmax loss一起使用：</span><br></pre></td></tr></table></figure><p>$$<br>softmax<br>$$</p><p><img src="https://i.loli.net/2021/10/25/ZVeDqaf5v4EnKm9.png" alt="image-20211025004211016"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">假如</span><br><span class="line"></span><br><span class="line">输出为 [0.3 0.3 0.4] 目标是 [0 0 1]</span><br><span class="line">则：</span><br><span class="line">交叉熵为：- (ln(0.3) * 0 + ln(0.3) * 0 + ln(0.4) * 1 ) = - ln4</span><br></pre></td></tr></table></figure><h3 id="对于softmax的改进："><a href="#对于softmax的改进：" class="headerlink" title="对于softmax的改进："></a>对于softmax的改进：</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1、Large-Margin Softmax Loss L-Softmax loss</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/10/25/KIV7MAyW6j8sdSB.png" alt="image-20211025005021695"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">这张图显示的是不同softmax loss和L-Softmax loss学习到的特征分布。</span><br><span class="line">第一列就是softmax，第2列是L-Softmax </span><br><span class="line">loss在参数m取不同值时的分布。通过可视化特征可知学习到的类间的特征是比较明显的，但是类内比较散。</span><br><span class="line"></span><br><span class="line">也就是说使得不同类别之间的夹角增大，同时同类分布也更为紧凑。</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/10/25/aIugtWSJpTvlKxC.png" alt="image-20211025011546484"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">因为最后是全连接层的输出：</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/10/25/aWOm7K2qAoDjd3S.png" alt="image-20211025011903423"></p><p><img src="https://i.loli.net/2021/10/25/5dRUQVgxm8vTjnA.png" alt="image-20211025011941901"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">因为cos在[0,π]上是减函数，</span><br><span class="line">当f1 &gt; f2 时，也就是说θ1&lt;θ2,样本将被分类为类别1,</span><br><span class="line">当f1 &lt; f2 时，也就是说θ1&gt;θ2,样本将被分类为类别2,</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/10/25/uBvEb68Dyoh5TZn.png"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">L-Softmax损失函数中对角度施加了更为强烈的约束,m&gt;=1</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/10/25/59VNlbKSinOh3zr.png" alt="image-20211025012636114"></p><p><img src="https://i.loli.net/2021/10/25/3L9CA2Qxe4GdlJM.png" alt="image-20211025012807781"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">2、Angular Softmax Loss A-Softmax</span><br><span class="line">Angular Softmax Loss(简称A-Softmax loss)与L-Softmax思想类似，主要区别是进一步加入了一个权重约束。</span><br><span class="line">使得||w|| = 1，就变成了</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/10/25/3GcBTQE9bgL4sVW.png" alt="image-20211025005437525"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">使得类别的判断将只依赖于样本与类别权重的夹角。</span><br></pre></td></tr></table></figure><h3 id="回归损失"><a href="#回归损失" class="headerlink" title="回归损失"></a>回归损失</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1、L1损失</span><br><span class="line">Mean absolute loss(MAE)也被称为L1 Loss，是以绝对误差作为距离</span><br><span class="line">由于L1 loss具有稀疏性，为了惩罚较大的值，因此常常将其作为正则项添加到其他loss中作为约束。</span><br><span class="line">L1 loss的最大问题是梯度在零点不平滑，导致会跳过极小值。</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/10/24/53QFT6aUDu4GqrX.png"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">2、L2损失</span><br><span class="line">Mean Squared Loss/ Quadratic Loss(MSE loss)也被称为L2 loss，或欧氏距离，它以误差的平方和作为距离：</span><br><span class="line">L2 loss也常常作为正则项。当预测值与目标值相差很大时, 梯度容易爆炸，因为梯度里包含了x−t。</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/10/24/oFuUOi58LA6ItaT.png"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">3、L1 loss与L2 loss的改进</span><br><span class="line"></span><br><span class="line">原始的L1 loss和L2 loss都有缺陷，比如L1 loss的最大问题是梯度不平滑，而L2 loss的最大问题是容易梯度爆炸，所以研究者们对其提出了很多的改进。</span><br><span class="line"></span><br><span class="line">在faster rcnn框架中，使用了smooth L1 loss来综合L1与L2 loss的优点。</span><br><span class="line"></span><br><span class="line">在x比较小时，上式等价于L2 loss，保持平滑。在x比较大时，上式等价于L1 loss，可以限制数值的大小。</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/10/24/NiLIMr5eu2FDfk8.jpg"></p><p><img src="https://i.loli.net/2021/10/24/cumGWvkODPKZeqr.png" alt="image-20211024235947407"></p><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;卷积-cnn&quot;&gt;&lt;a href=&quot;#卷积-cnn&quot; class=&quot;headerlink&quot; title=&quot;卷积(cnn)&quot;&gt;&lt;/a&gt;卷积(cnn)&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/10/24/YqCZOxnU51EL</summary>
      
    
    
    
    <category term="深度学习基础" scheme="http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="人工智能" scheme="http://example.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>英语单词一</title>
    <link href="http://example.com/post/afe428e5.html"/>
    <id>http://example.com/post/afe428e5.html</id>
    <published>2021-10-23T16:17:38.000Z</published>
    <updated>2021-10-23T16:38:04.481Z</updated>
    
    <content type="html"><![CDATA[<h1 id="A"><a href="#A" class="headerlink" title="A"></a>A</h1>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;A&quot;&gt;&lt;a href=&quot;#A&quot; class=&quot;headerlink&quot; title=&quot;A&quot;&gt;&lt;/a&gt;A&lt;/h1&gt;</summary>
      
    
    
    
    <category term="英语" scheme="http://example.com/categories/%E8%8B%B1%E8%AF%AD/"/>
    
    
    <category term="英语" scheme="http://example.com/tags/%E8%8B%B1%E8%AF%AD/"/>
    
  </entry>
  
  <entry>
    <title>test</title>
    <link href="http://example.com/post/d87f7e0c.html"/>
    <id>http://example.com/post/d87f7e0c.html</id>
    <published>2021-10-23T15:15:36.000Z</published>
    <updated>2021-10-23T15:17:50.455Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">test文件</span><br><span class="line">撒很难过船</span><br><span class="line"></span><br><span class="line">大萨达</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/10/23/eKFWfmxuVjSndR6.jpg"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;figure class=&quot;highlight plaintext&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;test文件&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;撒很难过船&lt;/span&gt;&lt;br&gt;</summary>
      
    
    
    
    <category term="人工智能" scheme="http://example.com/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    
    <category term="人工智能" scheme="http://example.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://example.com/post/16107.html"/>
    <id>http://example.com/post/16107.html</id>
    <published>2021-10-23T06:47:10.453Z</published>
    <updated>2021-10-23T11:16:13.161Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for</summary>
      
    
    
    
    
  </entry>
  
</feed>
