<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>blog</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2021-10-29T20:10:46.570Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>祎熵</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>pytorch学习经验总结</title>
    <link href="http://example.com/post/14fefbbe.html"/>
    <id>http://example.com/post/14fefbbe.html</id>
    <published>2021-10-25T18:10:09.000Z</published>
    <updated>2021-10-29T20:10:46.570Z</updated>
    
    <content type="html"><![CDATA[<h1 id="学习总结">学习总结</h1><h3 id="pytorch中view的用法">1、PyTorch中view的用法</h3><p>view可以通过现有的维度，进行其他维度的推测和修改。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a=torch.Tensor([[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]]])</span><br><span class="line">b=torch.Tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(a.view(<span class="number">1</span>,<span class="number">6</span>)) <span class="comment">#可以写成view（1，-1）</span></span><br><span class="line"><span class="built_in">print</span>(b.view(<span class="number">1</span>,<span class="number">6</span>))</span><br></pre></td></tr></table></figure><p>得到：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor([[<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>]]) </span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a=torch.Tensor([[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]]])</span><br><span class="line"><span class="built_in">print</span>(a.view(<span class="number">3</span>,<span class="number">2</span>)) <span class="comment">#可以写成view（3，-1）</span></span><br></pre></td></tr></table></figure><p>得到：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor([[<span class="number">1.</span>, <span class="number">2.</span>],</span><br><span class="line">        [<span class="number">3.</span>, <span class="number">4.</span>],</span><br><span class="line">        [<span class="number">5.</span>, <span class="number">6.</span>]])</span><br></pre></td></tr></table></figure><h3 id="pytorch中backward的用法">2、PyTorch中backward的用法</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">如果是标量，一个数，不用传参会隐式反向求导。</span><br><span class="line">而如果是个张量(除了0阶张量)，需要明确传递一个梯度参数。</span><br></pre></td></tr></table></figure><p>当我们在Q上调用.backward()时，autograd会计算这些梯度并将其存储在各自tensor的.grad属性中。</p><p>我们需要在Q.backward()中明确传递一个梯度参数。梯度是一个与Q相同形状的tensor，它代表Q的梯度与自身的关系.</p><p>等价地，我们也可以将Q汇总成一个标量，并隐式地向后调用，比如Q.sum().backward()</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">external_grad = torch.tensor([<span class="number">1.</span>, <span class="number">1.</span>])</span><br><span class="line">Q.backward(gradient=external_grad)</span><br></pre></td></tr></table></figure><p>或</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Q.<span class="built_in">sum</span>().backward()</span><br></pre></td></tr></table></figure><h3 id="pytorch自动求导中在测试阶段">3、pytorch自动求导中在测试阶段</h3><p>在with torch.no_grad()代码块中，参数的梯度不会被计算。</p><p>我们不需要求梯度。</p><h3 id="在有些时候我们需要主动获取参数">4、在有些时候我们需要主动获取参数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="number">1</span>、需要冻结一些层</span><br><span class="line">model = torchvision.models.resnet18(pretrained=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Freeze all the parameters in the network</span></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">    param.requires_grad = <span class="literal">False</span></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">2、使用requires_grad可以排除一些参数</span><br><span class="line"># construct an optimizer</span><br><span class="line">    params = [p for p in model.parameters() if p.requires_grad]</span><br><span class="line">    optimizer = torch.optim.SGD(params, lr=0.005,</span><br><span class="line">                                momentum=0.9, weight_decay=0.0005)</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;学习总结&quot;&gt;学习总结&lt;/h1&gt;
&lt;h3 id=&quot;pytorch中view的用法&quot;&gt;1、PyTorch中view的用法&lt;/h3&gt;
&lt;p&gt;view可以通过现有的维度，进行其他维度的推测和修改。&lt;/p&gt;
&lt;figure class=&quot;highlight pytho</summary>
      
    
    
    
    <category term="pytorch" scheme="http://example.com/categories/pytorch/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="pytorch" scheme="http://example.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch错误记录</title>
    <link href="http://example.com/post/ad55112e.html"/>
    <id>http://example.com/post/ad55112e.html</id>
    <published>2021-10-25T17:32:35.000Z</published>
    <updated>2021-10-27T17:45:32.226Z</updated>
    
    <content type="html"><![CDATA[<h1 id="pytorch错误记录">pytorch错误记录</h1><h3 id="问题-brokenpipeerror-errno-32-broken-pipe">1、问题 BrokenPipeError: [Errno 32] Broken pipe</h3><h4 id="原因windows使用dataloader时设置num_workers的问题">原因：<code>Windows</code>使用<code>DataLoader</code>时设置<code>num_workers</code>的问题。</h4><h4 id="解决方案num_workers不要更改设置为默认值0">解决方案:<code>num_workers</code>不要更改，设置为默认值0。</h4><p>num_workers参数允许输入的数据类型是<strong>整型</strong>，表示加载数据的<strong>子进程数</strong>。也就是使用<strong>多进程</strong>来加载数据，这样<strong>效率</strong>更高一些。如果设置为<code>0</code>表示子进程数是<code>0</code>，也就是只用一个主进程来加载数据。数据量不多的情况下，不需要用多进程，因为进程的创建和销毁也花时间。</p><h3 id="dataloader-worker-pids-22004-23344-exited-unexpectedly">2、DataLoader worker (pid(s) 22004, 23344) exited unexpectedly</h3><h3 id="解决loader中令num_workers0">解决：loader中令num_workers=0</h3><h3 id="在pycharm中使用matplotlib绘图时出现attributeerror-module-matplotlib-has-no-attribute-verbose">3、在pycharm中使用matplotlib绘图时，出现AttributeError: module 'matplotlib' has no attribute 'verbose'</h3><h3 id="解决在pycharm中打开-file----settings----tools----python-scientific-将show-plots-in-toolwindow去掉勾选并应用">解决：在pycharm中打开&quot; File --&gt; Settings --&gt; Tools --&gt; Python Scientific &quot;,将&quot;Show plots in toolwindow&quot;去掉勾选，并应用。</h3><h3 id="runtimeerror-stack-expects-each-tensor-to-be-equal-size">4、RuntimeError: stack expects each tensor to be equal size</h3><h3 id="原因dataloader输入的数据集的大小必须是一致的">原因：DataLoader输入的数据集的大小必须是一致的</h3><h3 id="必须统一成一样的大小">必须统一成一样的大小</h3><h3 id="valueerror-images-is-expected-to-be-a-list-of-3d-tensors-of-shape-c-h-w-torch.size480-640">5、ValueError: images is expected to be a list of 3d tensors of shape [C, H, W], torch.Size([480, 640]</h3>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;pytorch错误记录&quot;&gt;pytorch错误记录&lt;/h1&gt;
&lt;h3 id=&quot;问题-brokenpipeerror-errno-32-broken-pipe&quot;&gt;1、问题 BrokenPipeError: [Errno 32] Broken pipe&lt;/h3&gt;
&lt;</summary>
      
    
    
    
    <category term="pytorch" scheme="http://example.com/categories/pytorch/"/>
    
    
    <category term="pytorch" scheme="http://example.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch深度学习四</title>
    <link href="http://example.com/post/ec074f75.html"/>
    <id>http://example.com/post/ec074f75.html</id>
    <published>2021-10-25T17:23:49.000Z</published>
    <updated>2021-10-25T17:59:15.696Z</updated>
    
    <content type="html"><![CDATA[<h1 id="cifar10数据集分类">CIFAR10数据集分类</h1><p>我们将使用CIFAR10数据集。它有以下类别：&quot;飞机&quot;、&quot;汽车&quot;、&quot;鸟&quot;、&quot;猫&quot;、&quot;鹿&quot;、&quot;狗&quot;、&quot;青蛙&quot;、&quot;马&quot;、&quot;船&quot;、&quot;卡车&quot;。CIFAR-10中的图像大小为3x32x32，即尺寸为32x32像素的3通道彩色图像。</p><div class="figure"><img src="https://i.loli.net/2021/10/26/BrYbLZyKadtUNM5.png" alt="img" /><p class="caption">img</p></div><p><strong>训练一个图像分类器</strong></p><p>我们将按顺序进行以下步骤。</p><ol style="list-style-type: decimal"><li>使用Torchvision加载和规范化CIFAR10训练和测试数据集</li><li>定义一个卷积神经网络</li><li>定义一个损失函数</li><li>在训练数据上训练该网络</li><li>在测试数据上测试该网络</li></ol><h3 id="加载并归一化cifar10">1、加载并归一化CIFAR10</h3><p>使用torchvision，加载CIFAR10是非常容易的。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br></pre></td></tr></table></figure><p>torchvision数据集的输出是范围[0, 1]的PILImage图像。我们将它们转换为归一化范围[-1, 1]的tensor。</p><blockquote><p>注意:如果在Windows上运行并得到BrokenPipeError，请尝试将torch.utils.data.DataLoader()的num_worker设为0。</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">transform = transforms.Compose(</span><br><span class="line">    [transforms.ToTensor(),</span><br><span class="line">     transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))])</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">trainset = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>,</span><br><span class="line">                                        download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,</span><br><span class="line">                                          shuffle=<span class="literal">True</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">testset = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>,</span><br><span class="line">                                       download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,</span><br><span class="line">                                         shuffle=<span class="literal">False</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">classes = (<span class="string">&#x27;plane&#x27;</span>, <span class="string">&#x27;car&#x27;</span>, <span class="string">&#x27;bird&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>,</span><br><span class="line">           <span class="string">&#x27;deer&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;frog&#x27;</span>, <span class="string">&#x27;horse&#x27;</span>, <span class="string">&#x27;ship&#x27;</span>, <span class="string">&#x27;truck&#x27;</span>)</span><br></pre></td></tr></table></figure><p>输出：</p><div class="figure"><img src="https://i.loli.net/2021/10/26/5R6pv1xB3f9Ic7z.png" alt="image-20211026012846467" /><p class="caption">image-20211026012846467</p></div><p>让我们展示一些训练图像:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># functions to show an image</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">imshow</span>(<span class="params">img</span>):</span></span><br><span class="line">    img = img / <span class="number">2</span> + <span class="number">0.5</span>     <span class="comment"># unnormalize</span></span><br><span class="line">    npimg = img.numpy()</span><br><span class="line">    plt.imshow(np.transpose(npimg, (<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># get some random training images</span></span><br><span class="line">dataiter = <span class="built_in">iter</span>(trainloader)</span><br><span class="line">images, labels = dataiter.<span class="built_in">next</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># show images</span></span><br><span class="line">imshow(torchvision.utils.make_grid(images))</span><br><span class="line"><span class="comment"># print labels</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27; &#x27;</span>.join(<span class="string">&#x27;%5s&#x27;</span> % classes[labels[j]] <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(batch_size)))</span><br></pre></td></tr></table></figure><div class="figure"><img src="https://i.loli.net/2021/10/26/aG8wBRFVxrAnM12.png" alt="image-20211026013608496" /><p class="caption">image-20211026013608496</p></div><h3 id="定义一个卷积神经网络">2、定义一个卷积神经网络</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.pool(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.pool(F.relu(self.conv2(x)))</span><br><span class="line">        x = torch.flatten(x, <span class="number">1</span>) <span class="comment"># flatten all dimensions except batch</span></span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line"><span class="built_in">print</span>(net)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Net(</span><br><span class="line">  (conv1): Conv2d(<span class="number">3</span>, <span class="number">6</span>, kernel_size=(<span class="number">5</span>, <span class="number">5</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (pool): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="literal">False</span>)</span><br><span class="line">  (conv2): Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=(<span class="number">5</span>, <span class="number">5</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (fc1): Linear(in_features=<span class="number">400</span>, out_features=<span class="number">120</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  (fc2): Linear(in_features=<span class="number">120</span>, out_features=<span class="number">84</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  (fc3): Linear(in_features=<span class="number">84</span>, out_features=<span class="number">10</span>, bias=<span class="literal">True</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="定义一个损失函数和优化器">3、定义一个损失函数和优化器</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">让我们使用一个分类交叉熵损失和带momentum的SGD。</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure><p>我们如何在GPU上运行这些神经网络？</p><h3 id="在gpu上进行训练">4、在GPU上进行训练</h3><p>就像你如何将tensor转移到GPU上一样，你可以将神经网络转移到GPU上。</p><p>如果我们有CUDA可用，让我们首先把我们的设备定义为第一个可见的cuda设备。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assuming that we are on a CUDA machine, this should print a CUDA device:</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(device)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cuda:<span class="number">0</span></span><br></pre></td></tr></table></figure><p>本节的其余部分假设设备是一个CUDA设备。</p><p>然后，这些方法将递归所有模块，并将其参数和缓冲区转换为CUDA tensor。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net.to(device)</span><br></pre></td></tr></table></figure><p>请记住，你也必须将每一步的输入和目标发送到GPU。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">inputs, labels = data[<span class="number">0</span>].to(device), data[<span class="number">1</span>].to(device)</span><br></pre></td></tr></table></figure><h3 id="训练网络">5、训练网络</h3><p>这就是事情开始变得有趣的时候。我们只需在我们的数据迭代器上进行循环，并将图像输入到网络中并进行优化。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):  <span class="comment"># loop over the dataset multiple times</span></span><br><span class="line"></span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(trainloader, <span class="number">0</span>):</span><br><span class="line">        <span class="comment"># get the inputs; data is a list of [inputs, labels]</span></span><br><span class="line">        <span class="comment">#inputs, labels = data</span></span><br><span class="line">inputs, labels = data[<span class="number">0</span>].to(device), data[<span class="number">1</span>].to(device)</span><br><span class="line">        <span class="comment"># zero the parameter gradients</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># forward + backward + optimize</span></span><br><span class="line">        outputs = net(inputs)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># print statistics</span></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">2000</span> == <span class="number">1999</span>:    <span class="comment"># print every 2000 mini-batches</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;[%d, %5d] loss: %.3f&#x27;</span> %</span><br><span class="line">                  (epoch + <span class="number">1</span>, i + <span class="number">1</span>, running_loss / <span class="number">2000</span>))</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Finished Training&#x27;</span>)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[<span class="number">1</span>,  <span class="number">2000</span>] loss: <span class="number">2.223</span></span><br><span class="line">[<span class="number">1</span>,  <span class="number">4000</span>] loss: <span class="number">1.924</span></span><br><span class="line">[<span class="number">1</span>,  <span class="number">6000</span>] loss: <span class="number">1.711</span></span><br><span class="line">[<span class="number">1</span>,  <span class="number">8000</span>] loss: <span class="number">1.627</span></span><br><span class="line">[<span class="number">1</span>, <span class="number">10000</span>] loss: <span class="number">1.552</span></span><br><span class="line">[<span class="number">1</span>, <span class="number">12000</span>] loss: <span class="number">1.531</span></span><br><span class="line">[<span class="number">2</span>,  <span class="number">2000</span>] loss: <span class="number">1.458</span></span><br><span class="line">[<span class="number">2</span>,  <span class="number">4000</span>] loss: <span class="number">1.404</span></span><br><span class="line">[<span class="number">2</span>,  <span class="number">6000</span>] loss: <span class="number">1.385</span></span><br><span class="line">[<span class="number">2</span>,  <span class="number">8000</span>] loss: <span class="number">1.359</span></span><br><span class="line">[<span class="number">2</span>, <span class="number">10000</span>] loss: <span class="number">1.307</span></span><br><span class="line">[<span class="number">2</span>, <span class="number">12000</span>] loss: <span class="number">1.307</span></span><br><span class="line">Finished Training</span><br></pre></td></tr></table></figure><p>让我们快速保存我们的训练模型。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">PATH = <span class="string">&#x27;./cifar_net.pth&#x27;</span></span><br><span class="line">torch.save(net.state_dict(), PATH)</span><br></pre></td></tr></table></figure><h3 id="在测试数据上测试网络">6、在测试数据上测试网络</h3><p>我们已经在训练数据集上对网络进行了两次训练。但是我们需要检查网络是否学到了任何东西。</p><p>我们将通过预测神经网络输出的类别标签来检查，并将其与正确的类别标签进行对照。如果预测是正确的，我们就把这个样本添加到正确的预测列表中。</p><p>好的，第一步。让我们从测试集上显示一张图片来熟悉一下。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataiter = <span class="built_in">iter</span>(testloader)</span><br><span class="line">images, labels = dataiter.<span class="built_in">next</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># print images</span></span><br><span class="line">imshow(torchvision.utils.make_grid(images))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;GroundTruth: &#x27;</span>, <span class="string">&#x27; &#x27;</span>.join(<span class="string">&#x27;%5s&#x27;</span> % classes[labels[j]] <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>)))</span><br></pre></td></tr></table></figure><div class="figure"><img src="https://i.loli.net/2021/10/26/ds9f2cWzTYiK5gu.png" alt="image-20211026015038642" /><p class="caption">image-20211026015038642</p></div><p>接下来，让我们装回我们保存的模型（注意：保存和重新加载模型在这里不是必须的，我们这样做只是为了说明如何做）。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = Net()</span><br><span class="line">net.load_state_dict(torch.load(PATH))点击复制错误复制成功</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;All keys matched successfully&gt;</span><br></pre></td></tr></table></figure><p>好了，现在让我们看看神经网络认为上面这些例子是什么。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">outputs = net(images)</span><br></pre></td></tr></table></figure><p>输出是10个类别的置信度。一个类别的置信度越高，网络就越认为该图像属于该特定类别。因此，让我们得到最高置信度的索引。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">_, predicted = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Predicted: &#x27;</span>, <span class="string">&#x27; &#x27;</span>.join(<span class="string">&#x27;%5s&#x27;</span> % classes[predicted[j]]</span><br><span class="line">                              <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>)))</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Predicted:    cat  ship  ship  ship</span><br></pre></td></tr></table></figure><p>结果似乎很好。</p><p>让我们看看该网络在整个数据集上的表现如何。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">correct = <span class="number">0</span></span><br><span class="line">total = <span class="number">0</span></span><br><span class="line"><span class="comment"># since we&#x27;re not training, we don&#x27;t need to calculate the gradients for our outputs</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">        images, labels = data</span><br><span class="line">        <span class="comment"># calculate outputs by running images through the network</span></span><br><span class="line">        outputs = net(images)</span><br><span class="line">        <span class="comment"># the class with the highest energy is what we choose as prediction</span></span><br><span class="line">        _, predicted = torch.<span class="built_in">max</span>(outputs.data, <span class="number">1</span>)</span><br><span class="line">        total += labels.size(<span class="number">0</span>)</span><br><span class="line">        correct += (predicted == labels).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Accuracy of the network on the 10000 test images: %d %%&#x27;</span> % (</span><br><span class="line">    <span class="number">100</span> * correct / total))</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Accuracy of the network on the <span class="number">10000</span> test images: <span class="number">52</span> %</span><br></pre></td></tr></table></figure><p>让我们来看看哪些类预测表现好，哪些类别预测表现不好：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># prepare to count predictions for each class</span></span><br><span class="line">correct_pred = &#123;classname: <span class="number">0</span> <span class="keyword">for</span> classname <span class="keyword">in</span> classes&#125;</span><br><span class="line">total_pred = &#123;classname: <span class="number">0</span> <span class="keyword">for</span> classname <span class="keyword">in</span> classes&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># again no gradients needed</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">        images, labels = data</span><br><span class="line">        outputs = net(images)</span><br><span class="line">        _, predictions = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># collect the correct predictions for each class</span></span><br><span class="line">        <span class="keyword">for</span> label, prediction <span class="keyword">in</span> <span class="built_in">zip</span>(labels, predictions):</span><br><span class="line">            <span class="keyword">if</span> label == prediction:</span><br><span class="line">                correct_pred[classes[label]] += <span class="number">1</span></span><br><span class="line">            total_pred[classes[label]] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># print accuracy for each class</span></span><br><span class="line"><span class="keyword">for</span> classname, correct_count <span class="keyword">in</span> correct_pred.items():</span><br><span class="line">    accuracy = <span class="number">100</span> * <span class="built_in">float</span>(correct_count) / total_pred[classname]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Accuracy for class &#123;:5s&#125; is: &#123;:.1f&#125; %&quot;</span>.<span class="built_in">format</span>(classname,</span><br><span class="line">                                                   accuracy))</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Accuracy <span class="keyword">for</span> <span class="class"><span class="keyword">class</span> <span class="title">plane</span> <span class="title">is</span>:</span> <span class="number">46.4</span> %</span><br><span class="line">Accuracy <span class="keyword">for</span> <span class="class"><span class="keyword">class</span> <span class="title">car</span>   <span class="title">is</span>:</span> <span class="number">60.5</span> %</span><br><span class="line">Accuracy <span class="keyword">for</span> <span class="class"><span class="keyword">class</span> <span class="title">bird</span>  <span class="title">is</span>:</span> <span class="number">58.8</span> %</span><br><span class="line">Accuracy <span class="keyword">for</span> <span class="class"><span class="keyword">class</span> <span class="title">cat</span>   <span class="title">is</span>:</span> <span class="number">31.5</span> %</span><br><span class="line">Accuracy <span class="keyword">for</span> <span class="class"><span class="keyword">class</span> <span class="title">deer</span>  <span class="title">is</span>:</span> <span class="number">49.8</span> %</span><br><span class="line">Accuracy <span class="keyword">for</span> <span class="class"><span class="keyword">class</span> <span class="title">dog</span>   <span class="title">is</span>:</span> <span class="number">53.9</span> %</span><br><span class="line">Accuracy <span class="keyword">for</span> <span class="class"><span class="keyword">class</span> <span class="title">frog</span>  <span class="title">is</span>:</span> <span class="number">56.0</span> %</span><br><span class="line">Accuracy <span class="keyword">for</span> <span class="class"><span class="keyword">class</span> <span class="title">horse</span> <span class="title">is</span>:</span> <span class="number">48.8</span> %</span><br><span class="line">Accuracy <span class="keyword">for</span> <span class="class"><span class="keyword">class</span> <span class="title">ship</span>  <span class="title">is</span>:</span> <span class="number">63.5</span> %</span><br><span class="line">Accuracy <span class="keyword">for</span> <span class="class"><span class="keyword">class</span> <span class="title">truck</span> <span class="title">is</span>:</span> <span class="number">56.7</span> %</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;cifar10数据集分类&quot;&gt;CIFAR10数据集分类&lt;/h1&gt;
&lt;p&gt;我们将使用CIFAR10数据集。它有以下类别：&amp;quot;飞机&amp;quot;、&amp;quot;汽车&amp;quot;、&amp;quot;鸟&amp;quot;、&amp;quot;猫&amp;quot;、&amp;quot;鹿&amp;quot;、&amp;</summary>
      
    
    
    
    <category term="pytorch" scheme="http://example.com/categories/pytorch/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="pytorch" scheme="http://example.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch基础四</title>
    <link href="http://example.com/post/a9deff0a.html"/>
    <id>http://example.com/post/a9deff0a.html</id>
    <published>2021-10-25T16:48:46.000Z</published>
    <updated>2021-10-25T17:24:28.912Z</updated>
    
    <content type="html"><![CDATA[<h1 id="神经网络">神经网络</h1><p>神经网络可以用torch.nn包来构建。</p><p>现在你对autograd有了一丝了解，nn依赖于autograd来定义模型并对其进行微分。一个 nn.Module 包含层和一个返回输出的 forward(input) 方法。</p><p>例如，请看这个对数字图像进行分类的网络：</p><div class="figure"><img src="https://i.loli.net/2021/10/26/1Kpka6THP7SDrYz.png" alt="img" /><p class="caption">img</p></div><p>它是一个简单的前馈网络。它接受输入，一个接一个地将其送入若干层，最后给出输出。</p><p>一个典型的神经网络的训练过程如下。</p><ul><li>定义有一些可学习参数（或权重）的神经网络</li><li>在输入的数据集上进行迭代</li><li>通过网络处理输入</li><li>计算损失（输出离正确值有多远）。</li><li>将梯度传播回网络的参数中</li><li>更新网络的权重，通常使用一个简单的更新规则：权重 = 权重 - 学习率 * 梯度</li></ul><h3 id="定义网络">定义网络</h3><p>让我们来定义这个网络。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        <span class="comment"># 1 input image channel, 6 output channels, 5x5 square convolution</span></span><br><span class="line">        <span class="comment"># kernel</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="comment"># an affine operation: y = Wx + b</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)  <span class="comment"># 5*5 from image dimension</span></span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># Max pooling over a (2, 2) window</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">        <span class="comment"># If the size is a square, you can specify with a single number</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class="number">2</span>)</span><br><span class="line">        x = torch.flatten(x, <span class="number">1</span>) <span class="comment"># flatten all dimensions except the batch dimension</span></span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line"><span class="built_in">print</span>(net)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Net(</span><br><span class="line">  (conv1): Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=(<span class="number">5</span>, <span class="number">5</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (conv2): Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=(<span class="number">5</span>, <span class="number">5</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (fc1): Linear(in_features=<span class="number">400</span>, out_features=<span class="number">120</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  (fc2): Linear(in_features=<span class="number">120</span>, out_features=<span class="number">84</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  (fc3): Linear(in_features=<span class="number">84</span>, out_features=<span class="number">10</span>, bias=<span class="literal">True</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>你只需要定义前向函数，而后向函数（计算梯度的地方）会用autograd自动为你定义。你可以在前向函数中使用任何tensor操作。</p><p>一个模型的可学习参数由net.parameters()返回。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">params = <span class="built_in">list</span>(net.parameters())</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(params))</span><br><span class="line"><span class="built_in">print</span>(params[<span class="number">0</span>].size())  <span class="comment"># conv1&#x27;s .weight</span></span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="number">10</span></span><br><span class="line">torch.Size([<span class="number">6</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>])</span><br></pre></td></tr></table></figure><p>让我们试试一个随机的32x32的输入。注意：这个网络（LeNet）的预期输入尺寸是32x32。要在MNIST数据集上使用这个网络，请将数据集上的图像大小调整为32x32。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">out = net(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(out)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor([[ <span class="number">0.0796</span>,  <span class="number">0.0070</span>, -<span class="number">0.0793</span>,  <span class="number">0.0234</span>, -<span class="number">0.0222</span>, -<span class="number">0.0487</span>, -<span class="number">0.0889</span>, -<span class="number">0.0076</span>,</span><br><span class="line">         -<span class="number">0.0486</span>, -<span class="number">0.0754</span>]], grad_fn=&lt;AddmmBackward&gt;)</span><br></pre></td></tr></table></figure><p>将所有参数的梯度缓冲区归零，并以随机梯度进行反向传播。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net.zero_grad()</span><br><span class="line">out.backward(torch.randn(<span class="number">1</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure><blockquote><p>注意:torch.nn只支持小批次。整个 torch.nn 包只支持小批量样本的输入，而不是单个样本。例如，nn.Conv2d将接收一个4D tensor的nSamples x nChannels x Height x Width。如果你有一个单一的样本，只需使用input.unsqueeze(0)来添加一个虚假的批次维度。</p></blockquote><p>在进一步进行之前，让我们回顾一下到目前为止你所看到的所有的类。</p><p>回顾一下:</p><ul><li>torch.Tensor - 一个多维数组，支持像backward()这样的autograd操作。还可以保存tensor的梯度。</li><li>nn.Module - 神经网络模块。封装参数的便捷方式，方便将其移动到GPU，导出，加载等。</li><li>nn.Parameter - 一种tensor，当分配给一个模块的属性时，会自动注册为一个参数。</li><li>autograd.Function - 实现autograd操作的前向和后向定义。每个Tensor操作至少创建一个Function节点，连接到创建Tensor的函数，并对其历史进行编码。</li></ul><p>目前为止，我们涵盖了:</p><ul><li>定义一个神经网络</li><li>处理输入并向后调用</li></ul><p>还剩下:</p><ul><li>计算损失</li><li>更新网络的权重</li></ul><h3 id="损失函数">损失函数</h3><p>损失函数接收（输出，目标）一对输入，并计算出一个值，估计输出与目标的距离。</p><p>nn包中有几个不同的损失函数。一个简单的损失是：nn.MSELoss，它计算的是输入和目标之间的均方误差。</p><p>比如说:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">output = net(<span class="built_in">input</span>)</span><br><span class="line">target = torch.randn(<span class="number">10</span>)  <span class="comment"># a dummy target, for example</span></span><br><span class="line">target = target.view(<span class="number">1</span>, -<span class="number">1</span>)  <span class="comment"># make it the same shape as output</span></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">loss = criterion(output, target)</span><br><span class="line"><span class="built_in">print</span>(loss)点击复制错误复制成功</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor(<span class="number">1.3725</span>, grad_fn=&lt;MseLossBackward&gt;)</span><br></pre></td></tr></table></figure><p>现在，如果你按照损失的方向，使用它的.grad_fn属性，你会看到一个计算的图表，看起来像这样。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">input</span> -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d</span><br><span class="line">      -&gt; flatten -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear</span><br><span class="line">      -&gt; MSELoss</span><br><span class="line">      -&gt; loss</span><br></pre></td></tr></table></figure><p>因此，当我们调用loss.backward()时，就会得到整个图相对神经网络参数的微分，并且图中所有require_grad=True的tensor都会有其.grad tensor的梯度积累。</p><p>为了说明问题，让我们回顾一下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(loss.grad_fn)  <span class="comment"># MSELoss</span></span><br><span class="line"><span class="built_in">print</span>(loss.grad_fn.next_functions[<span class="number">0</span>][<span class="number">0</span>])  <span class="comment"># Linear</span></span><br><span class="line"><span class="built_in">print</span>(loss.grad_fn.next_functions[<span class="number">0</span>][<span class="number">0</span>].next_functions[<span class="number">0</span>][<span class="number">0</span>])  <span class="comment"># ReLU</span></span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;MseLossBackward <span class="built_in">object</span> at <span class="number">0x00000228AE1129B0</span>&gt;</span><br><span class="line">&lt;AddmmBackward <span class="built_in">object</span> at <span class="number">0x00000228AE112710</span>&gt;</span><br><span class="line">&lt;AccumulateGrad <span class="built_in">object</span> at <span class="number">0x00000228AE1129B0</span>&gt;</span><br></pre></td></tr></table></figure><h3 id="反向传播">反向传播</h3><p>为了反向传播误差，我们所要做的就是 loss.backward()。不过你需要清除现有的梯度，否则梯度会被累积到现有的梯度上。</p><p>现在我们将调用 loss.backward()，并看看conv1在回传前后的偏置梯度。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net.zero_grad()     <span class="comment"># zeroes the gradient buffers of all parameters</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;conv1.bias.grad before backward&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(net.conv1.bias.grad)</span><br><span class="line"></span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;conv1.bias.grad after backward&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(net.conv1.bias.grad)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">conv1.bias.grad before backward</span><br><span class="line"><span class="literal">None</span></span><br><span class="line">conv1.bias.grad after backward</span><br><span class="line">tensor([ <span class="number">0.0062</span>,  <span class="number">0.0148</span>,  <span class="number">0.0117</span>, -<span class="number">0.0082</span>, -<span class="number">0.0040</span>,  <span class="number">0.0063</span>])</span><br></pre></td></tr></table></figure><p>现在，我们已经看到了如何使用损失函数。</p><p>稍后阅读:</p><p>神经网络包包含各种模块和损失函数，它们构成了深度神经网络的组成部分。<a href="https://pytorch.org/docs/nn">这里</a>有一个带有文档的完整列表。</p><p>现在只剩下的是：</p><p>更新网络的权重</p><h3 id="更新权重">更新权重</h3><p>实践中使用的最简单的更新规则是随机梯度下降法（SGD）。</p><p>weight = weight - learning_rate * gradient</p><p>我们可以用简单的Python代码来实现这一点。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> net.parameters():</span><br><span class="line">    f.data.sub_(f.grad.data * learning_rate)点击复制错误复制成功</span><br></pre></td></tr></table></figure><p>然而，由于你使用神经网络，你想使用各种不同的更新规则，如SGD、Nesterov-SGD、Adam、RMSProp等。为了实现这一点，我们建立了一个小包：torch.opt，实现了所有这些方法。使用它是非常简单的。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># create your optimizer</span></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># in your training loop:</span></span><br><span class="line">optimizer.zero_grad()   <span class="comment"># zero the gradient buffers</span></span><br><span class="line">output = net(<span class="built_in">input</span>)</span><br><span class="line">loss = criterion(output, target)</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()    <span class="comment"># Does the update</span></span><br></pre></td></tr></table></figure><blockquote><p>使用optimizer.zero_grad()观察一下梯度缓冲区是如何被手动设置为零的。这是因为梯度是累积的，正如在Backprop部分所解释的。</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;神经网络&quot;&gt;神经网络&lt;/h1&gt;
&lt;p&gt;神经网络可以用torch.nn包来构建。&lt;/p&gt;
&lt;p&gt;现在你对autograd有了一丝了解，nn依赖于autograd来定义模型并对其进行微分。一个 nn.Module 包含层和一个返回输出的 forward(input</summary>
      
    
    
    
    <category term="pytorch" scheme="http://example.com/categories/pytorch/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="pytorch" scheme="http://example.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch基础三</title>
    <link href="http://example.com/post/e50c9314.html"/>
    <id>http://example.com/post/e50c9314.html</id>
    <published>2021-10-25T15:55:02.000Z</published>
    <updated>2021-10-25T18:29:05.593Z</updated>
    
    <content type="html"><![CDATA[<h1 id="autograd中的微分法">Autograd中的微分法</h1><p>让我们看看autograd是如何收集梯度的。我们创建了两个tensor a和b，并规定require_grad=True。这向autograd发出信号，对它们的每一个操作都应该被跟踪。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">a = torch.tensor([<span class="number">2.</span>, <span class="number">3.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor([<span class="number">6.</span>, <span class="number">4.</span>], requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>我们从a和b创建另一个tensor Q。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Q = <span class="number">3</span>*a**<span class="number">3</span> - b**<span class="number">2</span></span><br></pre></td></tr></table></figure><p>让我们假设a和b是NN的参数，Q是误差。在NN训练中，我们希望误差的梯度与参数有关，即</p><div class="figure"><img src="https://i.loli.net/2021/10/25/wL23YQHUDaAScXx.png" alt="img" /><p class="caption">img</p></div><p>当我们在Q上调用.backward()时，autograd会计算这些梯度并将其存储在各自tensor的.grad属性中。</p><p>我们需要在Q.backward()中明确传递一个梯度参数，因为它是一个矢量。梯度是一个与Q相同形状的tensor，它代表Q的梯度与自身的关系.</p><p>等价地，我们也可以将Q汇总成一个标量，并隐式地向后调用，比如Q.sum().backward()</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">external_grad = torch.tensor([<span class="number">1.</span>, <span class="number">1.</span>])</span><br><span class="line">Q.backward(gradient=external_grad)</span><br></pre></td></tr></table></figure><p>梯度现在被存入a.grad和b.grad中。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># check if collected gradients are correct</span></span><br><span class="line"><span class="built_in">print</span>(<span class="number">9</span>*a**<span class="number">2</span> == a.grad)</span><br><span class="line"><span class="built_in">print</span>(-<span class="number">2</span>*b == b.grad)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor([<span class="literal">True</span>, <span class="literal">True</span>])</span><br><span class="line">tensor([<span class="literal">True</span>, <span class="literal">True</span>])</span><br></pre></td></tr></table></figure><h1 id="计算图">计算图</h1><p>从概念上讲，autograd在一个由Function对象组成的有向无环图（DAG）中保存了数据（tensor）和所有执行的操作（以及产生的新tensor）的记录。在这个DAG中，叶子是输入tensor，根部是输出tensor。通过追踪这个图从根到叶，你可以使用链式规则自动计算梯度。</p><p>在一个前向传递中，autograd同时做两件事：</p><ul><li>运行请求的操作，计算出结果的tensor，以及</li><li>在DAG中维护该操作的梯度函数。</li></ul><p>当在DAG根上调用.backward()时，后向传递开始了。</p><ul><li>计算每个.grad_fn的梯度。</li><li>将它们累积到各自的tensor的 .grad 属性中，并且</li><li>使用链规则，一直传播到叶子tensor。</li></ul><p>下面是我们的例子中DAG的可视化表示。在图中，箭头的方向是向前传递的。节点代表前向传递中每个操作的后向函数。蓝色的叶子结点代表我们的叶子tensor a和b。</p><div class="figure"><img src="https://i.loli.net/2021/10/26/hPfQ1gmw9DJ6WXd.png" alt="img" /><p class="caption">img</p></div><p>DAG在PyTorch中是动态的。需要注意的是，图是从头开始重新创建的；在每次调用.backward()后，autograd开始填充一个新的图。这正是允许你在模型中使用控制流语句的原因；如果需要，你可以在每次迭代时改变形状、大小和操作。</p><p><strong>排除在DAG之外</strong></p><p>torch.autograd 追踪所有将 require_grad 标志设置为 True 的tensor上的操作。对于不需要梯度的tensor，将此属性设置为False将其排除在梯度计算DAG之外。</p><p>即使只有一个输入tensor的 requires_grad=True，一个操作的输出tensor也需要梯度。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">5</span>)</span><br><span class="line">y = torch.rand(<span class="number">5</span>, <span class="number">5</span>)</span><br><span class="line">z = torch.rand((<span class="number">5</span>, <span class="number">5</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">a = x + y</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Does `a` require gradients? : <span class="subst">&#123;a.requires_grad&#125;</span>&quot;</span>)</span><br><span class="line">b = x + z</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Does `b` require gradients?: <span class="subst">&#123;b.requires_grad&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Does `a` require gradients? : <span class="literal">False</span></span><br><span class="line">Does `b` require gradients?: <span class="literal">True</span></span><br></pre></td></tr></table></figure><p>在NN中，不计算梯度的参数通常被称为冻结参数。如果你事先知道你不需要这些参数的梯度，那么 &quot;冻结 &quot;你的模型的一部分是很有用的（这通过减少自动梯度计算提供了一些性能上的好处）。</p><p>从DAG中排除的另一个常见情况是对预训练的网络进行微调。</p><p>在微调中，我们冻结了大部分模型，通常只修改分类器层以对新标签进行预测。让我们通过一个小例子来证明这一点。像以前一样，我们加载一个预训练的resnet18模型，并冻结所有的参数。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"></span><br><span class="line">model = torchvision.models.resnet18(pretrained=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Freeze all the parameters in the network</span></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">    param.requires_grad = <span class="literal">False</span></span><br></pre></td></tr></table></figure><p>假设我们想在一个有10个标签的新数据集上微调模型。在resnet中，分类器是最后一个线性层model.fc。我们可以简单地用一个新的线性层（默认情况下是解冻的）来代替它，作为我们的分类器。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.fc = nn.Linear(<span class="number">512</span>, <span class="number">10</span>)</span><br></pre></td></tr></table></figure><p>现在，除了model.fc的参数，模型中的所有参数都被冻结。唯一能计算梯度的参数是model.fc的权重和偏置。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Optimize only the classifier</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">1e-2</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure><p>注意尽管我们在优化器中注册了所有的参数，但唯一计算梯度的参数（因此在梯度下降中更新）是分类器的权重和偏置。</p><p>在torch.no_grad()中，同样的排除功能可以作为一个上下文管理器使用。</p><blockquote><p>cvtutorials.com：这里的意思是在with torch.no_grad()代码块中，参数的梯度不会被计算。</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;autograd中的微分法&quot;&gt;Autograd中的微分法&lt;/h1&gt;
&lt;p&gt;让我们看看autograd是如何收集梯度的。我们创建了两个tensor a和b，并规定require_grad=True。这向autograd发出信号，对它们的每一个操作都应该被跟踪。&lt;/</summary>
      
    
    
    
    <category term="pytorch" scheme="http://example.com/categories/pytorch/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="pytorch" scheme="http://example.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch基础二</title>
    <link href="http://example.com/post/a7500519.html"/>
    <id>http://example.com/post/a7500519.html</id>
    <published>2021-10-25T15:11:00.000Z</published>
    <updated>2021-10-25T15:38:19.100Z</updated>
    
    <content type="html"><![CDATA[<h1 id="fashionmnist时装分类">FashionMNIST时装分类</h1><p>现在，我们有了一个模型和数据，是时候通过在数据上优化模型的参数来训练、验证和测试我们的模型了。训练模型是一个迭代的过程；在每个迭代中（称为epoch），模型对输出进行猜测，计算其猜测的误差（损失），收集误差相对于其参数的导数（正如我们在上一节看到的），并使用梯度下降优化这些参数。</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line">from torchvision import datasets</span><br><span class="line">from torchvision.transforms import ToTensor, Lambda</span><br></pre></td></tr></table></figure><h3 id="数据集">1、数据集：</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">training_data = datasets.FashionMNIST(</span><br><span class="line">    root=&quot;data&quot;,</span><br><span class="line">    train=True,</span><br><span class="line">    download=True,</span><br><span class="line">    transform=ToTensor()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">test_data = datasets.FashionMNIST(</span><br><span class="line">    root=&quot;data&quot;,</span><br><span class="line">    train=False,</span><br><span class="line">    download=True,</span><br><span class="line">    transform=ToTensor()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">train_dataloader = DataLoader(training_data, batch_size=64)</span><br><span class="line">test_dataloader = DataLoader(test_data, batch_size=64)</span><br></pre></td></tr></table></figure><h3 id="模型">2、模型：</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NeuralNetwork</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(NeuralNetwork, self).__init__()</span><br><span class="line">        self.flatten = nn.Flatten()</span><br><span class="line">        self.linear_relu_stack = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">28</span>*<span class="number">28</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">10</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        logits = self.linear_relu_stack(x)</span><br><span class="line">        <span class="keyword">return</span> logits</span><br><span class="line"></span><br><span class="line">model = NeuralNetwork()</span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">NeuralNetwork(</span><br><span class="line">  (flatten): Flatten(start_dim=<span class="number">1</span>, end_dim=-<span class="number">1</span>)</span><br><span class="line">  (linear_relu_stack): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Linear(in_features=<span class="number">784</span>, out_features=<span class="number">512</span>, bias=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">1</span>): ReLU()</span><br><span class="line">    (<span class="number">2</span>): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">512</span>, bias=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">3</span>): ReLU()</span><br><span class="line">    (<span class="number">4</span>): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">10</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="超参数">3、超参数</h3><p>超参数是可调整的参数，让你控制模型优化过程。不同的超参数值会影响模型的训练和收敛率</p><p>我们为训练定义了以下超参数：</p><ul><li>epoch数 - 在数据集上迭代的次数</li><li>批量大小--在更新参数之前，通过网络传播的数据样本的数量。</li><li>学习率--在每个批次/epoch更新模型参数的程度。较小的值产生缓慢的学习速度，而较大的值可能会导致训练期间的不可预测的行为。</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">learning_rate = <span class="number">1e-3</span></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">epochs = <span class="number">5</span></span><br></pre></td></tr></table></figure><h3 id="优化">4、优化：</h3><p>一旦我们设定了超参数，我们就可以通过优化Loop来训练和优化我们的模型。优化循环的每一次迭代被称为一个epoch。</p><p>每个epoch由两个主要部分组成。</p><ul><li><p>训练loop--在训练数据集上迭代，试图收敛到最佳参数。</p></li><li><p>验证/测试循环--迭代测试数据集，以检查模型性能是否在提高。</p></li></ul><p>让我们简单地熟悉一下训练loop中使用的一些概念。</p><h4 id="损失函数">4.1、损失函数</h4><p>当遇到一些训练数据时，我们未经训练的网络很可能不会给出正确的答案。损失函数衡量的是获得的结果与目标值的不相似程度，它是我们在训练期间想要最小化的损失函数。为了计算损失，我们使用给定数据样本的输入进行预测，并与真实数据标签值进行比较。</p><p>常见的损失函数包括用于回归任务的nn.MSELoss（均方误差）和用于分类的nn.NLLLoss（负对数似然）。nn.CrossEntropyLoss结合了nn.LogSoftmax和nn.NLLLoss。</p><p>我们将模型的输出对数传递给 nn.CrossEntropyLoss，它将对对数进行标准化处理并计算预测误差。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Initialize the loss function</span></span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure><h4 id="优化器">4.2、优化器</h4><p>优化是在每个训练步骤中调整模型参数以减少模型误差的过程。优化算法定义了这个过程是如何进行的（在这个例子中，我们使用随机梯度下降法）。所有的优化逻辑都被封装在优化器对象中。在这里，我们使用SGD优化器；此外，PyTorch中还有许多不同的优化器，如Adam和RMSProp，它们对不同类型的模型和数据有更好的效果。</p><p>我们通过注册需要训练的模型参数来初始化优化器，并传入学习率超参数。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)</span><br></pre></td></tr></table></figure><p>在训练loop中，优化分三步进行。</p><ul><li>调用optimizer.zero_grad()来重置模型参数的梯度。梯度默认为累加；为了防止重复计算，我们在每次迭代中明确地将其归零。</li><li>通过调用loss.backwards()对预测损失进行反向传播。PyTorch将损失的梯度与每个参数联系在一起。</li><li>一旦我们有了梯度，我们就可以调用optimizer.step()来根据向后传递中收集的梯度调整参数。</li></ul><h3 id="训练">5、训练</h3><p>我们定义了train_loop和test_loop，train_loop负责循环我们的优化代码，test_loop负责根据测试数据评估模型的性能。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_loop</span>(<span class="params">dataloader, model, loss_fn, optimizer</span>):</span></span><br><span class="line">    size = <span class="built_in">len</span>(dataloader.dataset)</span><br><span class="line">    <span class="keyword">for</span> batch, (X, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line">        <span class="comment"># Compute prediction and loss</span></span><br><span class="line">        pred = model(X)</span><br><span class="line">        loss = loss_fn(pred, y)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Backpropagation</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> batch % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            loss, current = loss.item(), batch * <span class="built_in">len</span>(X)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;loss: <span class="subst">&#123;loss:&gt;7f&#125;</span>  [<span class="subst">&#123;current:&gt;5d&#125;</span>/<span class="subst">&#123;size:&gt;5d&#125;</span>]&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_loop</span>(<span class="params">dataloader, model, loss_fn</span>):</span></span><br><span class="line">    size = <span class="built_in">len</span>(dataloader.dataset)</span><br><span class="line">    num_batches = <span class="built_in">len</span>(dataloader)</span><br><span class="line">    test_loss, correct = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> dataloader:</span><br><span class="line">            pred = model(X)</span><br><span class="line">            test_loss += loss_fn(pred, y).item()</span><br><span class="line">            correct += (pred.argmax(<span class="number">1</span>) == y).<span class="built_in">type</span>(torch.<span class="built_in">float</span>).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">    test_loss /= num_batches</span><br><span class="line">    correct /= size</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Test Error: \n Accuracy: <span class="subst">&#123;(<span class="number">100</span>*correct):&gt;<span class="number">0.1</span>f&#125;</span>%, Avg loss: <span class="subst">&#123;test_loss:&gt;8f&#125;</span> \n&quot;</span>)</span><br></pre></td></tr></table></figure><p>我们初始化损失函数和优化器，并将其传递给train_loop和test_loop。随意增加epochs的数量，以跟踪模型的改进性能。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)</span><br><span class="line"></span><br><span class="line">epochs = <span class="number">10</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;t+<span class="number">1</span>&#125;</span>\n-------------------------------&quot;</span>)</span><br><span class="line">    train_loop(train_dataloader, model, loss_fn, optimizer)</span><br><span class="line">    test_loop(test_dataloader, model, loss_fn)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Done!&quot;</span>)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Epoch <span class="number">1</span></span><br><span class="line">-------------------------------</span><br><span class="line">loss: <span class="number">2.300908</span>  [    <span class="number">0</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">2.295182</span>  [ <span class="number">6400</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">2.270906</span>  [<span class="number">12800</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">2.267570</span>  [<span class="number">19200</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">2.259168</span>  [<span class="number">25600</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">2.226254</span>  [<span class="number">32000</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">2.243158</span>  [<span class="number">38400</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">2.205932</span>  [<span class="number">44800</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">2.200457</span>  [<span class="number">51200</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">2.182929</span>  [<span class="number">57600</span>/<span class="number">60000</span>]</span><br><span class="line">Test Error: </span><br><span class="line"> Accuracy: <span class="number">44.3</span>%, Avg loss: <span class="number">2.165722</span> </span><br><span class="line"></span><br><span class="line">Epoch <span class="number">2</span></span><br><span class="line">-------------------------------</span><br><span class="line">loss: <span class="number">2.174429</span>  [    <span class="number">0</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">2.169780</span>  [ <span class="number">6400</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">2.109140</span>  [<span class="number">12800</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">2.121284</span>  [<span class="number">19200</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">2.088894</span>  [<span class="number">25600</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">2.025757</span>  [<span class="number">32000</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">2.058241</span>  [<span class="number">38400</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">1.983098</span>  [<span class="number">44800</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">1.979899</span>  [<span class="number">51200</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">1.916613</span>  [<span class="number">57600</span>/<span class="number">60000</span>]</span><br><span class="line">Test Error: </span><br><span class="line"> Accuracy: <span class="number">60.4</span>%, Avg loss: <span class="number">1.908849</span> </span><br><span class="line"></span><br><span class="line">Epoch <span class="number">3</span></span><br><span class="line">-------------------------------</span><br><span class="line">loss: <span class="number">1.943270</span>  [    <span class="number">0</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">1.921433</span>  [ <span class="number">6400</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">1.800139</span>  [<span class="number">12800</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">1.823319</span>  [<span class="number">19200</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">1.740386</span>  [<span class="number">25600</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">1.683273</span>  [<span class="number">32000</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">1.699353</span>  [<span class="number">38400</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">1.609211</span>  [<span class="number">44800</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">1.620515</span>  [<span class="number">51200</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">1.512938</span>  [<span class="number">57600</span>/<span class="number">60000</span>]</span><br><span class="line">Test Error: </span><br><span class="line"> Accuracy: <span class="number">61.7</span>%, Avg loss: <span class="number">1.530475</span> </span><br><span class="line"></span><br><span class="line">Epoch <span class="number">4</span></span><br><span class="line">-------------------------------</span><br><span class="line">loss: <span class="number">1.603113</span>  [    <span class="number">0</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">1.571732</span>  [ <span class="number">6400</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">1.416359</span>  [<span class="number">12800</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">1.467911</span>  [<span class="number">19200</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">1.363838</span>  [<span class="number">25600</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">1.356508</span>  [<span class="number">32000</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">1.367825</span>  [<span class="number">38400</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">1.303143</span>  [<span class="number">44800</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">1.326409</span>  [<span class="number">51200</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">1.225853</span>  [<span class="number">57600</span>/<span class="number">60000</span>]</span><br><span class="line">Test Error: </span><br><span class="line"> Accuracy: <span class="number">63.7</span>%, Avg loss: <span class="number">1.251046</span> </span><br><span class="line"></span><br><span class="line">Epoch <span class="number">5</span></span><br><span class="line">-------------------------------</span><br><span class="line">loss: <span class="number">1.333282</span>  [    <span class="number">0</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">1.320183</span>  [ <span class="number">6400</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">1.149248</span>  [<span class="number">12800</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">1.237831</span>  [<span class="number">19200</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">1.119001</span>  [<span class="number">25600</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">1.147076</span>  [<span class="number">32000</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">1.169061</span>  [<span class="number">38400</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">1.117458</span>  [<span class="number">44800</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">1.144186</span>  [<span class="number">51200</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">1.063922</span>  [<span class="number">57600</span>/<span class="number">60000</span>]</span><br><span class="line">Test Error: </span><br><span class="line"> Accuracy: <span class="number">64.9</span>%, Avg loss: <span class="number">1.082073</span> </span><br><span class="line"></span><br><span class="line">Epoch <span class="number">6</span></span><br><span class="line">-------------------------------</span><br><span class="line">loss: <span class="number">1.155662</span>  [    <span class="number">0</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">1.166080</span>  [ <span class="number">6400</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">0.977541</span>  [<span class="number">12800</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">1.099136</span>  [<span class="number">19200</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">0.974443</span>  [<span class="number">25600</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">1.010497</span>  [<span class="number">32000</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">1.048995</span>  [<span class="number">38400</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">1.001892</span>  [<span class="number">44800</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">1.029063</span>  [<span class="number">51200</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">0.964841</span>  [<span class="number">57600</span>/<span class="number">60000</span>]</span><br><span class="line">Test Error: </span><br><span class="line"> Accuracy: <span class="number">66.0</span>%, Avg loss: <span class="number">0.975931</span> </span><br><span class="line"></span><br><span class="line">Epoch <span class="number">7</span></span><br><span class="line">-------------------------------</span><br><span class="line">loss: <span class="number">1.035540</span>  [    <span class="number">0</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">1.068692</span>  [ <span class="number">6400</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">0.862561</span>  [<span class="number">12800</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">1.009486</span>  [<span class="number">19200</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">0.886436</span>  [<span class="number">25600</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">0.916163</span>  [<span class="number">32000</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">0.971357</span>  [<span class="number">38400</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">0.927605</span>  [<span class="number">44800</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">0.951319</span>  [<span class="number">51200</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">0.899080</span>  [<span class="number">57600</span>/<span class="number">60000</span>]</span><br><span class="line">Test Error: </span><br><span class="line"> Accuracy: <span class="number">67.2</span>%, Avg loss: <span class="number">0.904650</span> </span><br><span class="line"></span><br><span class="line">Epoch <span class="number">8</span></span><br><span class="line">-------------------------------</span><br><span class="line">loss: <span class="number">0.948853</span>  [    <span class="number">0</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">1.001911</span>  [ <span class="number">6400</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">0.781012</span>  [<span class="number">12800</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">0.947148</span>  [<span class="number">19200</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">0.828207</span>  [<span class="number">25600</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">0.848013</span>  [<span class="number">32000</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">0.916945</span>  [<span class="number">38400</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">0.877578</span>  [<span class="number">44800</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">0.896014</span>  [<span class="number">51200</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">0.852041</span>  [<span class="number">57600</span>/<span class="number">60000</span>]</span><br><span class="line">Test Error: </span><br><span class="line"> Accuracy: <span class="number">68.5</span>%, Avg loss: <span class="number">0.853586</span> </span><br><span class="line"></span><br><span class="line">Epoch <span class="number">9</span></span><br><span class="line">-------------------------------</span><br><span class="line">loss: <span class="number">0.882596</span>  [    <span class="number">0</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">0.952000</span>  [ <span class="number">6400</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">0.720312</span>  [<span class="number">12800</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">0.900999</span>  [<span class="number">19200</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">0.786528</span>  [<span class="number">25600</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">0.797113</span>  [<span class="number">32000</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">0.875853</span>  [<span class="number">38400</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">0.842273</span>  [<span class="number">44800</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">0.854816</span>  [<span class="number">51200</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">0.815920</span>  [<span class="number">57600</span>/<span class="number">60000</span>]</span><br><span class="line">Test Error: </span><br><span class="line"> Accuracy: <span class="number">69.8</span>%, Avg loss: <span class="number">0.814876</span> </span><br><span class="line"></span><br><span class="line">Epoch <span class="number">10</span></span><br><span class="line">-------------------------------</span><br><span class="line">loss: <span class="number">0.829697</span>  [    <span class="number">0</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">0.911849</span>  [ <span class="number">6400</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">0.673247</span>  [<span class="number">12800</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">0.865375</span>  [<span class="number">19200</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">0.754743</span>  [<span class="number">25600</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">0.758169</span>  [<span class="number">32000</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">0.842532</span>  [<span class="number">38400</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">0.816081</span>  [<span class="number">44800</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">0.822818</span>  [<span class="number">51200</span>/<span class="number">60000</span>]</span><br><span class="line">loss: <span class="number">0.787022</span>  [<span class="number">57600</span>/<span class="number">60000</span>]</span><br><span class="line">Test Error: </span><br><span class="line"> Accuracy: <span class="number">71.1</span>%, Avg loss: <span class="number">0.784023</span> </span><br><span class="line"></span><br><span class="line">Done!</span><br></pre></td></tr></table></figure><h3 id="保存和载入模型">6、保存和载入模型</h3><p>在本节中，我们将研究如何通过保存、加载和运行模型预测来保持模型状态</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.onnx <span class="keyword">as</span> onnx</span><br><span class="line"><span class="keyword">import</span> torchvision.models <span class="keyword">as</span> models</span><br></pre></td></tr></table></figure><p>PyTorch模型将学到的参数存储在内部状态字典中，称为state_dict。这些可以通过torch.save方法持久化。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.save(model.state_dict(), <span class="string">&quot;model.pth&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Saved PyTorch Model State to model.pth&quot;</span>)</span><br></pre></td></tr></table></figure><p>为了加载模型的权重，你需要先创建一个相同模型的实例，然后用load_state_dict()方法加载参数。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = NeuralNetwork()</span><br><span class="line">model.load_state_dict(torch.load(<span class="string">&quot;model.pth&quot;</span>))</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure><p><strong><em>请确保在推理前调用model.eval()方法，以将dropout和batch normalization层设置为eval模式。如果不这样做，将产生不一致的推理结果。</em></strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">classes = [</span><br><span class="line">    <span class="string">&quot;T-shirt/top&quot;</span>,</span><br><span class="line">    <span class="string">&quot;Trouser&quot;</span>,</span><br><span class="line">    <span class="string">&quot;Pullover&quot;</span>,</span><br><span class="line">    <span class="string">&quot;Dress&quot;</span>,</span><br><span class="line">    <span class="string">&quot;Coat&quot;</span>,</span><br><span class="line">    <span class="string">&quot;Sandal&quot;</span>,</span><br><span class="line">    <span class="string">&quot;Shirt&quot;</span>,</span><br><span class="line">    <span class="string">&quot;Sneaker&quot;</span>,</span><br><span class="line">    <span class="string">&quot;Bag&quot;</span>,</span><br><span class="line">    <span class="string">&quot;Ankle boot&quot;</span>,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">x, y = test_data[<span class="number">0</span>][<span class="number">0</span>], test_data[<span class="number">0</span>][<span class="number">1</span>]</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    pred = model(x)</span><br><span class="line">    predicted, actual = classes[pred[<span class="number">0</span>].argmax(<span class="number">0</span>)], classes[y]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Predicted: &quot;<span class="subst">&#123;predicted&#125;</span>&quot;, Actual: &quot;<span class="subst">&#123;actual&#125;</span>&quot;&#x27;</span>)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Predicted: <span class="string">&quot;Ankle boot&quot;</span>, Actual: <span class="string">&quot;Ankle boot&quot;</span></span><br></pre></td></tr></table></figure><p>在加载模型权重时，我们需要先将模型类实例化，因为该类定义了网络的结构。我们可能想把这个类的结构和模型一起保存，在这种情况下，我们可以把模型（而不是model.state_dict()）传给保存函数。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.save(model, <span class="string">&#x27;model.pth&#x27;</span>)</span><br></pre></td></tr></table></figure><p>然后我们可以像这样加载模型。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = torch.load(<span class="string">&#x27;model.pth&#x27;</span>)</span><br></pre></td></tr></table></figure><p>这种方法在序列化模型时使用Python的pickle模块，所以它在加载模型时，依赖于实际的可用的类定义。</p><h5 id="将模型导出为onnx">将模型导出为ONNX</h5><p>PyTorch也有内置的ONNX导出支持。然而，由于PyTorch执行图的动态性质，导出过程必须遍历执行图以产生持久的ONNX模型。出于这个原因，应该向导出程序传递一个适当大小的测试变量（在我们的例子中，将创建一个正确形状且值为零的tensor）。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">input_image = torch.zeros((<span class="number">1</span>,<span class="number">3</span>,<span class="number">224</span>,<span class="number">224</span>))</span><br><span class="line">onnx.export(model, input_image, <span class="string">&#x27;model.onnx&#x27;</span>)</span><br></pre></td></tr></table></figure><p>你可以用ONNX模型做很多事情，包括在不同平台和不同编程语言中运行推理。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;fashionmnist时装分类&quot;&gt;FashionMNIST时装分类&lt;/h1&gt;
&lt;p&gt;现在，我们有了一个模型和数据，是时候通过在数据上优化模型的参数来训练、验证和测试我们的模型了。训练模型是一个迭代的过程；在每个迭代中（称为epoch），模型对输出进行猜测，计算</summary>
      
    
    
    
    <category term="pytorch" scheme="http://example.com/categories/pytorch/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="pytorch" scheme="http://example.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch基础一</title>
    <link href="http://example.com/post/cb05edc5.html"/>
    <id>http://example.com/post/cb05edc5.html</id>
    <published>2021-10-25T11:29:36.000Z</published>
    <updated>2021-10-25T15:04:53.289Z</updated>
    
    <content type="html"><![CDATA[<h1 id="pytorch基础">pytorch基础</h1><h2 id="一tensor">一、tensor</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor是一种专有的数据结构，与数组和矩阵非常相似。在PyTorch中，我们使用tensor来编码一个模型的输入和输出，以及模型的参数。</span><br><span class="line">tensor类似于NumPy的ndarrays，只是tensor可以在GPU或其他硬件加速器上运行。事实上，tensor和NumPy数组通常可以共享相同的底层内存，不需要复制数据。tensor还为自动微分进行了优化</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import torch</span><br><span class="line">import numpy as np</span><br></pre></td></tr></table></figure><p>tensor可以通过各种方式进行初始化。</p><h3 id="初始化tensor">1、初始化tensor</h3><h4 id="直接初始化">1.1、直接初始化</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor可以直接从数据中创建。数据类型是自动推断出来的。</span><br><span class="line">data = [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]]</span><br><span class="line">x_data = torch.tensor(data)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[1, 2],</span></span><br><span class="line"><span class="string">        [3, 4]])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><h4 id="从numpy数组中初始化">1.2、从Numpy数组中初始化</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">可以从Numpy中创建tensor</span><br><span class="line">np_array = np.array(data)</span><br><span class="line">x_np = torch.from_numpy(np_array)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot; </span></span><br><span class="line"><span class="string">tensor([[1, 2],</span></span><br><span class="line"><span class="string">        [3, 4]], dtype=torch.int32)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><h4 id="从其他tensor中初始化">1.3、从其他tensor中初始化</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">新的tensor保留了参数tensor的属性（形状、数据类型），除非明确重写。</span><br><span class="line">x_ones = torch.ones_like(x_data) <span class="comment"># retains the properties of x_data</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Ones Tensor: \n <span class="subst">&#123;x_ones&#125;</span> \n&quot;</span>)</span><br><span class="line">x_rand = torch.rand_like(x_data, dtype=torch.<span class="built_in">float</span>) <span class="comment"># overrides the datatype of x_data</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Random Tensor: \n <span class="subst">&#123;x_rand&#125;</span> \n&quot;</span>)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot; </span></span><br><span class="line"><span class="string">Ones Tensor: </span></span><br><span class="line"><span class="string"> tensor([[1, 1],</span></span><br><span class="line"><span class="string">        [1, 1]]) </span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">Random Tensor: </span></span><br><span class="line"><span class="string"> tensor([[0.4452, 0.7225],</span></span><br><span class="line"><span class="string">        [0.6876, 0.3488]]) </span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><h4 id="使用随机数和常数初始化">1.4、使用随机数和常数初始化</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">shape是一个tensor的元组。在下面的代码中，它决定了输出tensor的维度。</span><br><span class="line">shape = (<span class="number">2</span>,<span class="number">3</span>,)</span><br><span class="line">rand_tensor = torch.rand(shape)</span><br><span class="line">ones_tensor = torch.ones(shape)</span><br><span class="line">zeros_tensor = torch.zeros(shape)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Random Tensor: \n <span class="subst">&#123;rand_tensor&#125;</span> \n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Ones Tensor: \n <span class="subst">&#123;ones_tensor&#125;</span> \n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Zeros Tensor: \n <span class="subst">&#123;zeros_tensor&#125;</span>&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Random Tensor: </span></span><br><span class="line"><span class="string"> tensor([[0.5200, 0.8270, 0.3728],</span></span><br><span class="line"><span class="string">        [0.7114, 0.4883, 0.0574]]) </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Ones Tensor: </span></span><br><span class="line"><span class="string"> tensor([[1., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 1.]]) </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Zeros Tensor: </span></span><br><span class="line"><span class="string"> tensor([[0., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 0., 0.]])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><h3 id="tensor的性质">2、tensor的性质</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tensor属性描述了它们的形状、数据类型以及存储它们的设备。</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor = torch.rand(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Shape of tensor: <span class="subst">&#123;tensor.shape&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Datatype of tensor: <span class="subst">&#123;tensor.dtype&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Device tensor is stored on: <span class="subst">&#123;tensor.device&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Shape of tensor: torch.Size([3, 4])</span></span><br><span class="line"><span class="string">Datatype of tensor: torch.float32</span></span><br><span class="line"><span class="string">Device tensor is stored on: cpu</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><h3 id="tensor的操作">3、 tensor的操作</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tensor操作，包括算术、线性代数、矩阵操作（转置、索引、切片）、采样等。</span><br><span class="line">默认情况下，tensor是在CPU上创建的。我们需要使用.to方法明确地将tensor移动到GPU上</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 如果有的话，我们把我们的tensor移到GPU上</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    tensor = tensor.to(<span class="string">&#x27;cuda&#x27;</span>)</span><br></pre></td></tr></table></figure><h4 id="索引和切分">3.1、索引和切分</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor = torch.ones(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;第一行: &#x27;</span>, tensor[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;第一列：&#x27;</span>, tensor[:, <span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;最后一列：&#x27;</span>, tensor[..., -<span class="number">1</span>])</span><br><span class="line">tensor[:,<span class="number">1</span>] = <span class="number">0</span></span><br><span class="line"><span class="built_in">print</span>(tensor)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">第一行:  tensor([1., 1., 1., 1.])</span></span><br><span class="line"><span class="string">第一列： tensor([1., 1., 1., 1.])</span></span><br><span class="line"><span class="string">最后一列： tensor([1., 1., 1., 1.])</span></span><br><span class="line"><span class="string">tensor([[1., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 0., 1., 1.]])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><h4 id="tensor拼接">3.2、tensor拼接</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t1 = torch.cat([tensor, tensor, tensor], dim=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(t1)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><h4 id="算术运算">3.3、算术运算</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 这将计算两个tensor之间的矩阵乘法，y1, y2, y3将有相同的值</span></span><br><span class="line">y1 = tensor @ tensor.T</span><br><span class="line">y2 = tensor.matmul(tensor.T)</span><br><span class="line"></span><br><span class="line">y3 = torch.rand_like(tensor)</span><br><span class="line">torch.matmul(tensor, tensor.T, out=y3)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[3., 3., 3., 3.],</span></span><br><span class="line"><span class="string">        [3., 3., 3., 3.],</span></span><br><span class="line"><span class="string">        [3., 3., 3., 3.],</span></span><br><span class="line"><span class="string">        [3., 3., 3., 3.]])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 这将计算出元素相乘的结果。z1，z2, z3有相同的值</span></span><br><span class="line">z1 = tensor * tensor</span><br><span class="line">z2 = tensor.mul(tensor)</span><br><span class="line"></span><br><span class="line">z3 = torch.rand_like(tensor)</span><br><span class="line">torch.mul(tensor, tensor, out=z3)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[1., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 0., 1., 1.]])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p><strong>单元素tensor</strong>:如果你有一个单元素tensor，例如将一个tensor的所有值总计成一个值，你可以使用item()将其变换为Python数值。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">agg = tensor.<span class="built_in">sum</span>()</span><br><span class="line">agg_item = agg.item()</span><br><span class="line"><span class="built_in">print</span>(agg_item, <span class="built_in">type</span>(agg_item))</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">12.0 &lt;class &#x27;float&#x27;&gt;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p><strong>原位操作</strong>将结果存储到操作数中的操作被称为原位操作。它们用后缀<em>来表示。例如：x.copy</em>(y), x.t_(), 将改变x。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(tensor, <span class="string">&quot;\n&quot;</span>)</span><br><span class="line">tensor.add_(<span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(tensor)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[1., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 0., 1., 1.]]) </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">tensor([[6., 5., 6., 6.],</span></span><br><span class="line"><span class="string">        [6., 5., 6., 6.],</span></span><br><span class="line"><span class="string">        [6., 5., 6., 6.],</span></span><br><span class="line"><span class="string">        [6., 5., 6., 6.]])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><h3 id="和numpy转换">4、和Numpy转换</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CPU上的张量和NumPy数组可以共享它们的底层内存位置，改变一个将改变另一个。</span><br></pre></td></tr></table></figure><h4 id="tensor转换为numpy数组">4.1、tensor转换为NumPy数组</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t = torch.ones(<span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;t: <span class="subst">&#123;t&#125;</span>&quot;</span>)</span><br><span class="line">n = t.numpy()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;n: <span class="subst">&#123;n&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">t: tensor([1., 1., 1., 1., 1.])</span></span><br><span class="line"><span class="string">n: [1. 1. 1. 1. 1.]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>tensor的变化反映在NumPy数组中。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t.add_(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;t: <span class="subst">&#123;t&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;n: <span class="subst">&#123;n&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">t: tensor([2., 2., 2., 2., 2.])</span></span><br><span class="line"><span class="string">n: [2. 2. 2. 2. 2.]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><h4 id="numpy数组转换为tensor">4.2、Numpy数组转换为tensor</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">n = np.ones(5)</span><br><span class="line">t = torch.from_numpy(n)</span><br><span class="line">np.add(n, 1, out=n)</span><br><span class="line">print(f&quot;t: &#123;t&#125;&quot;)</span><br><span class="line">print(f&quot;n: &#123;n&#125;&quot;)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)</span></span><br><span class="line"><span class="string">n: [2. 2. 2. 2. 2.]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><h2 id="二数据集和数据载入器">二、数据集和数据载入器</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PyTorch提供了两个数据模块：torch.utils.data.DataLoader和torch.utils.data.Dataset，允许你使用预先加载的数据集以及你自己的数据。Dataset存储了样本及其相应的标签，而DataLoader对Dataset包裹了一个可迭代的数据集，以便能够方便地访问这些样本。</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PyTorch库提供了一些预加载的数据集（如FashionMNIST），这些数据集是torch.utils.data.Dataset的子类，并实现了针对特定数据的功能。它们可以用来为你的模型建立原型和基准。</span><br></pre></td></tr></table></figure><h3 id="加载数据集">1、加载数据集</h3><p>从TorchVision加载Fashion-MNIST数据集的例子</p><p>Fashion-MNIST是一个由60,000个训练实例和10,000个测试实例组成的Zalando杂志中的图像数据集。每个例子包括一个28×28的灰度图像和这个图像的标签，标签是10类中的一个类别。</p><p>我们用以下参数加载FashionMNIST数据集。</p><ul><li>root是存储训练/测试数据的路径。</li><li>train指定训练或测试数据集。</li><li>download=True如果root目录下没有数据，则从互联网上下载数据。</li><li>transform和target_transform指定特征和标签的变换。</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> ToTensor</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">training_data = datasets.FashionMNIST( root=<span class="string">&quot;data&quot;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=ToTensor() )</span><br><span class="line"></span><br><span class="line">test_data = datasets.FashionMNIST( root=<span class="string">&quot;data&quot;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=ToTensor() )</span><br></pre></td></tr></table></figure><p>输出：</p><div class="figure"><img src="https://i.loli.net/2021/10/25/xwHnTrWKPsgJ82d.png" alt="image-20211025204306243" /><p class="caption">image-20211025204306243</p></div><h3 id="数据集的迭代和可视化">2、数据集的迭代和可视化</h3><p>我们可以像列表一样手动索引数据集：<code>training_data[index]</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">labels_map = &#123;</span><br><span class="line">    <span class="number">0</span>: <span class="string">&quot;T-Shirt&quot;</span>,</span><br><span class="line">    <span class="number">1</span>: <span class="string">&quot;Trouser&quot;</span>,</span><br><span class="line">    <span class="number">2</span>: <span class="string">&quot;Pullover&quot;</span>,</span><br><span class="line">    <span class="number">3</span>: <span class="string">&quot;Dress&quot;</span>,</span><br><span class="line">    <span class="number">4</span>: <span class="string">&quot;Coat&quot;</span>,</span><br><span class="line">    <span class="number">5</span>: <span class="string">&quot;Sandal&quot;</span>,</span><br><span class="line">    <span class="number">6</span>: <span class="string">&quot;Shirt&quot;</span>,</span><br><span class="line">    <span class="number">7</span>: <span class="string">&quot;Sneaker&quot;</span>,</span><br><span class="line">    <span class="number">8</span>: <span class="string">&quot;Bag&quot;</span>,</span><br><span class="line">    <span class="number">9</span>: <span class="string">&quot;Ankle Boot&quot;</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">figure = plt.figure(figsize=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line">cols, rows = <span class="number">3</span>, <span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, cols * rows + <span class="number">1</span>):</span><br><span class="line">    sample_idx = torch.randint(<span class="built_in">len</span>(training_data), size=(<span class="number">1</span>,)).item()</span><br><span class="line">    img, label = training_data[sample_idx]</span><br><span class="line">    figure.add_subplot(rows, cols, i)</span><br><span class="line">    plt.title(labels_map[label])</span><br><span class="line">    plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line">    plt.imshow(img.squeeze(), cmap=<span class="string">&quot;gray&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><div class="figure"><img src="https://i.loli.net/2021/10/25/6AIMenYh7UqzX4t.png" alt="image-20211025204330890" /><p class="caption">image-20211025204330890</p></div><h3 id="为你的文件创建一个自定义数据集">3、为你的文件创建一个自定义数据集</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">一个自定义数据集类必须实现三个函数：__init__, __len__, 和 __getitem__。</span><br></pre></td></tr></table></figure><p>FashionMNIST的图片被存储在一个目录img_dir中，它们的标签被分别存储在一个CSV文件annotations_file中。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> torchvision.io <span class="keyword">import</span> read_image</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomImageDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, annotations_file, img_dir, transform=<span class="literal">None</span>, target_transform=<span class="literal">None</span></span>):</span></span><br><span class="line">        self.img_labels = pd.read_csv(annotations_file)</span><br><span class="line">        self.img_dir = img_dir</span><br><span class="line">        self.transform = transform</span><br><span class="line">        self.target_transform = target_transform</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.img_labels)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, idx</span>):</span></span><br><span class="line">        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, <span class="number">0</span>])</span><br><span class="line">        image = read_image(img_path)</span><br><span class="line">        label = self.img_labels.iloc[idx, <span class="number">1</span>]</span><br><span class="line">        <span class="keyword">if</span> self.transform:</span><br><span class="line">            image = self.transform(image)</span><br><span class="line">        <span class="keyword">if</span> self.target_transform:</span><br><span class="line">            label = self.target_transform(label)</span><br><span class="line">        <span class="keyword">return</span> image, label</span><br></pre></td></tr></table></figure><h4 id="init">3.1、init</h4><p>在实例化数据集对象时，__init__函数运行一次。我们初始化包含图像目录、标注文件目录和变换（在下一节有更详细的介绍）。</p><p>labels.csv文件:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tshirt1.jpg, <span class="number">0</span></span><br><span class="line">tshirt2.jpg, <span class="number">0</span></span><br><span class="line">......</span><br><span class="line">ankleboot999.jpg, <span class="number">9</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, annotations_file, img_dir, transform=<span class="literal">None</span>, target_transform=<span class="literal">None</span></span>):</span></span><br><span class="line">    self.img_labels = pd.read_csv(annotations_file)</span><br><span class="line">    self.img_dir = img_dir</span><br><span class="line">    self.transform = transform</span><br><span class="line">    self.target_transform = target_transform</span><br></pre></td></tr></table></figure><h4 id="len">3.2、len</h4><p>函数 <code>__len__</code> 返回我们数据集中的样本数。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">len</span>(self.img_labels)</span><br></pre></td></tr></table></figure><h4 id="getitem">3.3、getitem</h4><p>函数 <code>__getitem__</code> 在给定的索引idx处加载并返回数据集中的一个样本。基于索引，它确定图像在磁盘上的位置，使用read_image将其变换为tensor，从self.img_labels中的csv数据中获取相应的标签，对其调用变换函数（如果适用），并在一个元组中返回tensor图像和相应标签。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, idx</span>):</span></span><br><span class="line">    img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, <span class="number">0</span>])</span><br><span class="line">    image = read_image(img_path)</span><br><span class="line">    label = self.img_labels.iloc[idx, <span class="number">1</span>]</span><br><span class="line">    <span class="keyword">if</span> self.transform:</span><br><span class="line">        image = self.transform(image)</span><br><span class="line">    <span class="keyword">if</span> self.target_transform:</span><br><span class="line">        label = self.target_transform(label)</span><br><span class="line">    <span class="keyword">return</span> image, label</span><br></pre></td></tr></table></figure><h3 id="用dataloaders准备你的数据进行训练">4、用DataLoaders准备你的数据进行训练</h3><p>数据集每次都会检索我们的数据集的特征和标签。在训练模型时，我们通常希望以 &quot;小批 &quot;的形式传递样本，在每个epoch中重新洗牌以减少模型的过拟合，并使用Python的multiprocessing来加快数据的检索速度。</p><p>DataLoader是一个可迭代的对象，它用一个简单的API为我们抽象了这种复杂性。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">train_dataloader = DataLoader(training_data, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_dataloader = DataLoader(test_data, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h3 id="通过dataloader进行迭代">5、通过DataLoader进行迭代</h3><p>我们已经将该数据集加载到DataLoader中，可以根据需要迭代数据集。下面的每次迭代都会返回一批train_features和train_labels（分别包含batch_size=64的特征和标签）。因为我们指定了shuffle=True，所以在我们迭代完所有的批次后，数据会被洗牌。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Display image and label.</span></span><br><span class="line">train_features, train_labels = <span class="built_in">next</span>(<span class="built_in">iter</span>(train_dataloader))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Feature batch shape: <span class="subst">&#123;train_features.size()&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Labels batch shape: <span class="subst">&#123;train_labels.size()&#125;</span>&quot;</span>)</span><br><span class="line">img = train_features[<span class="number">0</span>].squeeze()</span><br><span class="line">label = train_labels[<span class="number">0</span>]</span><br><span class="line">plt.imshow(img, cmap=<span class="string">&quot;gray&quot;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Label: <span class="subst">&#123;label&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><div class="figure"><img src="https://i.loli.net/2021/10/25/iQXM9DvkZs6HU1c.png" alt="image-20211025210207793.png" /><p class="caption">image-20211025210207793.png</p></div><h2 id="三变换">三、变换</h2><p>数据并不总是以训练机器学习算法所需的最终处理形式出现。我们使用变换来对数据进行一些操作，使其适合训练。</p><p>所有的TorchVision数据集都有两个参数--用于修改特征的transform和用于修改标签的target_transform--它们接受包含变换逻辑的调用语句。torchvision.transforms模块提供了几个常用的变换，开箱即用。</p><p>FashionMNIST的特征是PIL图像格式的，标签是整数。对于训练，我们需要将特征作为归一化的tensor，将标签作为one-hot编码的tensor。为了进行这些变换，我们使用ToTensor和Lambda。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> ToTensor, Lambda</span><br><span class="line"></span><br><span class="line">ds = datasets.FashionMNIST(</span><br><span class="line">    root=<span class="string">&quot;data&quot;</span>,</span><br><span class="line">    train=<span class="literal">True</span>,</span><br><span class="line">    download=<span class="literal">True</span>,</span><br><span class="line">    transform=ToTensor(),</span><br><span class="line">    target_transform=Lambda(<span class="keyword">lambda</span> y: torch.zeros(<span class="number">10</span>, dtype=torch.<span class="built_in">float</span>).scatter_(<span class="number">0</span>, torch.tensor(y), value=<span class="number">1</span>))</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p><strong>ToTensor()</strong></p><p>ToTensor将PIL图像或NumPy的ndarray变换为FloatTensor，并将图像的像素亮度值按[0., 1.]的范围进行缩放。</p><p><strong>Lambda变换</strong></p><p>Lambda变换应用任何用户定义的Lambda函数。在这里，我们定义了一个函数，把整数变成一个one-hot的tensor。它首先创建一个大小为10（我们数据集中的标签数量），值为0的tensor，并调用scatter_，在标签y给出的索引上分配一个value=1。</p><h2 id="四搭建神经网络">四、搭建神经网络</h2><p>神经网络由对数据进行操作的层/模块组成。torch.nn命名空间提供了您构建自己的神经网络所需的所有组件。PyTorch中的每个模块都是nn.Module的子类。一个神经网络本身就是一个由其他模块（层）组成的模块。这种嵌套结构允许轻松构建和管理复杂的架构。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br></pre></td></tr></table></figure><h3 id="获取训练的设备">1、获取训练的设备</h3><p>我们希望能够在像GPU这样的硬件加速器上训练我们的模型，如果它是可用的。让我们检查一下torch.cuda是否可用，否则我们继续使用CPU。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">device = <span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Using &#123;&#125; device&#x27;</span>.<span class="built_in">format</span>(device))</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Using cuda device</span><br></pre></td></tr></table></figure><h3 id="定义模型类">2、定义模型类</h3><p>我们通过子类化 nn.Module 来定义我们的神经网络，并在 <code>__init__</code>中初始化神经网络层。每个 nn.Module 子类都在 forward 方法中实现了对输入数据的操作。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NeuralNetwork</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(NeuralNetwork, self).__init__()</span><br><span class="line">        self.flatten = nn.Flatten()</span><br><span class="line">        self.linear_relu_stack = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">28</span>*<span class="number">28</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">10</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        logits = self.linear_relu_stack(x)</span><br><span class="line">        <span class="keyword">return</span> logits</span><br></pre></td></tr></table></figure><p>我们创建一个NeuralNetwork的实例，并将其移动到设备上，并打印其结构。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = NeuralNetwork().to(device)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">NeuralNetwork(</span></span><br><span class="line"><span class="string">  (flatten): Flatten(start_dim=1, end_dim=-1)</span></span><br><span class="line"><span class="string">  (linear_relu_stack): Sequential(</span></span><br><span class="line"><span class="string">    (0): Linear(in_features=784, out_features=512, bias=True)</span></span><br><span class="line"><span class="string">    (1): ReLU()</span></span><br><span class="line"><span class="string">    (2): Linear(in_features=512, out_features=512, bias=True)</span></span><br><span class="line"><span class="string">    (3): ReLU()</span></span><br><span class="line"><span class="string">    (4): Linear(in_features=512, out_features=10, bias=True)</span></span><br><span class="line"><span class="string">  )</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>为了使用这个模型，我们把输入数据传给它。这就执行了模型的forward函数，以及一些后台操作。</p><p>在输入数据上调用模型会返回一个10维的tensor，其中包含每个类别的原始预测值。我们通过nn.Softmax模块的一个实例来获得预测概率。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.rand(<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>, device=device)</span><br><span class="line">logits = model(X)</span><br><span class="line">pred_probab = nn.Softmax(dim=<span class="number">1</span>)(logits)</span><br><span class="line">y_pred = pred_probab.argmax(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Predicted class: <span class="subst">&#123;y_pred&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Predicted class: tensor([1], device=&#x27;cuda:0&#x27;)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><h3 id="模型层">3、模型层</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">input_image = torch.rand(<span class="number">3</span>,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line"><span class="built_in">print</span>(input_image.size())</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">torch.Size([3, 28, 28])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p><strong>nn.Flatten</strong></p><p>我们初始化nn.Flatten层，将每个28x28的二维图像变换为784个像素值的连续数组（minibatch的维度（dim=0）被保持）。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">flatten = nn.Flatten()</span><br><span class="line">flat_image = flatten(input_image)</span><br><span class="line"><span class="built_in">print</span>(flat_image.size())</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">torch.Size([3, 784])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p><strong>nn.Linear</strong></p><p>线性层是一个模块，使用其存储的权重和偏置对输入进行线性变换。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">layer1 = nn.Linear(in_features=<span class="number">28</span>*<span class="number">28</span>, out_features=<span class="number">20</span>)</span><br><span class="line">hidden1 = layer1(flat_image)</span><br><span class="line"><span class="built_in">print</span>(hidden1.size())</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">torch.Size([3, 20])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p><strong>nn.ReLU</strong></p><p>非线性激活是在模型的输入和输出之间建立复杂的映射关系。它们被应用在线性变换之后，以引入非线性，帮助神经网络学习各种各样的函数。</p><p>在这个模型中，我们在线性层之间使用了nn.ReLU，但还有其他激活函数可以在你的模型中引入非线性。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Before ReLU: <span class="subst">&#123;hidden1&#125;</span>\n\n&quot;</span>)</span><br><span class="line">hidden1 = nn.ReLU()(hidden1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;After ReLU: <span class="subst">&#123;hidden1&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Before ReLU: tensor([[-0.2050, -0.2515, -0.1101, -0.5910,  0.0241, -0.4991, -0.0064,  0.2330,</span></span><br><span class="line"><span class="string">          0.2104, -0.2930, -0.4654, -0.6682,  0.0789,  0.2525,  0.3306, -0.4441,</span></span><br><span class="line"><span class="string">         -0.1403,  0.2946,  0.2446, -0.6398],</span></span><br><span class="line"><span class="string">        [-0.2331, -0.3806, -0.2077, -0.7201, -0.2562, -0.4168, -0.0570,  0.0775,</span></span><br><span class="line"><span class="string">          0.1734, -0.0644, -0.2212, -0.4178, -0.1430,  0.3815,  0.3207, -0.4322,</span></span><br><span class="line"><span class="string">         -0.2514, -0.0818,  0.0162, -0.7603],</span></span><br><span class="line"><span class="string">        [ 0.1153, -0.3066,  0.6950, -0.4477,  0.0225, -0.3306,  0.2582, -0.0583,</span></span><br><span class="line"><span class="string">          0.3550, -0.1699, -0.5302, -0.6426, -0.3060,  0.2715,  0.0820, -0.2693,</span></span><br><span class="line"><span class="string">         -0.3574,  0.1241,  0.3639, -0.9418]], grad_fn=&lt;AddmmBackward&gt;)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">After ReLU: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0241, 0.0000, 0.0000, 0.2330, 0.2104,</span></span><br><span class="line"><span class="string">         0.0000, 0.0000, 0.0000, 0.0789, 0.2525, 0.3306, 0.0000, 0.0000, 0.2946,</span></span><br><span class="line"><span class="string">         0.2446, 0.0000],</span></span><br><span class="line"><span class="string">        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0775, 0.1734,</span></span><br><span class="line"><span class="string">         0.0000, 0.0000, 0.0000, 0.0000, 0.3815, 0.3207, 0.0000, 0.0000, 0.0000,</span></span><br><span class="line"><span class="string">         0.0162, 0.0000],</span></span><br><span class="line"><span class="string">        [0.1153, 0.0000, 0.6950, 0.0000, 0.0225, 0.0000, 0.2582, 0.0000, 0.3550,</span></span><br><span class="line"><span class="string">         0.0000, 0.0000, 0.0000, 0.0000, 0.2715, 0.0820, 0.0000, 0.0000, 0.1241,</span></span><br><span class="line"><span class="string">         0.3639, 0.0000]], grad_fn=&lt;ReluBackward0&gt;)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p><strong>nn.Sequential</strong></p><p>nn.Sequential是一个有序模块的容器。数据以定义的顺序通过所有的模块。你可以使用 序列容器来组建一个快速的网络，如seq_modules：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">seq_modules = nn.Sequential(</span><br><span class="line">    flatten,</span><br><span class="line">    layer1,</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">20</span>, <span class="number">10</span>)</span><br><span class="line">)</span><br><span class="line">input_image = torch.rand(<span class="number">3</span>,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line">logits = seq_modules(input_image)</span><br></pre></td></tr></table></figure><p><strong>nn.Softmax</strong></p><p>神经网络的最后一个线性层返回logits--[-infty, infty]中的原始值--并传递给nn.Softmax模块。对数被缩放到数值区间[0, 1]，代表模型对每个类别的预测概率。 dim参数表示数值必须和为1的维度。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">softmax = nn.Softmax(dim=<span class="number">1</span>)</span><br><span class="line">pred_probab = softmax(logits)</span><br></pre></td></tr></table></figure><h3 id="模型参数">4、模型参数</h3><p>神经网络中的许多层都是参数化的，也就是说，层相关的权重和偏置在训练中被优化。nn.Module的子类会自动跟踪你的模型对象中定义的所有字段，并使用你的模型的 parameters() 或 named_parameters() 方法访问所有参数。</p><p>在这个例子中，我们遍历每个参数，并打印其大小和预览其值。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Model structure: &quot;</span>, model, <span class="string">&quot;\n\n&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Layer: <span class="subst">&#123;name&#125;</span> | Size: <span class="subst">&#123;param.size()&#125;</span> | Values : <span class="subst">&#123;param[:<span class="number">2</span>]&#125;</span> \n&quot;</span>)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Model structure:  NeuralNetwork(</span><br><span class="line">  (flatten): Flatten(start_dim=<span class="number">1</span>, end_dim=-<span class="number">1</span>)</span><br><span class="line">  (linear_relu_stack): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Linear(in_features=<span class="number">784</span>, out_features=<span class="number">512</span>, bias=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">1</span>): ReLU()</span><br><span class="line">    (<span class="number">2</span>): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">512</span>, bias=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">3</span>): ReLU()</span><br><span class="line">    (<span class="number">4</span>): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">10</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  )</span><br><span class="line">) </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Layer: linear_relu_stack<span class="number">.0</span>.weight | Size: torch.Size([<span class="number">512</span>, <span class="number">784</span>]) | Values : tensor([[-<span class="number">0.0331</span>,  <span class="number">0.0108</span>, -<span class="number">0.0115</span>,  ...,  <span class="number">0.0196</span>,  <span class="number">0.0255</span>,  <span class="number">0.0289</span>],</span><br><span class="line">        [-<span class="number">0.0168</span>,  <span class="number">0.0280</span>, -<span class="number">0.0132</span>,  ..., -<span class="number">0.0103</span>, -<span class="number">0.0099</span>, -<span class="number">0.0121</span>]],</span><br><span class="line">       device=<span class="string">&#x27;cuda:0&#x27;</span>, grad_fn=&lt;SliceBackward&gt;) </span><br><span class="line"></span><br><span class="line">Layer: linear_relu_stack<span class="number">.0</span>.bias | Size: torch.Size([<span class="number">512</span>]) | Values : tensor([-<span class="number">0.0108</span>,  <span class="number">0.0137</span>], device=<span class="string">&#x27;cuda:0&#x27;</span>, grad_fn=&lt;SliceBackward&gt;) </span><br><span class="line"></span><br><span class="line">Layer: linear_relu_stack<span class="number">.2</span>.weight | Size: torch.Size([<span class="number">512</span>, <span class="number">512</span>]) | Values : tensor([[ <span class="number">0.0048</span>,  <span class="number">0.0112</span>,  <span class="number">0.0430</span>,  ..., -<span class="number">0.0423</span>, -<span class="number">0.0438</span>,  <span class="number">0.0150</span>],</span><br><span class="line">        [-<span class="number">0.0213</span>, -<span class="number">0.0016</span>, -<span class="number">0.0128</span>,  ...,  <span class="number">0.0230</span>,  <span class="number">0.0200</span>, -<span class="number">0.0120</span>]],</span><br><span class="line">       device=<span class="string">&#x27;cuda:0&#x27;</span>, grad_fn=&lt;SliceBackward&gt;) </span><br><span class="line"></span><br><span class="line">Layer: linear_relu_stack<span class="number">.2</span>.bias | Size: torch.Size([<span class="number">512</span>]) | Values : tensor([<span class="number">0.0374</span>, <span class="number">0.0331</span>], device=<span class="string">&#x27;cuda:0&#x27;</span>, grad_fn=&lt;SliceBackward&gt;) </span><br><span class="line"></span><br><span class="line">Layer: linear_relu_stack<span class="number">.4</span>.weight | Size: torch.Size([<span class="number">10</span>, <span class="number">512</span>]) | Values : tensor([[ <span class="number">0.0318</span>, -<span class="number">0.0424</span>,  <span class="number">0.0038</span>,  ...,  <span class="number">0.0241</span>, -<span class="number">0.0187</span>, -<span class="number">0.0105</span>],</span><br><span class="line">        [ <span class="number">0.0004</span>,  <span class="number">0.0132</span>,  <span class="number">0.0343</span>,  ..., -<span class="number">0.0182</span>,  <span class="number">0.0013</span>, -<span class="number">0.0020</span>]],</span><br><span class="line">       device=<span class="string">&#x27;cuda:0&#x27;</span>, grad_fn=&lt;SliceBackward&gt;) </span><br><span class="line"></span><br><span class="line">Layer: linear_relu_stack<span class="number">.4</span>.bias | Size: torch.Size([<span class="number">10</span>]) | Values : tensor([<span class="number">0.0109</span>, <span class="number">0.0427</span>], device=<span class="string">&#x27;cuda:0&#x27;</span>, grad_fn=&lt;SliceBackward&gt;) </span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="五使用torch.autograd进行自动微分">五、使用torch.autograd进行自动微分</h2><p>在训练神经网络时，最常使用的算法是反向传播算法。在这种算法中，参数（模型权重）是根据损失函数相对于给定参数的梯度来调整的。</p><p>为了计算这些梯度，PyTorch有一个内置的微分引擎，叫做torch.autograd。它支持对任何计算图的梯度进行自动计算。</p><p>考虑最简单的单层神经网络，输入x，参数w和b，以及一些损失函数。它可以在PyTorch中以如下方式定义：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.ones(<span class="number">5</span>)  <span class="comment"># input tensor</span></span><br><span class="line">y = torch.zeros(<span class="number">3</span>)  <span class="comment"># expected output</span></span><br><span class="line">w = torch.randn(<span class="number">5</span>, <span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.randn(<span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">z = torch.matmul(x, w)+b</span><br><span class="line">loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)</span><br></pre></td></tr></table></figure><h3 id="tensor函数和计算图">1、tensor、函数和计算图</h3><div class="figure"><img src="https://i.loli.net/2021/10/25/iulTGxK2vWg1V97.png" alt="计算图" /><p class="caption">计算图</p></div><p>在这个网络中，w和b是参数，我们需要进行优化。因此，我们需要能够计算损失函数相对于这些变量的梯度。为了做到这一点，我们设置了这些tensor的 requires_grad 属性。</p><p>注意:你可以在创建tensor时设置requires_grad的值，或者在以后使用x.requires_grad_(True)方法来设置。</p><p>我们应用于tensor来构建计算图的函数实际上是一个Function类的对象。这个对象知道如何在forward方向上计算函数，也知道如何在后向传播步骤中计算其导数。对后向传播函数的引用被存储在tensor的grad_fn属性中。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Gradient function for z =&#x27;</span>, z.grad_fn)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Gradient function for loss =&#x27;</span>, loss.grad_fn)</span><br></pre></td></tr></table></figure><h3 id="计算梯度">2、计算梯度</h3><p>为了优化神经网络中的参数权重，我们需要计算损失函数相对于参数的导数</p><p>为了计算这些导数，我们调用 loss.backward()，然后从 w.grad 和 b.grad 中获取数值。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loss.backward()</span><br><span class="line"><span class="built_in">print</span>(w.grad)</span><br><span class="line"><span class="built_in">print</span>(b.grad)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[0.3256, 0.2634, 0.1137],</span></span><br><span class="line"><span class="string">        [0.3256, 0.2634, 0.1137],</span></span><br><span class="line"><span class="string">        [0.3256, 0.2634, 0.1137],</span></span><br><span class="line"><span class="string">        [0.3256, 0.2634, 0.1137],</span></span><br><span class="line"><span class="string">        [0.3256, 0.2634, 0.1137]])</span></span><br><span class="line"><span class="string">tensor([0.3256, 0.2634, 0.1137])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><ul><li>我们只能获得计算图的叶子节点的grad属性，这些节点的requires_grad属性设置为True。对于我们图中的所有其他节点，梯度将不可用。</li><li>出于性能方面的考虑，我们只能在一个给定的图上使用一次backward来进行梯度计算。如果我们需要在同一个图上进行多次backward调用，我们需要在backward调用中传递 retain_graph=True。</li></ul><h3 id="禁止梯度跟踪">3、禁止梯度跟踪</h3><p>默认情况下，所有带有require_grad=True的tensor都在跟踪它们的计算历史并支持梯度计算。然而，在某些情况下，我们不需要这样做，例如，当我们已经训练了模型，只是想把它应用于一些输入数据，也就是说，我们只想通过网络进行前向计算。我们可以通过用torch.no_grad()块包围我们的计算代码来停止跟踪计算。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">z = torch.matmul(x, w)+b</span><br><span class="line"><span class="built_in">print</span>(z.requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    z = torch.matmul(x, w)+b</span><br><span class="line"><span class="built_in">print</span>(z.requires_grad)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">True</span><br><span class="line">False</span><br></pre></td></tr></table></figure><p>另一种实现相同结果的方法是在tensor上使用detach()方法。</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">z = torch.matmul(x, w)+b</span><br><span class="line">z_det = z.detach()</span><br><span class="line">print(z_det.requires_grad)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">False</span><br></pre></td></tr></table></figure><p>你可能想禁用梯度跟踪，原因如下：</p><ul><li>将神经网络中的一些参数标记为冻结参数。这是对预训练的网络进行微调的一个非常常见的情况。</li><li>当你只做前向传递时，为了加快计算速度，对不跟踪梯度的tensor的计算会更有效率。</li></ul><h3 id="更多">更多</h3><p>从概念上讲，autograd在一个由Function对象组成的有向无环图（DAG）中保存了数据（tensor）和所有执行的操作（以及产生的新tensor）的记录。在这个DAG中，叶子是输入tensor，根部是输出tensor。通过追踪这个图从根到叶，你可以使用链式规则自动计算梯度。</p><p>在一个前向传递中，autograd同时做两件事。</p><ul><li>运行请求的操作，计算出一个结果tensor。</li><li>在DAG中维护该操作的梯度函数。</li></ul><p>当在DAG根上调用.backward()时，后向传递开始了。</p><ul><li>计算每个.grad_fn的梯度。</li><li>将它们累积到各自tensor的 .grad 属性中</li><li>使用链式规则，一直传播到叶子tensor。</li></ul><p>注意:在PyTorch中，DAG是动态的。需要注意的是，图是从头开始重新创建的；在每次调用.backward()后，autograd开始填充一个新的图。这正是允许你在模型中使用控制流语句的原因；如果需要，你可以在每次迭代时改变形状、大小和操作。</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line">from torchvision import datasets</span><br><span class="line">from torchvision.transforms import ToTensor, Lambda</span><br><span class="line"></span><br><span class="line">training_data = datasets.FashionMNIST(</span><br><span class="line">    root=&quot;data&quot;,</span><br><span class="line">    train=True,</span><br><span class="line">    download=True,</span><br><span class="line">    transform=ToTensor()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">test_data = datasets.FashionMNIST(</span><br><span class="line">    root=&quot;data&quot;,</span><br><span class="line">    train=False,</span><br><span class="line">    download=True,</span><br><span class="line">    transform=ToTensor()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">train_dataloader = DataLoader(training_data, batch_size=64)</span><br><span class="line">test_dataloader = DataLoader(test_data, batch_size=64)</span><br><span class="line"></span><br><span class="line">class NeuralNetwork(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(NeuralNetwork, self).__init__()</span><br><span class="line">        self.flatten = nn.Flatten()</span><br><span class="line">        self.linear_relu_stack = nn.Sequential(</span><br><span class="line">            nn.Linear(28*28, 512),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(512, 512),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(512, 10),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        logits = self.linear_relu_stack(x)</span><br><span class="line">        return logits</span><br><span class="line"></span><br><span class="line">model = NeuralNetwork()</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;pytorch基础&quot;&gt;pytorch基础&lt;/h1&gt;
&lt;h2 id=&quot;一tensor&quot;&gt;一、tensor&lt;/h2&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span c</summary>
      
    
    
    
    <category term="pytorch" scheme="http://example.com/categories/pytorch/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="pytorch" scheme="http://example.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch深度学习三</title>
    <link href="http://example.com/post/a0d5236b.html"/>
    <id>http://example.com/post/a0d5236b.html</id>
    <published>2021-10-25T11:00:09.000Z</published>
    <updated>2021-10-25T11:26:02.033Z</updated>
    
    <content type="html"><![CDATA[<h1 id="pytorch">pytorch</h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">使用pytorch实现线性模型</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure><p>数据</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># x和y数据必须是矩阵，所以如[1.0]</span></span><br><span class="line">x_data = torch.Tensor([[<span class="number">1.0</span>], [<span class="number">2.0</span>], [<span class="number">3.0</span>]])</span><br><span class="line">y_data = torch.Tensor([[<span class="number">2.0</span>], [<span class="number">4.0</span>], [<span class="number">6.0</span>]])</span><br><span class="line">loss_list = []</span><br></pre></td></tr></table></figure><p>模型</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearModel</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(LinearModel, self).__init__()</span><br><span class="line">        <span class="comment"># 实例化一个linear对象</span></span><br><span class="line">        self.linear = nn.Linear(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># 可调用的对象</span></span><br><span class="line">        y_pred = self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line">    </span><br><span class="line">model = LinearModel()</span><br></pre></td></tr></table></figure><p>优化器和损失函数</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># criterion = torch.nn.MSELoss(size_average=False)</span></span><br><span class="line">criterion = torch.nn.MSELoss(reduction = <span class="string">&#x27;sum&#x27;</span>) <span class="comment">#求和</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr = <span class="number">0.01</span>) <span class="comment">#学习率lr = 0.01</span></span><br></pre></td></tr></table></figure><p>训练</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500</span>):</span><br><span class="line">    y_pred = model(x_data)</span><br><span class="line">    loss = criterion(y_pred, y_data)</span><br><span class="line">    <span class="comment"># 打印loss对象会自动调用__str__(),不会产生计算图</span></span><br><span class="line">    <span class="built_in">print</span>(epoch, loss.item())</span><br><span class="line">    loss_list.append(loss.item())</span><br><span class="line">    <span class="comment">#每次需要归零，不然梯度会积累</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="comment">#参数更新</span></span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure><p>结果：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span> <span class="number">8.456552505493164</span></span><br><span class="line"><span class="number">1</span> <span class="number">4.0267653465271</span></span><br><span class="line"><span class="number">2</span> <span class="number">2.050983428955078</span></span><br><span class="line"><span class="number">3</span> <span class="number">1.167707085609436</span></span><br><span class="line"><span class="number">4</span> <span class="number">0.7708380222320557</span></span><br><span class="line"></span><br><span class="line">...................</span><br><span class="line">...................</span><br><span class="line">...................</span><br><span class="line"></span><br><span class="line"><span class="number">491</span> <span class="number">0.0003971618425566703</span></span><br><span class="line"><span class="number">492</span> <span class="number">0.00039145682239905</span></span><br><span class="line"><span class="number">493</span> <span class="number">0.00038583995774388313</span></span><br><span class="line"><span class="number">494</span> <span class="number">0.0003802851715590805</span></span><br><span class="line"><span class="number">495</span> <span class="number">0.0003748224989976734</span></span><br><span class="line"><span class="number">496</span> <span class="number">0.00036944064777344465</span></span><br><span class="line"><span class="number">497</span> <span class="number">0.00036413324414752424</span></span><br><span class="line"><span class="number">498</span> <span class="number">0.0003588950785342604</span></span><br><span class="line"><span class="number">499</span> <span class="number">0.0003537364536896348</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 画图</span></span><br><span class="line">plt.plot(<span class="built_in">range</span>(<span class="number">500</span>), loss_list)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;epoch&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><div class="figure"><img src="https://i.loli.net/2021/10/25/cZYV6AWHdvORPFn.png" alt="image-20211025192214494" /><p class="caption">image-20211025192214494</p></div><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;w = &#x27;</span>, model.linear.weight.item())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;b = &#x27;</span>, model.linear.bias.item())</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">w =  <span class="number">1.9874792098999023</span></span><br><span class="line">b =  <span class="number">0.02846282161772251</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Test </span></span><br><span class="line"><span class="comment"># 输入是1×1矩阵，输出也是1×1矩阵</span></span><br><span class="line">x_test = torch.Tensor([[<span class="number">2.0</span>]]) </span><br><span class="line">y_test = model(x_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;y_pred = &#x27;</span>, y_test.data)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y_pred =  tensor([[<span class="number">4.0034</span>]])</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;pytorch&quot;&gt;pytorch&lt;/h1&gt;
&lt;figure class=&quot;highlight plaintext&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;使用pytorch实现线性模型&lt;/span</summary>
      
    
    
    
    <category term="pytorch" scheme="http://example.com/categories/pytorch/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="pytorch" scheme="http://example.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch深度学习二</title>
    <link href="http://example.com/post/e289b566.html"/>
    <id>http://example.com/post/e289b566.html</id>
    <published>2021-10-25T10:23:44.000Z</published>
    <updated>2021-10-25T10:58:15.031Z</updated>
    
    <content type="html"><![CDATA[<h1 id="pytorch">pytorch</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">使用梯度下降进行求解</span><br></pre></td></tr></table></figure><div class="figure"><img src="https://i.loli.net/2021/10/25/8clrVKoF6udLbWs.png" alt="image-20211025185053738" /><p class="caption">image-20211025185053738</p></div><div class="figure"><img src="https://i.loli.net/2021/10/25/9z8ayRL1d32GZeu.png" alt="image-20211025185107672" /><p class="caption">image-20211025185107672</p></div><div class="figure"><img src="https://i.loli.net/2021/10/25/TLmfhDd1PsZw7JV.png" alt="image-20211025185123456" /><p class="caption">image-20211025185123456</p></div><div class="figure"><img src="https://i.loli.net/2021/10/25/n3rb2o6QsMfCk78.png" alt="image-20211025185137466" /><p class="caption">image-20211025185137466</p></div><div class="figure"><img src="https://i.loli.net/2021/10/25/jJsVux5qO2bkcaS.png" alt="image-20211025185150736" /><p class="caption">image-20211025185150736</p></div><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>]</span><br><span class="line">loss_list = []</span><br><span class="line">w = <span class="number">1.0</span> <span class="comment">#权重初始化为1</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 向前传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x * w</span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">x_data, y_data</span>):</span> <span class="comment">#x_data [1.0, 2.0, 3.0] y_data [2.0, 4.0, 6.0]</span></span><br><span class="line">    loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(x_data, y_data):</span><br><span class="line">        y_predict = forward(x)</span><br><span class="line">        loss += (y_predict - y) ** <span class="number">2</span>  <span class="comment">#总loss</span></span><br><span class="line">    <span class="keyword">return</span> loss / <span class="built_in">len</span>(x_data)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 求梯度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span>(<span class="params">x_data, y_data</span>):</span></span><br><span class="line">    grad = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(x_data, y_data):</span><br><span class="line">        <span class="comment"># 向前传播</span></span><br><span class="line">        temp = forward(x)</span><br><span class="line">        <span class="comment"># 求梯度</span></span><br><span class="line">        grad += <span class="number">2</span> * x *(temp - y)</span><br><span class="line">    <span class="keyword">return</span> grad / <span class="built_in">len</span>(x_data)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># train</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="comment"># 求损失值</span></span><br><span class="line">    loss_val = loss(x_data, y_data)</span><br><span class="line">    loss_list.append(loss_val)</span><br><span class="line">    <span class="comment"># 求梯度值</span></span><br><span class="line">    grad_val = gradient(x_data, y_data)</span><br><span class="line">    <span class="comment"># 更新参数w</span></span><br><span class="line">    w -= <span class="number">0.01</span> *grad_val</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Epoch: &quot;</span>, epoch, <span class="string">&quot;w = &quot;</span>, w, <span class="string">&quot;loss = &quot;</span>, loss_val)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 绘图</span></span><br><span class="line">plt.plot(<span class="built_in">range</span>(<span class="number">100</span>), loss_list[<span class="number">0</span>:<span class="number">100</span>])</span><br><span class="line">plt.ylabel(<span class="string">&quot;loss&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Epoch&quot;</span>)</span><br><span class="line">plt.show()  </span><br></pre></td></tr></table></figure><div class="figure"><img src="https://i.loli.net/2021/10/25/LxF4phI2AzJckHM.png" alt="image-20211025183650352" /><p class="caption">image-20211025183650352</p></div><p>预测</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y_predict = []</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> x_data:</span><br><span class="line">    temp = forward(x)</span><br><span class="line">    y_predict.append(temp)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(y_predict)</span><br><span class="line"><span class="comment"># 绘图</span></span><br><span class="line">plt.plot(x_data, y_predict,<span class="string">&#x27;o&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;y_predict&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;x_data&quot;</span>)</span><br><span class="line">plt.show()  </span><br></pre></td></tr></table></figure><div class="figure"><img src="https://i.loli.net/2021/10/25/MhnPQLv5yYDtfe4.png" alt="image-20211025184841418" /><p class="caption">image-20211025184841418</p></div>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;pytorch&quot;&gt;pytorch&lt;/h1&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;使用梯度下降进行求解&lt;/span&gt;&lt;br&gt;&lt;/pre</summary>
      
    
    
    
    <category term="pytorch" scheme="http://example.com/categories/pytorch/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="pytorch" scheme="http://example.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch深度学习一</title>
    <link href="http://example.com/post/d9099bcf.html"/>
    <id>http://example.com/post/d9099bcf.html</id>
    <published>2021-10-25T07:00:58.000Z</published>
    <updated>2021-10-25T09:27:35.179Z</updated>
    
    <content type="html"><![CDATA[<h1 id="pytorch">pytorch</h1><h3 id="线性模型">1、线性模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x * w</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    y_pred = forward(x)</span><br><span class="line">    <span class="keyword">return</span> (y_pred - y) * (y_pred - y)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 保存权重</span></span><br><span class="line">w_list = []</span><br><span class="line"><span class="comment"># 保存权重的损失函数值</span></span><br><span class="line">loss_list = []</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 损失函数MSE</span></span><br><span class="line"><span class="keyword">for</span> w <span class="keyword">in</span> np.arange(<span class="number">0.0</span>, <span class="number">4.1</span>, <span class="number">0.1</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;w = &#x27;</span>, w)</span><br><span class="line">    loss_sum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x_val, y_val <span class="keyword">in</span> <span class="built_in">zip</span>(x_data, y_data):</span><br><span class="line">        <span class="comment"># 为了打印y预测值，其实loss里也计算了</span></span><br><span class="line">        y_pred_val = forward(x_val)</span><br><span class="line">        loss_val = loss(x_val, y_val)</span><br><span class="line">        loss_sum += loss_val</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;\t&#x27;</span>, x_val, y_val,</span><br><span class="line">              y_pred_val, loss_val)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;loss = &#x27;</span>, loss_sum / <span class="number">3</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;☆&#x27;</span>*<span class="number">55</span>)</span><br><span class="line">    w_list.append(w)</span><br><span class="line">    loss_list.append(loss_sum / <span class="number">3</span>)</span><br></pre></td></tr></table></figure><p>结果：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">w =  0.0</span><br><span class="line"> 1.0 2.0 0.0 4.0</span><br><span class="line"> 2.0 4.0 0.0 16.0</span><br><span class="line"> 3.0 6.0 0.0 36.0</span><br><span class="line">loss =  18.666666666666668</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  0.1</span><br><span class="line"> 1.0 2.0 0.1 3.61</span><br><span class="line"> 2.0 4.0 0.2 14.44</span><br><span class="line"> 3.0 6.0 0.30000000000000004 32.49</span><br><span class="line">loss =  16.846666666666668</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  0.2</span><br><span class="line"> 1.0 2.0 0.2 3.24</span><br><span class="line"> 2.0 4.0 0.4 12.96</span><br><span class="line"> 3.0 6.0 0.6000000000000001 29.160000000000004</span><br><span class="line">loss =  15.120000000000003</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  0.30000000000000004</span><br><span class="line"> 1.0 2.0 0.30000000000000004 2.8899999999999997</span><br><span class="line"> 2.0 4.0 0.6000000000000001 11.559999999999999</span><br><span class="line"> 3.0 6.0 0.9000000000000001 26.009999999999998</span><br><span class="line">loss =  13.486666666666665</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  0.4</span><br><span class="line"> 1.0 2.0 0.4 2.5600000000000005</span><br><span class="line"> 2.0 4.0 0.8 10.240000000000002</span><br><span class="line"> 3.0 6.0 1.2000000000000002 23.04</span><br><span class="line">loss =  11.946666666666667</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  0.5</span><br><span class="line"> 1.0 2.0 0.5 2.25</span><br><span class="line"> 2.0 4.0 1.0 9.0</span><br><span class="line"> 3.0 6.0 1.5 20.25</span><br><span class="line">loss =  10.5</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  0.6000000000000001</span><br><span class="line"> 1.0 2.0 0.6000000000000001 1.9599999999999997</span><br><span class="line"> 2.0 4.0 1.2000000000000002 7.839999999999999</span><br><span class="line"> 3.0 6.0 1.8000000000000003 17.639999999999993</span><br><span class="line">loss =  9.146666666666663</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  0.7000000000000001</span><br><span class="line"> 1.0 2.0 0.7000000000000001 1.6899999999999995</span><br><span class="line"> 2.0 4.0 1.4000000000000001 6.759999999999998</span><br><span class="line"> 3.0 6.0 2.1 15.209999999999999</span><br><span class="line">loss =  7.886666666666666</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  0.8</span><br><span class="line"> 1.0 2.0 0.8 1.44</span><br><span class="line"> 2.0 4.0 1.6 5.76</span><br><span class="line"> 3.0 6.0 2.4000000000000004 12.959999999999997</span><br><span class="line">loss =  6.719999999999999</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  0.9</span><br><span class="line"> 1.0 2.0 0.9 1.2100000000000002</span><br><span class="line"> 2.0 4.0 1.8 4.840000000000001</span><br><span class="line"> 3.0 6.0 2.7 10.889999999999999</span><br><span class="line">loss =  5.646666666666666</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  1.0</span><br><span class="line"> 1.0 2.0 1.0 1.0</span><br><span class="line"> 2.0 4.0 2.0 4.0</span><br><span class="line"> 3.0 6.0 3.0 9.0</span><br><span class="line">loss =  4.666666666666667</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  1.1</span><br><span class="line"> 1.0 2.0 1.1 0.8099999999999998</span><br><span class="line"> 2.0 4.0 2.2 3.2399999999999993</span><br><span class="line"> 3.0 6.0 3.3000000000000003 7.289999999999998</span><br><span class="line">loss =  3.779999999999999</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  1.2000000000000002</span><br><span class="line"> 1.0 2.0 1.2000000000000002 0.6399999999999997</span><br><span class="line"> 2.0 4.0 2.4000000000000004 2.5599999999999987</span><br><span class="line"> 3.0 6.0 3.6000000000000005 5.759999999999997</span><br><span class="line">loss =  2.986666666666665</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  1.3</span><br><span class="line"> 1.0 2.0 1.3 0.48999999999999994</span><br><span class="line"> 2.0 4.0 2.6 1.9599999999999997</span><br><span class="line"> 3.0 6.0 3.9000000000000004 4.409999999999998</span><br><span class="line">loss =  2.2866666666666657</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  1.4000000000000001</span><br><span class="line"> 1.0 2.0 1.4000000000000001 0.3599999999999998</span><br><span class="line"> 2.0 4.0 2.8000000000000003 1.4399999999999993</span><br><span class="line"> 3.0 6.0 4.2 3.2399999999999993</span><br><span class="line">loss =  1.6799999999999995</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  1.5</span><br><span class="line"> 1.0 2.0 1.5 0.25</span><br><span class="line"> 2.0 4.0 3.0 1.0</span><br><span class="line"> 3.0 6.0 4.5 2.25</span><br><span class="line">loss =  1.1666666666666667</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  1.6</span><br><span class="line"> 1.0 2.0 1.6 0.15999999999999992</span><br><span class="line"> 2.0 4.0 3.2 0.6399999999999997</span><br><span class="line"> 3.0 6.0 4.800000000000001 1.4399999999999984</span><br><span class="line">loss =  0.746666666666666</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  1.7000000000000002</span><br><span class="line"> 1.0 2.0 1.7000000000000002 0.0899999999999999</span><br><span class="line"> 2.0 4.0 3.4000000000000004 0.3599999999999996</span><br><span class="line"> 3.0 6.0 5.1000000000000005 0.809999999999999</span><br><span class="line">loss =  0.4199999999999995</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  1.8</span><br><span class="line"> 1.0 2.0 1.8 0.03999999999999998</span><br><span class="line"> 2.0 4.0 3.6 0.15999999999999992</span><br><span class="line"> 3.0 6.0 5.4 0.3599999999999996</span><br><span class="line">loss =  0.1866666666666665</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  1.9000000000000001</span><br><span class="line"> 1.0 2.0 1.9000000000000001 0.009999999999999974</span><br><span class="line"> 2.0 4.0 3.8000000000000003 0.0399999999999999</span><br><span class="line"> 3.0 6.0 5.7 0.0899999999999999</span><br><span class="line">loss =  0.046666666666666586</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  2.0</span><br><span class="line"> 1.0 2.0 2.0 0.0</span><br><span class="line"> 2.0 4.0 4.0 0.0</span><br><span class="line"> 3.0 6.0 6.0 0.0</span><br><span class="line">loss =  0.0</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  2.1</span><br><span class="line"> 1.0 2.0 2.1 0.010000000000000018</span><br><span class="line"> 2.0 4.0 4.2 0.04000000000000007</span><br><span class="line"> 3.0 6.0 6.300000000000001 0.09000000000000043</span><br><span class="line">loss =  0.046666666666666835</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  2.2</span><br><span class="line"> 1.0 2.0 2.2 0.04000000000000007</span><br><span class="line"> 2.0 4.0 4.4 0.16000000000000028</span><br><span class="line"> 3.0 6.0 6.6000000000000005 0.36000000000000065</span><br><span class="line">loss =  0.18666666666666698</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  2.3000000000000003</span><br><span class="line"> 1.0 2.0 2.3000000000000003 0.09000000000000016</span><br><span class="line"> 2.0 4.0 4.6000000000000005 0.36000000000000065</span><br><span class="line"> 3.0 6.0 6.9 0.8100000000000006</span><br><span class="line">loss =  0.42000000000000054</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  2.4000000000000004</span><br><span class="line"> 1.0 2.0 2.4000000000000004 0.16000000000000028</span><br><span class="line"> 2.0 4.0 4.800000000000001 0.6400000000000011</span><br><span class="line"> 3.0 6.0 7.200000000000001 1.4400000000000026</span><br><span class="line">loss =  0.7466666666666679</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  2.5</span><br><span class="line"> 1.0 2.0 2.5 0.25</span><br><span class="line"> 2.0 4.0 5.0 1.0</span><br><span class="line"> 3.0 6.0 7.5 2.25</span><br><span class="line">loss =  1.1666666666666667</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  2.6</span><br><span class="line"> 1.0 2.0 2.6 0.3600000000000001</span><br><span class="line"> 2.0 4.0 5.2 1.4400000000000004</span><br><span class="line"> 3.0 6.0 7.800000000000001 3.2400000000000024</span><br><span class="line">loss =  1.6800000000000008</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  2.7</span><br><span class="line"> 1.0 2.0 2.7 0.49000000000000027</span><br><span class="line"> 2.0 4.0 5.4 1.960000000000001</span><br><span class="line"> 3.0 6.0 8.100000000000001 4.410000000000006</span><br><span class="line">loss =  2.2866666666666693</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  2.8000000000000003</span><br><span class="line"> 1.0 2.0 2.8000000000000003 0.6400000000000005</span><br><span class="line"> 2.0 4.0 5.6000000000000005 2.560000000000002</span><br><span class="line"> 3.0 6.0 8.4 5.760000000000002</span><br><span class="line">loss =  2.986666666666668</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  2.9000000000000004</span><br><span class="line"> 1.0 2.0 2.9000000000000004 0.8100000000000006</span><br><span class="line"> 2.0 4.0 5.800000000000001 3.2400000000000024</span><br><span class="line"> 3.0 6.0 8.700000000000001 7.290000000000005</span><br><span class="line">loss =  3.780000000000003</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  3.0</span><br><span class="line"> 1.0 2.0 3.0 1.0</span><br><span class="line"> 2.0 4.0 6.0 4.0</span><br><span class="line"> 3.0 6.0 9.0 9.0</span><br><span class="line">loss =  4.666666666666667</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  3.1</span><br><span class="line"> 1.0 2.0 3.1 1.2100000000000002</span><br><span class="line"> 2.0 4.0 6.2 4.840000000000001</span><br><span class="line"> 3.0 6.0 9.3 10.890000000000004</span><br><span class="line">loss =  5.646666666666668</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  3.2</span><br><span class="line"> 1.0 2.0 3.2 1.4400000000000004</span><br><span class="line"> 2.0 4.0 6.4 5.760000000000002</span><br><span class="line"> 3.0 6.0 9.600000000000001 12.96000000000001</span><br><span class="line">loss =  6.720000000000003</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  3.3000000000000003</span><br><span class="line"> 1.0 2.0 3.3000000000000003 1.6900000000000006</span><br><span class="line"> 2.0 4.0 6.6000000000000005 6.7600000000000025</span><br><span class="line"> 3.0 6.0 9.9 15.210000000000003</span><br><span class="line">loss =  7.886666666666668</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  3.4000000000000004</span><br><span class="line"> 1.0 2.0 3.4000000000000004 1.960000000000001</span><br><span class="line"> 2.0 4.0 6.800000000000001 7.840000000000004</span><br><span class="line"> 3.0 6.0 10.200000000000001 17.640000000000008</span><br><span class="line">loss =  9.14666666666667</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  3.5</span><br><span class="line"> 1.0 2.0 3.5 2.25</span><br><span class="line"> 2.0 4.0 7.0 9.0</span><br><span class="line"> 3.0 6.0 10.5 20.25</span><br><span class="line">loss =  10.5</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  3.6</span><br><span class="line"> 1.0 2.0 3.6 2.5600000000000005</span><br><span class="line"> 2.0 4.0 7.2 10.240000000000002</span><br><span class="line"> 3.0 6.0 10.8 23.040000000000006</span><br><span class="line">loss =  11.94666666666667</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  3.7</span><br><span class="line"> 1.0 2.0 3.7 2.8900000000000006</span><br><span class="line"> 2.0 4.0 7.4 11.560000000000002</span><br><span class="line"> 3.0 6.0 11.100000000000001 26.010000000000016</span><br><span class="line">loss =  13.486666666666673</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  3.8000000000000003</span><br><span class="line"> 1.0 2.0 3.8000000000000003 3.240000000000001</span><br><span class="line"> 2.0 4.0 7.6000000000000005 12.960000000000004</span><br><span class="line"> 3.0 6.0 11.4 29.160000000000004</span><br><span class="line">loss =  15.120000000000005</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  3.9000000000000004</span><br><span class="line"> 1.0 2.0 3.9000000000000004 3.610000000000001</span><br><span class="line"> 2.0 4.0 7.800000000000001 14.440000000000005</span><br><span class="line"> 3.0 6.0 11.700000000000001 32.49000000000001</span><br><span class="line">loss =  16.84666666666667</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  4.0</span><br><span class="line"> 1.0 2.0 4.0 4.0</span><br><span class="line"> 2.0 4.0 8.0 16.0</span><br><span class="line"> 3.0 6.0 12.0 36.0</span><br><span class="line">loss =  18.666666666666668</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"> <span class="comment"># 绘loss变化图，横坐标是w，纵坐标是loss</span></span><br><span class="line">plt.plot(w_list, loss_list)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;w&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><div class="figure"><img src="https://i.loli.net/2021/10/25/ANq6IBCF9OEZnov.png" alt="image-20211025164754953" /><p class="caption">image-20211025164754953</p></div><p>预测</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 当w为2时间，损失最小</span></span><br><span class="line">y_pers = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> x_data:</span><br><span class="line">    y_per =<span class="number">2</span> * i</span><br><span class="line">    y_pers.append(y_per)</span><br><span class="line">    </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;预测结果:&quot;</span>,<span class="built_in">str</span>(y_pers))    </span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">预测结果: [<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.plot(x_data,y_data,<span class="string">&#x27;o&#x27;</span>,color=<span class="string">&#x27;red&#x27;</span> )</span><br><span class="line">plt.plot(x_data,y_pers)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;predict&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><div class="figure"><img src="https://i.loli.net/2021/10/25/XjHdxEn95cB1U84.png" alt="image-20211025164651345" /><p class="caption">image-20211025164651345</p></div><div class="figure"><img src="https://i.loli.net/2021/10/25/dv7NYHnSXxLjVCK.jpg" /></div><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">实现线性模型（y = w x  + b）并输出loss的3D图像</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> cm</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 线性模型 加入一个偏置b</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">x,w,b</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x * w + b</span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失函数 mse损失</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">x, y, w, b</span>):</span></span><br><span class="line">    y_pred = forward(x, w, b)</span><br><span class="line">    <span class="keyword">return</span> (y_pred - y) * (y_pred - y)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#计算每个x，b对应的损失loss</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_loss</span>(<span class="params">w,b</span>):</span></span><br><span class="line">    l_sum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x_val, y_val <span class="keyword">in</span> <span class="built_in">zip</span>(x_data, y_data):</span><br><span class="line">        y_pred_val = forward(x_val,w,b)</span><br><span class="line">        loss_val = loss(x_val, y_val,w,b)</span><br><span class="line">        l_sum += loss_val</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;\t&#x27;</span>, x_val, y_val, y_pred_val, loss_val)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;loss=&#x27;</span>, l_sum / <span class="number">3</span>)</span><br><span class="line">    <span class="keyword">return</span>  l_sum/<span class="number">3</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.定义网格化数据</span></span><br><span class="line">b_list=np.arange(-<span class="number">30</span>,<span class="number">30</span>,<span class="number">0.1</span>)</span><br><span class="line">w_list=np.arange(-<span class="number">30</span>,<span class="number">30</span>,<span class="number">0.1</span>);</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 2.生成网格化数据</span></span><br><span class="line">xx, yy = np.meshgrid(b_list, w_list, sparse=<span class="literal">False</span>, indexing=<span class="string">&#x27;xy&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.每个点的对应高度 loss</span></span><br><span class="line">zz=get_loss(xx,yy)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"> 1.0 2.0 [[-6.00000000e+01 -5.99000000e+01 -5.98000000e+01 ... -3.00000000e-01</span><br><span class="line">  -2.00000000e-01 -1.00000000e-01]</span><br><span class="line"> [-5.99000000e+01 -5.98000000e+01 -5.97000000e+01 ... -2.00000000e-01</span><br><span class="line">  -1.00000000e-01  8.52651283e-13]</span><br><span class="line"> [-5.98000000e+01 -5.97000000e+01 -5.96000000e+01 ... -1.00000000e-01</span><br><span class="line">   8.52651283e-13  1.00000000e-01]</span><br><span class="line"> ...</span><br><span class="line"> [-3.00000000e-01 -2.00000000e-01 -1.00000000e-01 ...  5.94000000e+01</span><br><span class="line">   5.95000000e+01  5.96000000e+01]</span><br><span class="line"> [-2.00000000e-01 -1.00000000e-01  8.52651283e-13 ...  5.95000000e+01</span><br><span class="line">   5.96000000e+01  5.97000000e+01]</span><br><span class="line"> [-1.00000000e-01  8.52651283e-13  1.00000000e-01 ...  5.96000000e+01</span><br><span class="line">   5.97000000e+01  5.98000000e+01]] [[3.84400e+03 3.83161e+03 3.81924e+03 ... 5.29000e+00 4.84000e+00</span><br><span class="line">  4.41000e+00]</span><br><span class="line"> [3.83161e+03 3.81924e+03 3.80689e+03 ... 4.84000e+00 4.41000e+00</span><br><span class="line">  4.00000e+00]</span><br><span class="line"> [3.81924e+03 3.80689e+03 3.79456e+03 ... 4.41000e+00 4.00000e+00</span><br><span class="line">  3.61000e+00]</span><br><span class="line"> ...</span><br><span class="line"> [5.29000e+00 4.84000e+00 4.41000e+00 ... 3.29476e+03 3.30625e+03</span><br><span class="line">  3.31776e+03]</span><br><span class="line"> [4.84000e+00 4.41000e+00 4.00000e+00 ... 3.30625e+03 3.31776e+03</span><br><span class="line">  3.32929e+03]</span><br><span class="line"> [4.41000e+00 4.00000e+00 3.61000e+00 ... 3.31776e+03 3.32929e+03</span><br><span class="line">  3.34084e+03]]</span><br><span class="line"> 2.0 4.0 [[-90.  -89.8 -89.6 ...  29.4  29.6  29.8]</span><br><span class="line"> [-89.9 -89.7 -89.5 ...  29.5  29.7  29.9]</span><br><span class="line"> [-89.8 -89.6 -89.4 ...  29.6  29.8  30. ]</span><br><span class="line"> ...</span><br><span class="line"> [-30.3 -30.1 -29.9 ...  89.1  89.3  89.5]</span><br><span class="line"> [-30.2 -30.  -29.8 ...  89.2  89.4  89.6]</span><br><span class="line"> [-30.1 -29.9 -29.7 ...  89.3  89.5  89.7]] [[8836.   8798.44 8760.96 ...  645.16  655.36  665.64]</span><br><span class="line"> [8817.21 8779.69 8742.25 ...  650.25  660.49  670.81]</span><br><span class="line"> [8798.44 8760.96 8723.56 ...  655.36  665.64  676.  ]</span><br><span class="line"> ...</span><br><span class="line"> [1176.49 1162.81 1149.21 ... 7242.01 7276.09 7310.25]</span><br><span class="line"> [1169.64 1156.   1142.44 ... 7259.04 7293.16 7327.36]</span><br><span class="line"> [1162.81 1149.21 1135.69 ... 7276.09 7310.25 7344.49]]</span><br><span class="line"> 3.0 6.0 [[-120.  -119.7 -119.4 ...   59.1   59.4   59.7]</span><br><span class="line"> [-119.9 -119.6 -119.3 ...   59.2   59.5   59.8]</span><br><span class="line"> [-119.8 -119.5 -119.2 ...   59.3   59.6   59.9]</span><br><span class="line"> ...</span><br><span class="line"> [ -60.3  -60.   -59.7 ...  118.8  119.1  119.4]</span><br><span class="line"> [ -60.2  -59.9  -59.6 ...  118.9  119.2  119.5]</span><br><span class="line"> [ -60.1  -59.8  -59.5 ...  119.   119.3  119.6]] [[15876.   15800.49 15725.16 ...  2819.61  2851.56  2883.69]</span><br><span class="line"> [15850.81 15775.36 15700.09 ...  2830.24  2862.25  2894.44]</span><br><span class="line"> [15825.64 15750.25 15675.04 ...  2840.89  2872.96  2905.21]</span><br><span class="line"> ...</span><br><span class="line"> [ 4395.69  4356.    4316.49 ... 12723.84 12791.61 12859.56]</span><br><span class="line"> [ 4382.44  4342.81  4303.36 ... 12746.41 12814.24 12882.25]</span><br><span class="line"> [ 4369.21  4329.64  4290.25 ... 12769.   12836.89 12904.96]]</span><br><span class="line">loss= [[9518.66666667 9476.84666667 9435.12       ... 1156.68666667</span><br><span class="line">  1170.58666667 1184.58      ]</span><br><span class="line"> [9499.87666667 9458.09666667 9416.41       ... 1161.77666667</span><br><span class="line">  1175.71666667 1189.75      ]</span><br><span class="line"> [9481.10666667 9439.36666667 9397.72       ... 1166.88666667</span><br><span class="line">  1180.86666667 1194.94      ]</span><br><span class="line"> ...</span><br><span class="line"> [1859.15666667 1841.21666667 1823.37       ... 7753.53666667</span><br><span class="line">  7791.31666667 7829.19      ]</span><br><span class="line"> [1852.30666667 1834.40666667 1816.6        ... 7770.56666667</span><br><span class="line">  7808.38666667 7846.3       ]</span><br><span class="line"> [1845.47666667 1827.61666667 1809.85       ... 7787.61666667</span><br><span class="line">  7825.47666667 7863.43      ]]</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">fig = plt.figure()</span><br><span class="line">ax = Axes3D(fig)</span><br><span class="line">plt.title(&quot;3D&quot;)</span><br><span class="line">ax.plot_surface(xx, yy, zz, cmap=plt.get_cmap(&#x27;rainbow&#x27;)) # 设置曲面的颜色</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><div class="figure"><img src="https://i.loli.net/2021/10/25/gj2lr6tU3s9qZhk.png" alt="image-20211025172320588" /><p class="caption">image-20211025172320588</p></div>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;pytorch&quot;&gt;pytorch&lt;/h1&gt;
&lt;h3 id=&quot;线性模型&quot;&gt;1、线性模型&lt;/h3&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;lin</summary>
      
    
    
    
    <category term="pytorch" scheme="http://example.com/categories/pytorch/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="pytorch" scheme="http://example.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>注意力机制</title>
    <link href="http://example.com/post/141d1667.html"/>
    <id>http://example.com/post/141d1667.html</id>
    <published>2021-10-24T07:06:02.000Z</published>
    <updated>2021-10-27T15:10:29.418Z</updated>
    
    <content type="html"><![CDATA[<h1 id="注意力机制">注意力机制</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">注意力机制的核心在于通过计算一个注意力<span class="built_in">map</span>，来强调最相关的特征，并避免不相关特征的干扰。</span><br><span class="line">获取注意力<span class="built_in">map</span>的方法可分为两类：无参数、有参数，主要的区别在于注意图中的重要性权重是否可学习：</span><br><span class="line">注意力机制为深度网络提供了突出给定图像中最重要区域的能力</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">1D 通道注意力(任务)what</span><br><span class="line">2D 空间注意力(空间)where</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">对于1D数据来说，在注意力方面，SE仅关注了通道注意力，没考虑空间方面的注意力。</span><br></pre></td></tr></table></figure><h3 id="空间注意力">空间注意力：</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">在卷积网络的下采样层，可以使得CNN 可以有小范围的平移不变性。</span><br></pre></td></tr></table></figure><div class="figure"><img src="https://i.loli.net/2021/10/27/sI7R4j9uBYVfSJM.png" alt="w" /><p class="caption">w</p></div><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">这时，我们如果显示的进行仿射变换(旋转缩放平移)，再送到CNN网络中，会起到更好的效果。</span><br></pre></td></tr></table></figure><ol style="list-style-type: decimal"><li>Localisation net 预测一个仿射变换矩阵</li><li>Grid generator 由目标坐标映射到源坐标</li><li>Sampler 重新采样</li></ol><div class="figure"><img src="https://i.loli.net/2021/10/27/ogUZaAlCjkx968N.png" alt="image-20211027031830889" /><p class="caption">image-20211027031830889</p></div><h3 id="pytorch中的仿射变换affine_grid">Pytorch中的仿射变换(affine_grid)</h3><p>图片的旋转、平移、缩放等可以看做一个像素的重采样过程。将原图的像素映射到目标图像的对应位置上，可以</p><div class="figure"><img src="https://i.loli.net/2021/10/27/ND42fLWCUHKTEkR.png" alt="image-20211027141043111" /><p class="caption">image-20211027141043111</p></div><p>其中为xs,ys为原图的坐标，x,y为目标图的坐标，该变换称为前向变换，遍历原图像素，求出改像素在目标图像的对应位置。</p><p>前向变换虽然符合逻辑，但是却使得目标图像上很多位置没有对应的像素。因此一种更合理的方式是使用后向变换，即从目标图像出发，遍历目标图像的每个位置，求出每个位置在原图中的对应像素。此时，公式变为：</p><div class="figure"><img src="https://i.loli.net/2021/10/27/KqNUVxu3BHL6ybM.png" alt="image-20211027141330808" /><p class="caption">image-20211027141330808</p></div><p>二、pytorch中的仿射变换</p><p>pytorch中就使用的为后向变换。主要涉及两个函数</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">F.affine_grid(theta,size)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">F.grid_sample(input, grid, mode=&#x27;bilinear&#x27;, padding_mode=&#x27;zeros&#x27;)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1.F.affine_grid根据输入的变换矩阵theta和尺寸利用后向变换求出目标图像每个像素在原图像的位置。</span><br><span class="line"></span><br><span class="line">    theta是一个[N,2,3]的tensor，N为batchsize大小；2行3列共六个参数，为affine的变换矩阵，坐标(a,c,e)，即横坐标的变换参数，前两个为权重，最后一个为偏移，偏移值e是一个相对于图像宽归一化后的参数,并非像素值，数值范围(0-1),例如0.5表示左移半个图像的宽度。y坐标的变换参数(b,d,f)。</span><br><span class="line"></span><br><span class="line">   size是一个tuple，为(N,C,H,W)</span><br><span class="line">   output为[N,h,w,2]的Tensor，表示在原图中的对应位置。</span><br><span class="line"></span><br><span class="line">2. F.grid_sample()为重采样函数，根据输入的原图和位置对应关系矩阵（F.affine_grid的输出）对原图像素进行重采样，构成变换后的图像。由于重采样过程中，在原图中的位置会出现小数，因此需要对原图进行插值，默认为双线性插值。</span><br></pre></td></tr></table></figure><p>实现：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">10</span>, kernel_size=<span class="number">5</span>) </span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">10</span>, <span class="number">20</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.conv2_drop = nn.Dropout2d()</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">320</span>, <span class="number">50</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">50</span>, <span class="number">10</span>)</span><br><span class="line">        </span><br><span class="line"><span class="comment"># Spatial transformer localization-network</span></span><br><span class="line">        self.localization = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">8</span>, kernel_size=<span class="number">7</span>),  <span class="comment">#(n,8,22,22)</span></span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, stride=<span class="number">2</span>), <span class="comment">#(n,8,11,11)</span></span><br><span class="line">            nn.ReLU(<span class="literal">True</span>), <span class="comment">#(n,8,11,11)</span></span><br><span class="line">            nn.Conv2d(<span class="number">8</span>, <span class="number">10</span>, kernel_size=<span class="number">5</span>),<span class="comment">#(n,10,7,7)</span></span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, stride=<span class="number">2</span>),<span class="comment">#(n,10,3,3)</span></span><br><span class="line">            nn.ReLU(<span class="literal">True</span>)<span class="comment">#(n,10,3,3)</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Regressor for the 3 * 2 affine matrix</span></span><br><span class="line">        self.fc_loc = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">10</span> * <span class="number">3</span> * <span class="number">3</span>, <span class="number">32</span>),<span class="comment">#xs:(n,32)</span></span><br><span class="line">            nn.ReLU(<span class="literal">True</span>), <span class="comment">#xs:(n,32)</span></span><br><span class="line">            nn.Linear(<span class="number">32</span>, <span class="number">3</span> * <span class="number">2</span>)<span class="comment">#xs:(n,3 * 2)</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Initialize the weights/bias with identity transformation</span></span><br><span class="line">        self.fc_loc[<span class="number">2</span>].weight.data.zero_()</span><br><span class="line">        self.fc_loc[<span class="number">2</span>].bias.data.copy_(torch.tensor([<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>], dtype=torch.<span class="built_in">float</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Spatial transformer network forward function</span></span><br><span class="line">    <span class="comment">#空间注意力</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stn</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        xs = self.localization(x)  <span class="comment">#x:(n,c,h,w) (n,1,28,28)  xs:(n,10,3,3)</span></span><br><span class="line">        xs = xs.view(-<span class="number">1</span>, <span class="number">10</span> * <span class="number">3</span> * <span class="number">3</span>) <span class="comment">#xs:(n,10 * 3 * 3)</span></span><br><span class="line">        theta = self.fc_loc(xs) <span class="comment">#xs:(n,3 * 2)</span></span><br><span class="line">        theta = theta.view(-<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>) <span class="comment">#xs:(n,2,3)  [1,0,0][0,1,0]</span></span><br><span class="line"></span><br><span class="line">        grid = F.affine_grid(theta, x.size())</span><br><span class="line">        x = F.grid_sample(x, grid)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x <span class="comment">#(n,1,28,28)</span></span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># transform the input</span></span><br><span class="line">        <span class="comment">#空间注意力</span></span><br><span class="line">        x = self.stn(x) <span class="comment">#(n,1,28,28)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Perform the usual forward pass</span></span><br><span class="line">        <span class="comment">#cnn网络</span></span><br><span class="line">        x = F.relu(F.max_pool2d(self.conv1(x), <span class="number">2</span>))<span class="comment">#(n,1,28,28)-&gt;(n,10,24,24)-&gt;(n,10,12,12)-&gt;(n,10,12,12)</span></span><br><span class="line">        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), <span class="number">2</span>))<span class="comment">#(n,10,12,12)-&gt;(n,20,8,8)-&gt;(n,20,4,4)-&gt;(n,20,4,4)</span></span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">320</span>) <span class="comment">#(n,320)</span></span><br><span class="line">        x = F.relu(self.fc1(x))<span class="comment">#(n,50)</span></span><br><span class="line">        x = F.dropout(x, training=self.training)<span class="comment">#(n,50)</span></span><br><span class="line">        x = self.fc2(x)<span class="comment">#(n,10)</span></span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(x, dim=<span class="number">1</span>)<span class="comment">#(n,1)</span></span><br><span class="line">        </span><br><span class="line">model = Net().to(device)</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;注意力机制&quot;&gt;注意力机制&lt;/h1&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;注意力机制的核心在于通过计算一个注意力&lt;span cla</summary>
      
    
    
    
    <category term="深度学习" scheme="http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="人工智能" scheme="http://example.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="注意力机制" scheme="http://example.com/tags/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"/>
    
  </entry>
  
  <entry>
    <title>人脸识别一</title>
    <link href="http://example.com/post/937cd58.html"/>
    <id>http://example.com/post/937cd58.html</id>
    <published>2021-10-24T05:57:51.000Z</published>
    <updated>2021-10-24T17:41:34.109Z</updated>
    
    <content type="html"><![CDATA[]]></content>
    
    
      
      
    <summary type="html">
</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>目标检测一</title>
    <link href="http://example.com/post/f9b95e7d.html"/>
    <id>http://example.com/post/f9b95e7d.html</id>
    <published>2021-10-24T05:57:22.000Z</published>
    <updated>2021-10-24T17:41:34.112Z</updated>
    
    <content type="html"><![CDATA[<h3 id="yolo">YOLO</h3><h3 id="yolo的改进">YOLO的改进</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1、去掉FPN</span><br><span class="line">yolo-f</span><br><span class="line"></span><br><span class="line">2、anchor Free</span><br><span class="line">yolo-x</span><br><span class="line">将每个位置预测个数从3减少到1，并直接预测四个值（即：到网格的左上角的偏移量和box的高宽）；同时，将中心点设为正样本，并预设了一个尺度范围为每个对象指定FPN级别。</span><br><span class="line"></span><br><span class="line">3、NMS-Free</span><br><span class="line">OneNet</span><br><span class="line">4、</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;yolo&quot;&gt;YOLO&lt;/h3&gt;
&lt;h3 id=&quot;yolo的改进&quot;&gt;YOLO的改进&lt;/h3&gt;
&lt;figure class=&quot;highlight plaintext&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;li</summary>
      
    
    
    
    <category term="目标检测" scheme="http://example.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
    
    <category term="人工智能" scheme="http://example.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="目标检测" scheme="http://example.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>深度学习之生成网络cyclegan</title>
    <link href="http://example.com/post/8b28088e.html"/>
    <id>http://example.com/post/8b28088e.html</id>
    <published>2021-10-23T16:43:40.000Z</published>
    <updated>2021-10-23T19:39:41.301Z</updated>
    
    <content type="html"><![CDATA[<h1 id="cyclegan">Cyclegan</h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">贡献：</span><br><span class="line">提出了循环一致性损失</span><br><span class="line">使用非对称的数据就可以进行风格转换</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">总共使用了两队生成器和判别器。</span><br></pre></td></tr></table></figure><div class="figure"><img src="https://i.loli.net/2021/10/24/Bdo5CKXvTqup4jJ.png" /></div><div class="figure"><img src="https://i.loli.net/2021/10/24/XCteayNqGjv46ZW.png" /></div><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">循环一致性损失：相当于一张图片风格为A，通过G_AB 生成风格为B的假图 -&gt; fake_B,</span><br><span class="line">然后通过 G_BA 生成风格为A假图 fake_A,最终得到的fake_A能够骗过D_A，反之亦然。</span><br><span class="line">A -G_AB&gt; fake_B -G_BA&gt; fake_A</span><br><span class="line">为什么循环一致性损失能够保证在非对称的数据下，图像的内容信息不会改变？</span><br><span class="line"></span><br><span class="line">简单来说：判别器是判断图像风格的，如果没有循环一致性损失时，从源域（风格A）到目标域（风格B）中任意一个或许是最简单的方法，</span><br><span class="line">但是当加入了循环一致性损失后，从源域到目标域的变化中，最简单的方法是只变化风格，而不变化内容，从而约束了生成器的生成。</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;cyclegan&quot;&gt;Cyclegan&lt;/h1&gt;
&lt;figure class=&quot;highlight plaintext&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;贡献：&lt;/span&gt;&lt;br&gt;&lt;span</summary>
      
    
    
    
    <category term="生成网络" scheme="http://example.com/categories/%E7%94%9F%E6%88%90%E7%BD%91%E7%BB%9C/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="人工智能" scheme="http://example.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="生成网络" scheme="http://example.com/tags/%E7%94%9F%E6%88%90%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>深度学习之生成网络gan</title>
    <link href="http://example.com/post/f5c1817e.html"/>
    <id>http://example.com/post/f5c1817e.html</id>
    <published>2021-10-23T16:43:16.000Z</published>
    <updated>2021-10-23T17:04:46.423Z</updated>
    
    <content type="html"><![CDATA[<h1 id="gan">Gan</h1><h1 id="gan的改进">Gan的改进</h1>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;gan&quot;&gt;Gan&lt;/h1&gt;
&lt;h1 id=&quot;gan的改进&quot;&gt;Gan的改进&lt;/h1&gt;
</summary>
      
    
    
    
    <category term="生成网络" scheme="http://example.com/categories/%E7%94%9F%E6%88%90%E7%BD%91%E7%BB%9C/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="人工智能" scheme="http://example.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="生成网络" scheme="http://example.com/tags/%E7%94%9F%E6%88%90%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>深度学习基础一</title>
    <link href="http://example.com/post/9f799b3c.html"/>
    <id>http://example.com/post/9f799b3c.html</id>
    <published>2021-10-23T16:20:02.000Z</published>
    <updated>2021-10-24T17:41:10.521Z</updated>
    
    <content type="html"><![CDATA[<h1 id="卷积cnn">卷积(cnn)</h1><div class="figure"><img src="https://i.loli.net/2021/10/24/YqCZOxnU51ELi8Q.png" /></div><div class="figure"><img src="https://i.loli.net/2021/10/24/7n9Si4u1d6l3oIN.gif" /></div><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">卷积核：简单来说就可以看做一个特征提取器。</span><br><span class="line"></span><br><span class="line">步长stride：每次卷积核在特征图上滑动的步长，默认只滑动一个像素。</span><br><span class="line">当滑动的步长越大，输出的特征图(feature map)就越小，相当于下面说的，池化操作。</span><br><span class="line"></span><br><span class="line">padding操作：填充，对于图像边缘的像素来说，只进行了一次卷积操作，而内部的卷积则进行了至少两次卷积操作，</span><br><span class="line">作用有两个：</span><br><span class="line">1、通过填充后，可以使得卷积后的feature map 和原来的feature map一样的大小,</span><br><span class="line">可以用来控制卷积层输出的特征图的大小。</span><br><span class="line">2、通过对边缘向外填充像素点，来使得边缘像素点，也来充分提取特征。</span><br><span class="line"></span><br><span class="line">感受野：由于卷积操作后，假如是3*3的卷积核，对应于原来的feature map，3*3区域大小的像素</span><br><span class="line">就变为了一个像素区域，因此输出的feature map就相当于原来的3*3的大小，随着卷积层的</span><br><span class="line">加深，感受野也会越来越大。</span><br></pre></td></tr></table></figure><p>当只有一个卷积核时学习到的特征有限，因此需要多个卷积核：</p><div class="figure"><img src="https://i.loli.net/2021/10/24/upofgIP5DrBdbXe.png" /></div><div class="figure"><img src="https://i.loli.net/2021/10/24/in7tUKYS4jWOywx.png" alt="0005" /><p class="caption">0005</p></div><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">一组多个卷积核就能学到多个特征。</span><br><span class="line">对于k个卷积核中的一个卷积核，每张特征图上对应一个卷积核，</span><br><span class="line">在卷积的时候，每张特征图上卷积核不是同一个卷积核，</span><br><span class="line">也就是说对于3通道来说，卷积核就变为了3D，也就是3个卷积核。</span><br><span class="line">卷积完后，然后相加，就变为了一个feature map</span><br><span class="line">对于k个卷积核，最终会得到k个feature map。</span><br></pre></td></tr></table></figure><div class="figure"><img src="https://i.loli.net/2021/10/24/qO12soTD6GRlpc3.png" /></div><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">通过上面的例子再说一下：</span><br><span class="line">1、上面图上有两个卷积核，都是3*3的，每个卷积核上的值叫做权重  Bias b是偏置，可以不用管</span><br><span class="line">2、3*3的卷积核，因为上一层feature map是3个，也就是通道数是3，卷积核就会变为3*3*3</span><br><span class="line">3、且每个卷积核不是同一个卷积核，然后每个卷积核和对应的卷积层进行卷积，得到的值，</span><br><span class="line">然后相加，最终变为一个值，图中就是5.</span><br></pre></td></tr></table></figure><h3 id="输入层">1、输入层</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">一般是一张图像 H*W*C</span><br><span class="line">H：图像高 W ：图像宽 c：图像通道数，刚开始图像通道数为3，RGB</span><br></pre></td></tr></table></figure><h3 id="卷积层">2、卷积层</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">就是我们刚才说的卷积操作。</span><br><span class="line"></span><br><span class="line">有一点需要注意，每层的卷积核是权重共享的。</span><br><span class="line"></span><br><span class="line">为什么要多层卷积？？？？</span><br><span class="line">卷积一直在做的一件事就是对特征提取。</span><br><span class="line">多层卷积相当于做了多次特征提取，随着网络的加深提取到的特征越来越特殊，</span><br><span class="line">越来越能代表这种物体的特征。</span><br></pre></td></tr></table></figure><h3 id="池化层">3、池化层</h3><div class="figure"><img src="https://i.loli.net/2021/10/24/uGerRDV4a5oOZv2.png" alt="0006" /><p class="caption">0006</p></div><div class="figure"><img src="https://i.loli.net/2021/10/24/SJ1AoQqvflEc7eZ.png" alt="0007" /><p class="caption">0007</p></div><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">特征选择作用：采样层实际上就是一个特征选择的过程，挑选出对我们最有用的信息，去除多余的，冗余的信息</span><br><span class="line">1、最大池化</span><br><span class="line">和卷积核大小类似，加入说我们选2*2的大小，进行池化</span><br><span class="line">最大池化就是说从这2*2 ，也就是4个点中，选择最大的那个，其他舍弃</span><br><span class="line">池化可以一定程度提高空间不变性，比如旋转后，和旋转前，池化是一样的，但是局部有限区域内。</span><br><span class="line"></span><br><span class="line">2、平均池化</span><br><span class="line">就是把四个数加起来除4，当做特征值</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">可以看到，池化说到底还是一个特征选择，信息过滤的过程，我们会损失了一部分信息，来减小参数，计算量</span><br><span class="line">但是现在有的地方也不用池化层了。</span><br></pre></td></tr></table></figure><h3 id="激活函数加入非线性">4、激活函数(加入非线性)</h3><div class="figure"><img src="https://i.loli.net/2021/10/24/hLZuNxep4nrMyzf.png" alt="000" /><p class="caption">000</p></div><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">对于原有的卷积层，只能处理线性问题，对于非线性分类问题解决不了。</span><br><span class="line">那么无论神经网络的层数有多少还是在解决线性函数问题，因为两个线性函数的组合还是线性的。</span><br><span class="line"></span><br><span class="line">因此需要引入非线性函数。</span><br><span class="line">sigmoid函数：</span><br><span class="line">在反向传播时，容易出现梯度消失</span><br><span class="line">计算量大</span><br><span class="line">tanh函数：</span><br><span class="line">在反向传播时，容易出现梯度消失</span><br><span class="line">计算量大</span><br><span class="line">Relu函数：</span><br><span class="line">优点：避免梯度消失的问题，通过使得部分神经元为0，抑制作用，避免过拟合问题</span><br><span class="line">缺点：当值为负数的时候，就会变为0，神经元完全不起作用。</span><br><span class="line"></span><br><span class="line">注：现在一般都会使用Relu函数和其改进版本，如ELU、PRelu等，从某种程度上避免了使部分神经元死掉的问题。</span><br></pre></td></tr></table></figure><h3 id="标准化层">5、标准化层</h3><div class="figure"><img src="https://i.loli.net/2021/10/24/uBCUmf89LDoShkN.png" /></div><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">什么是标准化？？？</span><br><span class="line">数据的标准化是指将数据按照比例缩放,使之落入一个特定的区间.</span><br><span class="line"></span><br><span class="line">为什么要做标准化？？？</span><br><span class="line">1. 加快网络的训练和收敛的速度</span><br><span class="line">将数据分布在均值为零，方差为1状态下，使得梯度稳定，容易收敛。</span><br><span class="line"></span><br><span class="line">2. 控制梯度爆炸和防止梯度消失</span><br><span class="line">控制数据集中分布在0值附近，有两个好处，</span><br><span class="line">例如sigmoid函数：</span><br><span class="line">1、在经过sigmoid函数将值约束到0附近，梯度不会消失。</span><br><span class="line">因此通常会加在全连接和激励函数之间。</span><br><span class="line">2、如果不使用标准化可能初始loss过大，梯度反向传播就会积累，前面几层会变得非常大，产生梯度爆炸的问题。</span><br><span class="line">而使用标准化后权值就不会很大了。</span><br><span class="line"></span><br><span class="line">3. 防止过拟合</span><br><span class="line">标准化层在使用过程中，通常会以考虑整个batch的数据进行标准化，</span><br><span class="line">考虑整体比只考虑单个肯定能够一定程度上解决过拟合问题</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="全连接层">6、全连接层</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">为什么需要全连接层？</span><br><span class="line">卷积层我们会发现，有一个天生致命的缺点，那就是没有全局，</span><br><span class="line">因此我们需要一个来把握全局的，那就是全连接层。</span><br><span class="line">相当于把前面，每个学习到的特征进行一个汇总操作，使得我们能够把握到整体。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">全连接层如果加深层数，增加神经元输量，网络能力会得到提升，但是也有可能出现过拟合问题。</span><br></pre></td></tr></table></figure><h3 id="输出层">7、输出层</h3><div class="figure"><img src="https://i.loli.net/2021/10/25/hDmXqrZFkHx2lBe.png" /></div><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">对于分类任务来说：</span><br><span class="line">FC+Softmax+Cross-entropy loss</span><br><span class="line">1、当输入为X, 预测类别为j 的概率为P</span><br><span class="line">2、所有预测类别概率和为1</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">一句话说就是：卷积层负责提取特征，池化负责特征选择，激活函数增加非线性能力，标准化层用来约束数据分布，全连接层负责分类</span><br></pre></td></tr></table></figure><h1 id="损失函数">损失函数</h1><h3 id="分类损失">分类损失</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1、交叉熵损失CE</span><br><span class="line">用于度量两个函数的分布.</span><br><span class="line"></span><br><span class="line">信息熵:去掉冗余信息后的平均信息量。衡量不确定性，不确定性大，信息熵就越大。</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">相对熵(KL散度): 下图</span><br><span class="line">KL散度用于估计两个分布的相似性</span><br><span class="line">其中l(p,p)是分布p的熵，而l(p,q)就是p和q的交叉熵。</span><br><span class="line">假如p是一个已知的分布，则熵是一个常数。</span><br><span class="line">此时KL与l(p,q)也就是交叉熵只有一个常数的差异，两者是等价的。</span><br></pre></td></tr></table></figure><div class="figure"><img src="https://i.loli.net/2021/10/24/ae6Rd5IFBGs7Myw.jpg" /></div><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">交叉熵CE:</span><br></pre></td></tr></table></figure><div class="figure"><img src="https://i.loli.net/2021/10/24/kSmxX8PlvJoji9E.jpg" /></div><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">由于交叉熵正负样本平衡，对于正负样本不平衡网络就不能很好的学习</span><br><span class="line">2、平衡交叉熵损失（正负样本不平衡问题）</span><br><span class="line">3、专注难样本 Focal loss（难易样本）</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">在工程上，经常和softmax loss一起使用：</span><br></pre></td></tr></table></figure><p><span class="math display">\[softmax\]</span></p><div class="figure"><img src="https://i.loli.net/2021/10/25/ZVeDqaf5v4EnKm9.png" alt="image-20211025004211016" /><p class="caption">image-20211025004211016</p></div><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">假如</span><br><span class="line"></span><br><span class="line">输出为 [0.3 0.3 0.4] 目标是 [0 0 1]</span><br><span class="line">则：</span><br><span class="line">交叉熵为：- (ln(0.3) * 0 + ln(0.3) * 0 + ln(0.4) * 1 ) = - ln4</span><br></pre></td></tr></table></figure><h3 id="对于softmax的改进">对于softmax的改进：</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1、Large-Margin Softmax Loss L-Softmax loss</span><br></pre></td></tr></table></figure><div class="figure"><img src="https://i.loli.net/2021/10/25/KIV7MAyW6j8sdSB.png" alt="image-20211025005021695" /><p class="caption">image-20211025005021695</p></div><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">这张图显示的是不同softmax loss和L-Softmax loss学习到的特征分布。</span><br><span class="line">第一列就是softmax，第2列是L-Softmax </span><br><span class="line">loss在参数m取不同值时的分布。通过可视化特征可知学习到的类间的特征是比较明显的，但是类内比较散。</span><br><span class="line"></span><br><span class="line">也就是说使得不同类别之间的夹角增大，同时同类分布也更为紧凑。</span><br></pre></td></tr></table></figure><div class="figure"><img src="https://i.loli.net/2021/10/25/aIugtWSJpTvlKxC.png" alt="image-20211025011546484" /><p class="caption">image-20211025011546484</p></div><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">因为最后是全连接层的输出：</span><br></pre></td></tr></table></figure><div class="figure"><img src="https://i.loli.net/2021/10/25/aWOm7K2qAoDjd3S.png" alt="image-20211025011903423" /><p class="caption">image-20211025011903423</p></div><div class="figure"><img src="https://i.loli.net/2021/10/25/5dRUQVgxm8vTjnA.png" alt="image-20211025011941901" /><p class="caption">image-20211025011941901</p></div><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">因为cos在[0,π]上是减函数，</span><br><span class="line">当f1 &gt; f2 时，也就是说θ1&lt;θ2,样本将被分类为类别1,</span><br><span class="line">当f1 &lt; f2 时，也就是说θ1&gt;θ2,样本将被分类为类别2,</span><br></pre></td></tr></table></figure><div class="figure"><img src="https://i.loli.net/2021/10/25/uBvEb68Dyoh5TZn.png" /></div><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">L-Softmax损失函数中对角度施加了更为强烈的约束,m&gt;=1</span><br></pre></td></tr></table></figure><div class="figure"><img src="https://i.loli.net/2021/10/25/59VNlbKSinOh3zr.png" alt="image-20211025012636114" /><p class="caption">image-20211025012636114</p></div><div class="figure"><img src="https://i.loli.net/2021/10/25/3L9CA2Qxe4GdlJM.png" alt="image-20211025012807781" /><p class="caption">image-20211025012807781</p></div><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">2、Angular Softmax Loss A-Softmax</span><br><span class="line">Angular Softmax Loss(简称A-Softmax loss)与L-Softmax思想类似，主要区别是进一步加入了一个权重约束。</span><br><span class="line">使得||w|| = 1，就变成了</span><br></pre></td></tr></table></figure><div class="figure"><img src="https://i.loli.net/2021/10/25/3GcBTQE9bgL4sVW.png" alt="image-20211025005437525" /><p class="caption">image-20211025005437525</p></div><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">使得类别的判断将只依赖于样本与类别权重的夹角。</span><br></pre></td></tr></table></figure><h3 id="回归损失">回归损失</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1、L1损失</span><br><span class="line">Mean absolute loss(MAE)也被称为L1 Loss，是以绝对误差作为距离</span><br><span class="line">由于L1 loss具有稀疏性，为了惩罚较大的值，因此常常将其作为正则项添加到其他loss中作为约束。</span><br><span class="line">L1 loss的最大问题是梯度在零点不平滑，导致会跳过极小值。</span><br></pre></td></tr></table></figure><div class="figure"><img src="https://i.loli.net/2021/10/24/53QFT6aUDu4GqrX.png" /></div><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">2、L2损失</span><br><span class="line">Mean Squared Loss/ Quadratic Loss(MSE loss)也被称为L2 loss，或欧氏距离，它以误差的平方和作为距离：</span><br><span class="line">L2 loss也常常作为正则项。当预测值与目标值相差很大时, 梯度容易爆炸，因为梯度里包含了x−t。</span><br></pre></td></tr></table></figure><div class="figure"><img src="https://i.loli.net/2021/10/24/oFuUOi58LA6ItaT.png" /></div><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">3、L1 loss与L2 loss的改进</span><br><span class="line"></span><br><span class="line">原始的L1 loss和L2 loss都有缺陷，比如L1 loss的最大问题是梯度不平滑，而L2 loss的最大问题是容易梯度爆炸，所以研究者们对其提出了很多的改进。</span><br><span class="line"></span><br><span class="line">在faster rcnn框架中，使用了smooth L1 loss来综合L1与L2 loss的优点。</span><br><span class="line"></span><br><span class="line">在x比较小时，上式等价于L2 loss，保持平滑。在x比较大时，上式等价于L1 loss，可以限制数值的大小。</span><br></pre></td></tr></table></figure><div class="figure"><img src="https://i.loli.net/2021/10/24/NiLIMr5eu2FDfk8.jpg" /></div><div class="figure"><img src="https://i.loli.net/2021/10/24/cumGWvkODPKZeqr.png" alt="image-20211024235947407" /><p class="caption">image-20211024235947407</p></div><hr />]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;卷积cnn&quot;&gt;卷积(cnn)&lt;/h1&gt;
&lt;div class=&quot;figure&quot;&gt;
&lt;img src=&quot;https://i.loli.net/2021/10/24/YqCZOxnU51ELi8Q.png&quot; /&gt;

&lt;/div&gt;
&lt;div class=&quot;fi</summary>
      
    
    
    
    <category term="深度学习基础" scheme="http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="人工智能" scheme="http://example.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
  </entry>
  
  <entry>
    <title>英语单词一</title>
    <link href="http://example.com/post/afe428e5.html"/>
    <id>http://example.com/post/afe428e5.html</id>
    <published>2021-10-23T16:17:38.000Z</published>
    <updated>2021-10-23T16:38:04.481Z</updated>
    
    <content type="html"><![CDATA[<h1 id="a">A</h1>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;a&quot;&gt;A&lt;/h1&gt;
</summary>
      
    
    
    
    <category term="英语" scheme="http://example.com/categories/%E8%8B%B1%E8%AF%AD/"/>
    
    
    <category term="英语" scheme="http://example.com/tags/%E8%8B%B1%E8%AF%AD/"/>
    
  </entry>
  
  <entry>
    <title>test</title>
    <link href="http://example.com/post/d87f7e0c.html"/>
    <id>http://example.com/post/d87f7e0c.html</id>
    <published>2021-10-23T15:15:36.000Z</published>
    <updated>2021-10-25T15:08:43.513Z</updated>
    
    <content type="html"><![CDATA[<div class="figure"><img src="https://i.loli.net/2021/10/23/eKFWfmxuVjSndR6.jpg" /></div>]]></content>
    
    
      
      
    <summary type="html">&lt;div class=&quot;figure&quot;&gt;
&lt;img src=&quot;https://i.loli.net/2021/10/23/eKFWfmxuVjSndR6.jpg&quot; /&gt;

&lt;/div&gt;
</summary>
      
    
    
    
    <category term="人工智能" scheme="http://example.com/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    
    <category term="人工智能" scheme="http://example.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
  </entry>
  
</feed>
