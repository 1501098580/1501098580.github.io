<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>blog</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2021-10-25T15:04:53.289Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>祎熵</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>pytorch基础一</title>
    <link href="http://example.com/post/cb05edc5.html"/>
    <id>http://example.com/post/cb05edc5.html</id>
    <published>2021-10-25T11:29:36.000Z</published>
    <updated>2021-10-25T15:04:53.289Z</updated>
    
    <content type="html"><![CDATA[<h1 id="pytorch基础"><a href="#pytorch基础" class="headerlink" title="pytorch基础"></a>pytorch基础</h1><h2 id="一、tensor"><a href="#一、tensor" class="headerlink" title="一、tensor"></a>一、tensor</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor是一种专有的数据结构，与数组和矩阵非常相似。在PyTorch中，我们使用tensor来编码一个模型的输入和输出，以及模型的参数。</span><br><span class="line">tensor类似于NumPy的ndarrays，只是tensor可以在GPU或其他硬件加速器上运行。事实上，tensor和NumPy数组通常可以共享相同的底层内存，不需要复制数据。tensor还为自动微分进行了优化</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import torch</span><br><span class="line">import numpy as np</span><br></pre></td></tr></table></figure><p>tensor可以通过各种方式进行初始化。</p><h3 id="1、初始化tensor"><a href="#1、初始化tensor" class="headerlink" title="1、初始化tensor"></a>1、初始化tensor</h3><h4 id="1-1、直接初始化"><a href="#1-1、直接初始化" class="headerlink" title="1.1、直接初始化"></a>1.1、直接初始化</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor可以直接从数据中创建。数据类型是自动推断出来的。</span><br><span class="line">data = [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]]</span><br><span class="line">x_data = torch.tensor(data)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[1, 2],</span></span><br><span class="line"><span class="string">        [3, 4]])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><h4 id="1-2、从Numpy数组中初始化"><a href="#1-2、从Numpy数组中初始化" class="headerlink" title="1.2、从Numpy数组中初始化"></a>1.2、从Numpy数组中初始化</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">可以从Numpy中创建tensor</span><br><span class="line">np_array = np.array(data)</span><br><span class="line">x_np = torch.from_numpy(np_array)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot; </span></span><br><span class="line"><span class="string">tensor([[1, 2],</span></span><br><span class="line"><span class="string">        [3, 4]], dtype=torch.int32)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><h4 id="1-3、从其他tensor中初始化"><a href="#1-3、从其他tensor中初始化" class="headerlink" title="1.3、从其他tensor中初始化"></a>1.3、从其他tensor中初始化</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">新的tensor保留了参数tensor的属性（形状、数据类型），除非明确重写。</span><br><span class="line">x_ones = torch.ones_like(x_data) <span class="comment"># retains the properties of x_data</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Ones Tensor: \n <span class="subst">&#123;x_ones&#125;</span> \n&quot;</span>)</span><br><span class="line">x_rand = torch.rand_like(x_data, dtype=torch.<span class="built_in">float</span>) <span class="comment"># overrides the datatype of x_data</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Random Tensor: \n <span class="subst">&#123;x_rand&#125;</span> \n&quot;</span>)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot; </span></span><br><span class="line"><span class="string">Ones Tensor: </span></span><br><span class="line"><span class="string"> tensor([[1, 1],</span></span><br><span class="line"><span class="string">        [1, 1]]) </span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">Random Tensor: </span></span><br><span class="line"><span class="string"> tensor([[0.4452, 0.7225],</span></span><br><span class="line"><span class="string">        [0.6876, 0.3488]]) </span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><h4 id="1-4、使用随机数和常数初始化"><a href="#1-4、使用随机数和常数初始化" class="headerlink" title="1.4、使用随机数和常数初始化"></a>1.4、使用随机数和常数初始化</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">shape是一个tensor的元组。在下面的代码中，它决定了输出tensor的维度。</span><br><span class="line">shape = (<span class="number">2</span>,<span class="number">3</span>,)</span><br><span class="line">rand_tensor = torch.rand(shape)</span><br><span class="line">ones_tensor = torch.ones(shape)</span><br><span class="line">zeros_tensor = torch.zeros(shape)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Random Tensor: \n <span class="subst">&#123;rand_tensor&#125;</span> \n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Ones Tensor: \n <span class="subst">&#123;ones_tensor&#125;</span> \n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Zeros Tensor: \n <span class="subst">&#123;zeros_tensor&#125;</span>&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Random Tensor: </span></span><br><span class="line"><span class="string"> tensor([[0.5200, 0.8270, 0.3728],</span></span><br><span class="line"><span class="string">        [0.7114, 0.4883, 0.0574]]) </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Ones Tensor: </span></span><br><span class="line"><span class="string"> tensor([[1., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 1.]]) </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Zeros Tensor: </span></span><br><span class="line"><span class="string"> tensor([[0., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 0., 0.]])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><h3 id="2、tensor的性质"><a href="#2、tensor的性质" class="headerlink" title="2、tensor的性质"></a>2、tensor的性质</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tensor属性描述了它们的形状、数据类型以及存储它们的设备。</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor = torch.rand(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Shape of tensor: <span class="subst">&#123;tensor.shape&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Datatype of tensor: <span class="subst">&#123;tensor.dtype&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Device tensor is stored on: <span class="subst">&#123;tensor.device&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Shape of tensor: torch.Size([3, 4])</span></span><br><span class="line"><span class="string">Datatype of tensor: torch.float32</span></span><br><span class="line"><span class="string">Device tensor is stored on: cpu</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><h3 id="3、-tensor的操作"><a href="#3、-tensor的操作" class="headerlink" title="3、 tensor的操作"></a>3、 tensor的操作</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tensor操作，包括算术、线性代数、矩阵操作（转置、索引、切片）、采样等。</span><br><span class="line">默认情况下，tensor是在CPU上创建的。我们需要使用.to方法明确地将tensor移动到GPU上</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 如果有的话，我们把我们的tensor移到GPU上</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    tensor = tensor.to(<span class="string">&#x27;cuda&#x27;</span>)</span><br></pre></td></tr></table></figure><h4 id="3-1、索引和切分"><a href="#3-1、索引和切分" class="headerlink" title="3.1、索引和切分"></a>3.1、索引和切分</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor = torch.ones(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;第一行: &#x27;</span>, tensor[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;第一列：&#x27;</span>, tensor[:, <span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;最后一列：&#x27;</span>, tensor[..., -<span class="number">1</span>])</span><br><span class="line">tensor[:,<span class="number">1</span>] = <span class="number">0</span></span><br><span class="line"><span class="built_in">print</span>(tensor)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">第一行:  tensor([1., 1., 1., 1.])</span></span><br><span class="line"><span class="string">第一列： tensor([1., 1., 1., 1.])</span></span><br><span class="line"><span class="string">最后一列： tensor([1., 1., 1., 1.])</span></span><br><span class="line"><span class="string">tensor([[1., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 0., 1., 1.]])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><h4 id="3-2、tensor拼接"><a href="#3-2、tensor拼接" class="headerlink" title="3.2、tensor拼接"></a>3.2、tensor拼接</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t1 = torch.cat([tensor, tensor, tensor], dim=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(t1)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><h4 id="3-3、算术运算"><a href="#3-3、算术运算" class="headerlink" title="3.3、算术运算"></a>3.3、算术运算</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 这将计算两个tensor之间的矩阵乘法，y1, y2, y3将有相同的值</span></span><br><span class="line">y1 = tensor @ tensor.T</span><br><span class="line">y2 = tensor.matmul(tensor.T)</span><br><span class="line"></span><br><span class="line">y3 = torch.rand_like(tensor)</span><br><span class="line">torch.matmul(tensor, tensor.T, out=y3)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[3., 3., 3., 3.],</span></span><br><span class="line"><span class="string">        [3., 3., 3., 3.],</span></span><br><span class="line"><span class="string">        [3., 3., 3., 3.],</span></span><br><span class="line"><span class="string">        [3., 3., 3., 3.]])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 这将计算出元素相乘的结果。z1，z2, z3有相同的值</span></span><br><span class="line">z1 = tensor * tensor</span><br><span class="line">z2 = tensor.mul(tensor)</span><br><span class="line"></span><br><span class="line">z3 = torch.rand_like(tensor)</span><br><span class="line">torch.mul(tensor, tensor, out=z3)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[1., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 0., 1., 1.]])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p><strong>单元素tensor</strong>:如果你有一个单元素tensor，例如将一个tensor的所有值总计成一个值，你可以使用item()将其变换为Python数值。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">agg = tensor.<span class="built_in">sum</span>()</span><br><span class="line">agg_item = agg.item()</span><br><span class="line"><span class="built_in">print</span>(agg_item, <span class="built_in">type</span>(agg_item))</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">12.0 &lt;class &#x27;float&#x27;&gt;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p><strong>原位操作</strong>将结果存储到操作数中的操作被称为原位操作。它们用后缀<em>来表示。例如：x.copy</em>(y), x.t_(), 将改变x。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(tensor, <span class="string">&quot;\n&quot;</span>)</span><br><span class="line">tensor.add_(<span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(tensor)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[1., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 0., 1., 1.]]) </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">tensor([[6., 5., 6., 6.],</span></span><br><span class="line"><span class="string">        [6., 5., 6., 6.],</span></span><br><span class="line"><span class="string">        [6., 5., 6., 6.],</span></span><br><span class="line"><span class="string">        [6., 5., 6., 6.]])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><h3 id="4、和Numpy转换"><a href="#4、和Numpy转换" class="headerlink" title="4、和Numpy转换"></a>4、和Numpy转换</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CPU上的张量和NumPy数组可以共享它们的底层内存位置，改变一个将改变另一个。</span><br></pre></td></tr></table></figure><h4 id="4-1、tensor转换为NumPy数组"><a href="#4-1、tensor转换为NumPy数组" class="headerlink" title="4.1、tensor转换为NumPy数组"></a>4.1、tensor转换为NumPy数组</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t = torch.ones(<span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;t: <span class="subst">&#123;t&#125;</span>&quot;</span>)</span><br><span class="line">n = t.numpy()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;n: <span class="subst">&#123;n&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">t: tensor([1., 1., 1., 1., 1.])</span></span><br><span class="line"><span class="string">n: [1. 1. 1. 1. 1.]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>tensor的变化反映在NumPy数组中。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t.add_(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;t: <span class="subst">&#123;t&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;n: <span class="subst">&#123;n&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">t: tensor([2., 2., 2., 2., 2.])</span></span><br><span class="line"><span class="string">n: [2. 2. 2. 2. 2.]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><h4 id="4-2、Numpy数组转换为tensor"><a href="#4-2、Numpy数组转换为tensor" class="headerlink" title="4.2、Numpy数组转换为tensor"></a>4.2、Numpy数组转换为tensor</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">n = np.ones(5)</span><br><span class="line">t = torch.from_numpy(n)</span><br><span class="line">np.add(n, 1, out=n)</span><br><span class="line">print(f&quot;t: &#123;t&#125;&quot;)</span><br><span class="line">print(f&quot;n: &#123;n&#125;&quot;)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)</span></span><br><span class="line"><span class="string">n: [2. 2. 2. 2. 2.]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><h2 id="二、数据集和数据载入器"><a href="#二、数据集和数据载入器" class="headerlink" title="二、数据集和数据载入器"></a>二、数据集和数据载入器</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PyTorch提供了两个数据模块：torch.utils.data.DataLoader和torch.utils.data.Dataset，允许你使用预先加载的数据集以及你自己的数据。Dataset存储了样本及其相应的标签，而DataLoader对Dataset包裹了一个可迭代的数据集，以便能够方便地访问这些样本。</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PyTorch库提供了一些预加载的数据集（如FashionMNIST），这些数据集是torch.utils.data.Dataset的子类，并实现了针对特定数据的功能。它们可以用来为你的模型建立原型和基准。</span><br></pre></td></tr></table></figure><h3 id="1、加载数据集"><a href="#1、加载数据集" class="headerlink" title="1、加载数据集"></a>1、加载数据集</h3><p>从TorchVision加载Fashion-MNIST数据集的例子</p><p>Fashion-MNIST是一个由60,000个训练实例和10,000个测试实例组成的Zalando杂志中的图像数据集。每个例子包括一个28×28的灰度图像和这个图像的标签，标签是10类中的一个类别。</p><p>我们用以下参数加载FashionMNIST数据集。</p><ul><li>root是存储训练/测试数据的路径。</li><li>train指定训练或测试数据集。</li><li>download=True如果root目录下没有数据，则从互联网上下载数据。</li><li>transform和target_transform指定特征和标签的变换。</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> ToTensor</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">training_data = datasets.FashionMNIST( root=<span class="string">&quot;data&quot;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=ToTensor() )</span><br><span class="line"></span><br><span class="line">test_data = datasets.FashionMNIST( root=<span class="string">&quot;data&quot;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=ToTensor() )</span><br></pre></td></tr></table></figure><p>输出：</p><p><img src="https://i.loli.net/2021/10/25/xwHnTrWKPsgJ82d.png" alt="image-20211025204306243"></p><h3 id="2、数据集的迭代和可视化"><a href="#2、数据集的迭代和可视化" class="headerlink" title="2、数据集的迭代和可视化"></a>2、数据集的迭代和可视化</h3><p>我们可以像列表一样手动索引数据集：<code>training_data[index]</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">labels_map = &#123;</span><br><span class="line">    <span class="number">0</span>: <span class="string">&quot;T-Shirt&quot;</span>,</span><br><span class="line">    <span class="number">1</span>: <span class="string">&quot;Trouser&quot;</span>,</span><br><span class="line">    <span class="number">2</span>: <span class="string">&quot;Pullover&quot;</span>,</span><br><span class="line">    <span class="number">3</span>: <span class="string">&quot;Dress&quot;</span>,</span><br><span class="line">    <span class="number">4</span>: <span class="string">&quot;Coat&quot;</span>,</span><br><span class="line">    <span class="number">5</span>: <span class="string">&quot;Sandal&quot;</span>,</span><br><span class="line">    <span class="number">6</span>: <span class="string">&quot;Shirt&quot;</span>,</span><br><span class="line">    <span class="number">7</span>: <span class="string">&quot;Sneaker&quot;</span>,</span><br><span class="line">    <span class="number">8</span>: <span class="string">&quot;Bag&quot;</span>,</span><br><span class="line">    <span class="number">9</span>: <span class="string">&quot;Ankle Boot&quot;</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">figure = plt.figure(figsize=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line">cols, rows = <span class="number">3</span>, <span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, cols * rows + <span class="number">1</span>):</span><br><span class="line">    sample_idx = torch.randint(<span class="built_in">len</span>(training_data), size=(<span class="number">1</span>,)).item()</span><br><span class="line">    img, label = training_data[sample_idx]</span><br><span class="line">    figure.add_subplot(rows, cols, i)</span><br><span class="line">    plt.title(labels_map[label])</span><br><span class="line">    plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line">    plt.imshow(img.squeeze(), cmap=<span class="string">&quot;gray&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/10/25/6AIMenYh7UqzX4t.png" alt="image-20211025204330890"></p><h3 id="3、为你的文件创建一个自定义数据集"><a href="#3、为你的文件创建一个自定义数据集" class="headerlink" title="3、为你的文件创建一个自定义数据集"></a>3、为你的文件创建一个自定义数据集</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">一个自定义数据集类必须实现三个函数：__init__, __len__, 和 __getitem__。</span><br></pre></td></tr></table></figure><p>FashionMNIST的图片被存储在一个目录img_dir中，它们的标签被分别存储在一个CSV文件annotations_file中。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> torchvision.io <span class="keyword">import</span> read_image</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomImageDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, annotations_file, img_dir, transform=<span class="literal">None</span>, target_transform=<span class="literal">None</span></span>):</span></span><br><span class="line">        self.img_labels = pd.read_csv(annotations_file)</span><br><span class="line">        self.img_dir = img_dir</span><br><span class="line">        self.transform = transform</span><br><span class="line">        self.target_transform = target_transform</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.img_labels)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, idx</span>):</span></span><br><span class="line">        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, <span class="number">0</span>])</span><br><span class="line">        image = read_image(img_path)</span><br><span class="line">        label = self.img_labels.iloc[idx, <span class="number">1</span>]</span><br><span class="line">        <span class="keyword">if</span> self.transform:</span><br><span class="line">            image = self.transform(image)</span><br><span class="line">        <span class="keyword">if</span> self.target_transform:</span><br><span class="line">            label = self.target_transform(label)</span><br><span class="line">        <span class="keyword">return</span> image, label</span><br></pre></td></tr></table></figure><h4 id="3-1、init"><a href="#3-1、init" class="headerlink" title="3.1、init"></a>3.1、init</h4><p>在实例化数据集对象时，__init__函数运行一次。我们初始化包含图像目录、标注文件目录和变换（在下一节有更详细的介绍）。</p><p>labels.csv文件:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tshirt1.jpg, <span class="number">0</span></span><br><span class="line">tshirt2.jpg, <span class="number">0</span></span><br><span class="line">......</span><br><span class="line">ankleboot999.jpg, <span class="number">9</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, annotations_file, img_dir, transform=<span class="literal">None</span>, target_transform=<span class="literal">None</span></span>):</span></span><br><span class="line">    self.img_labels = pd.read_csv(annotations_file)</span><br><span class="line">    self.img_dir = img_dir</span><br><span class="line">    self.transform = transform</span><br><span class="line">    self.target_transform = target_transform</span><br></pre></td></tr></table></figure><h4 id="3-2、len"><a href="#3-2、len" class="headerlink" title="3.2、len"></a>3.2、len</h4><p>函数 <code>__len__</code> 返回我们数据集中的样本数。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">len</span>(self.img_labels)</span><br></pre></td></tr></table></figure><h4 id="3-3、getitem"><a href="#3-3、getitem" class="headerlink" title="3.3、getitem"></a>3.3、getitem</h4><p>函数 <code>__getitem__</code> 在给定的索引idx处加载并返回数据集中的一个样本。基于索引，它确定图像在磁盘上的位置，使用read_image将其变换为tensor，从self.img_labels中的csv数据中获取相应的标签，对其调用变换函数（如果适用），并在一个元组中返回tensor图像和相应标签。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, idx</span>):</span></span><br><span class="line">    img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, <span class="number">0</span>])</span><br><span class="line">    image = read_image(img_path)</span><br><span class="line">    label = self.img_labels.iloc[idx, <span class="number">1</span>]</span><br><span class="line">    <span class="keyword">if</span> self.transform:</span><br><span class="line">        image = self.transform(image)</span><br><span class="line">    <span class="keyword">if</span> self.target_transform:</span><br><span class="line">        label = self.target_transform(label)</span><br><span class="line">    <span class="keyword">return</span> image, label</span><br></pre></td></tr></table></figure><h3 id="4、用DataLoaders准备你的数据进行训练"><a href="#4、用DataLoaders准备你的数据进行训练" class="headerlink" title="4、用DataLoaders准备你的数据进行训练"></a>4、用DataLoaders准备你的数据进行训练</h3><p>数据集每次都会检索我们的数据集的特征和标签。在训练模型时，我们通常希望以 “小批 “的形式传递样本，在每个epoch中重新洗牌以减少模型的过拟合，并使用Python的multiprocessing来加快数据的检索速度。</p><p>DataLoader是一个可迭代的对象，它用一个简单的API为我们抽象了这种复杂性。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">train_dataloader = DataLoader(training_data, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_dataloader = DataLoader(test_data, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h3 id="5、通过DataLoader进行迭代"><a href="#5、通过DataLoader进行迭代" class="headerlink" title="5、通过DataLoader进行迭代"></a>5、通过DataLoader进行迭代</h3><p>我们已经将该数据集加载到DataLoader中，可以根据需要迭代数据集。下面的每次迭代都会返回一批train_features和train_labels（分别包含batch_size=64的特征和标签）。因为我们指定了shuffle=True，所以在我们迭代完所有的批次后，数据会被洗牌。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Display image and label.</span></span><br><span class="line">train_features, train_labels = <span class="built_in">next</span>(<span class="built_in">iter</span>(train_dataloader))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Feature batch shape: <span class="subst">&#123;train_features.size()&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Labels batch shape: <span class="subst">&#123;train_labels.size()&#125;</span>&quot;</span>)</span><br><span class="line">img = train_features[<span class="number">0</span>].squeeze()</span><br><span class="line">label = train_labels[<span class="number">0</span>]</span><br><span class="line">plt.imshow(img, cmap=<span class="string">&quot;gray&quot;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Label: <span class="subst">&#123;label&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/10/25/iQXM9DvkZs6HU1c.png" alt="image-20211025210207793.png"></p><h2 id="三、变换"><a href="#三、变换" class="headerlink" title="三、变换"></a>三、变换</h2><p>数据并不总是以训练机器学习算法所需的最终处理形式出现。我们使用变换来对数据进行一些操作，使其适合训练。</p><p>所有的TorchVision数据集都有两个参数–用于修改特征的transform和用于修改标签的target_transform–它们接受包含变换逻辑的调用语句。torchvision.transforms模块提供了几个常用的变换，开箱即用。</p><p>FashionMNIST的特征是PIL图像格式的，标签是整数。对于训练，我们需要将特征作为归一化的tensor，将标签作为one-hot编码的tensor。为了进行这些变换，我们使用ToTensor和Lambda。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> ToTensor, Lambda</span><br><span class="line"></span><br><span class="line">ds = datasets.FashionMNIST(</span><br><span class="line">    root=<span class="string">&quot;data&quot;</span>,</span><br><span class="line">    train=<span class="literal">True</span>,</span><br><span class="line">    download=<span class="literal">True</span>,</span><br><span class="line">    transform=ToTensor(),</span><br><span class="line">    target_transform=Lambda(<span class="keyword">lambda</span> y: torch.zeros(<span class="number">10</span>, dtype=torch.<span class="built_in">float</span>).scatter_(<span class="number">0</span>, torch.tensor(y), value=<span class="number">1</span>))</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p><strong>ToTensor()</strong></p><p>ToTensor将PIL图像或NumPy的ndarray变换为FloatTensor，并将图像的像素亮度值按[0., 1.]的范围进行缩放。</p><p><strong>Lambda变换</strong></p><p>Lambda变换应用任何用户定义的Lambda函数。在这里，我们定义了一个函数，把整数变成一个one-hot的tensor。它首先创建一个大小为10（我们数据集中的标签数量），值为0的tensor，并调用scatter_，在标签y给出的索引上分配一个value=1。</p><h2 id="四、搭建神经网络"><a href="#四、搭建神经网络" class="headerlink" title="四、搭建神经网络"></a>四、搭建神经网络</h2><p>神经网络由对数据进行操作的层/模块组成。torch.nn命名空间提供了您构建自己的神经网络所需的所有组件。PyTorch中的每个模块都是nn.Module的子类。一个神经网络本身就是一个由其他模块（层）组成的模块。这种嵌套结构允许轻松构建和管理复杂的架构。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br></pre></td></tr></table></figure><h3 id="1、获取训练的设备"><a href="#1、获取训练的设备" class="headerlink" title="1、获取训练的设备"></a>1、获取训练的设备</h3><p>我们希望能够在像GPU这样的硬件加速器上训练我们的模型，如果它是可用的。让我们检查一下torch.cuda是否可用，否则我们继续使用CPU。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">device = <span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Using &#123;&#125; device&#x27;</span>.<span class="built_in">format</span>(device))</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Using cuda device</span><br></pre></td></tr></table></figure><h3 id="2、定义模型类"><a href="#2、定义模型类" class="headerlink" title="2、定义模型类"></a>2、定义模型类</h3><p>我们通过子类化 nn.Module 来定义我们的神经网络，并在 <code>__init__</code>中初始化神经网络层。每个 nn.Module 子类都在 forward 方法中实现了对输入数据的操作。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NeuralNetwork</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(NeuralNetwork, self).__init__()</span><br><span class="line">        self.flatten = nn.Flatten()</span><br><span class="line">        self.linear_relu_stack = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">28</span>*<span class="number">28</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">10</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        logits = self.linear_relu_stack(x)</span><br><span class="line">        <span class="keyword">return</span> logits</span><br></pre></td></tr></table></figure><p>我们创建一个NeuralNetwork的实例，并将其移动到设备上，并打印其结构。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = NeuralNetwork().to(device)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">NeuralNetwork(</span></span><br><span class="line"><span class="string">  (flatten): Flatten(start_dim=1, end_dim=-1)</span></span><br><span class="line"><span class="string">  (linear_relu_stack): Sequential(</span></span><br><span class="line"><span class="string">    (0): Linear(in_features=784, out_features=512, bias=True)</span></span><br><span class="line"><span class="string">    (1): ReLU()</span></span><br><span class="line"><span class="string">    (2): Linear(in_features=512, out_features=512, bias=True)</span></span><br><span class="line"><span class="string">    (3): ReLU()</span></span><br><span class="line"><span class="string">    (4): Linear(in_features=512, out_features=10, bias=True)</span></span><br><span class="line"><span class="string">  )</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>为了使用这个模型，我们把输入数据传给它。这就执行了模型的forward函数，以及一些后台操作。</p><p>在输入数据上调用模型会返回一个10维的tensor，其中包含每个类别的原始预测值。我们通过nn.Softmax模块的一个实例来获得预测概率。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.rand(<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>, device=device)</span><br><span class="line">logits = model(X)</span><br><span class="line">pred_probab = nn.Softmax(dim=<span class="number">1</span>)(logits)</span><br><span class="line">y_pred = pred_probab.argmax(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Predicted class: <span class="subst">&#123;y_pred&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Predicted class: tensor([1], device=&#x27;cuda:0&#x27;)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><h3 id="3、模型层"><a href="#3、模型层" class="headerlink" title="3、模型层"></a>3、模型层</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">input_image = torch.rand(<span class="number">3</span>,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line"><span class="built_in">print</span>(input_image.size())</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">torch.Size([3, 28, 28])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p><strong>nn.Flatten</strong></p><p>我们初始化nn.Flatten层，将每个28x28的二维图像变换为784个像素值的连续数组（minibatch的维度（dim=0）被保持）。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">flatten = nn.Flatten()</span><br><span class="line">flat_image = flatten(input_image)</span><br><span class="line"><span class="built_in">print</span>(flat_image.size())</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">torch.Size([3, 784])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p><strong>nn.Linear</strong></p><p>线性层是一个模块，使用其存储的权重和偏置对输入进行线性变换。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">layer1 = nn.Linear(in_features=<span class="number">28</span>*<span class="number">28</span>, out_features=<span class="number">20</span>)</span><br><span class="line">hidden1 = layer1(flat_image)</span><br><span class="line"><span class="built_in">print</span>(hidden1.size())</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">torch.Size([3, 20])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p><strong>nn.ReLU</strong></p><p>非线性激活是在模型的输入和输出之间建立复杂的映射关系。它们被应用在线性变换之后，以引入非线性，帮助神经网络学习各种各样的函数。</p><p>在这个模型中，我们在线性层之间使用了nn.ReLU，但还有其他激活函数可以在你的模型中引入非线性。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Before ReLU: <span class="subst">&#123;hidden1&#125;</span>\n\n&quot;</span>)</span><br><span class="line">hidden1 = nn.ReLU()(hidden1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;After ReLU: <span class="subst">&#123;hidden1&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Before ReLU: tensor([[-0.2050, -0.2515, -0.1101, -0.5910,  0.0241, -0.4991, -0.0064,  0.2330,</span></span><br><span class="line"><span class="string">          0.2104, -0.2930, -0.4654, -0.6682,  0.0789,  0.2525,  0.3306, -0.4441,</span></span><br><span class="line"><span class="string">         -0.1403,  0.2946,  0.2446, -0.6398],</span></span><br><span class="line"><span class="string">        [-0.2331, -0.3806, -0.2077, -0.7201, -0.2562, -0.4168, -0.0570,  0.0775,</span></span><br><span class="line"><span class="string">          0.1734, -0.0644, -0.2212, -0.4178, -0.1430,  0.3815,  0.3207, -0.4322,</span></span><br><span class="line"><span class="string">         -0.2514, -0.0818,  0.0162, -0.7603],</span></span><br><span class="line"><span class="string">        [ 0.1153, -0.3066,  0.6950, -0.4477,  0.0225, -0.3306,  0.2582, -0.0583,</span></span><br><span class="line"><span class="string">          0.3550, -0.1699, -0.5302, -0.6426, -0.3060,  0.2715,  0.0820, -0.2693,</span></span><br><span class="line"><span class="string">         -0.3574,  0.1241,  0.3639, -0.9418]], grad_fn=&lt;AddmmBackward&gt;)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">After ReLU: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0241, 0.0000, 0.0000, 0.2330, 0.2104,</span></span><br><span class="line"><span class="string">         0.0000, 0.0000, 0.0000, 0.0789, 0.2525, 0.3306, 0.0000, 0.0000, 0.2946,</span></span><br><span class="line"><span class="string">         0.2446, 0.0000],</span></span><br><span class="line"><span class="string">        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0775, 0.1734,</span></span><br><span class="line"><span class="string">         0.0000, 0.0000, 0.0000, 0.0000, 0.3815, 0.3207, 0.0000, 0.0000, 0.0000,</span></span><br><span class="line"><span class="string">         0.0162, 0.0000],</span></span><br><span class="line"><span class="string">        [0.1153, 0.0000, 0.6950, 0.0000, 0.0225, 0.0000, 0.2582, 0.0000, 0.3550,</span></span><br><span class="line"><span class="string">         0.0000, 0.0000, 0.0000, 0.0000, 0.2715, 0.0820, 0.0000, 0.0000, 0.1241,</span></span><br><span class="line"><span class="string">         0.3639, 0.0000]], grad_fn=&lt;ReluBackward0&gt;)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p><strong>nn.Sequential</strong></p><p>nn.Sequential是一个有序模块的容器。数据以定义的顺序通过所有的模块。你可以使用 序列容器来组建一个快速的网络，如seq_modules：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">seq_modules = nn.Sequential(</span><br><span class="line">    flatten,</span><br><span class="line">    layer1,</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">20</span>, <span class="number">10</span>)</span><br><span class="line">)</span><br><span class="line">input_image = torch.rand(<span class="number">3</span>,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line">logits = seq_modules(input_image)</span><br></pre></td></tr></table></figure><p><strong>nn.Softmax</strong></p><p>神经网络的最后一个线性层返回logits–[-infty, infty]中的原始值–并传递给nn.Softmax模块。对数被缩放到数值区间[0, 1]，代表模型对每个类别的预测概率。 dim参数表示数值必须和为1的维度。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">softmax = nn.Softmax(dim=<span class="number">1</span>)</span><br><span class="line">pred_probab = softmax(logits)</span><br></pre></td></tr></table></figure><h3 id="4、模型参数"><a href="#4、模型参数" class="headerlink" title="4、模型参数"></a>4、模型参数</h3><p>神经网络中的许多层都是参数化的，也就是说，层相关的权重和偏置在训练中被优化。nn.Module的子类会自动跟踪你的模型对象中定义的所有字段，并使用你的模型的 parameters() 或 named_parameters() 方法访问所有参数。</p><p>在这个例子中，我们遍历每个参数，并打印其大小和预览其值。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Model structure: &quot;</span>, model, <span class="string">&quot;\n\n&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Layer: <span class="subst">&#123;name&#125;</span> | Size: <span class="subst">&#123;param.size()&#125;</span> | Values : <span class="subst">&#123;param[:<span class="number">2</span>]&#125;</span> \n&quot;</span>)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Model structure:  NeuralNetwork(</span><br><span class="line">  (flatten): Flatten(start_dim=<span class="number">1</span>, end_dim=-<span class="number">1</span>)</span><br><span class="line">  (linear_relu_stack): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Linear(in_features=<span class="number">784</span>, out_features=<span class="number">512</span>, bias=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">1</span>): ReLU()</span><br><span class="line">    (<span class="number">2</span>): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">512</span>, bias=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">3</span>): ReLU()</span><br><span class="line">    (<span class="number">4</span>): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">10</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  )</span><br><span class="line">) </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Layer: linear_relu_stack<span class="number">.0</span>.weight | Size: torch.Size([<span class="number">512</span>, <span class="number">784</span>]) | Values : tensor([[-<span class="number">0.0331</span>,  <span class="number">0.0108</span>, -<span class="number">0.0115</span>,  ...,  <span class="number">0.0196</span>,  <span class="number">0.0255</span>,  <span class="number">0.0289</span>],</span><br><span class="line">        [-<span class="number">0.0168</span>,  <span class="number">0.0280</span>, -<span class="number">0.0132</span>,  ..., -<span class="number">0.0103</span>, -<span class="number">0.0099</span>, -<span class="number">0.0121</span>]],</span><br><span class="line">       device=<span class="string">&#x27;cuda:0&#x27;</span>, grad_fn=&lt;SliceBackward&gt;) </span><br><span class="line"></span><br><span class="line">Layer: linear_relu_stack<span class="number">.0</span>.bias | Size: torch.Size([<span class="number">512</span>]) | Values : tensor([-<span class="number">0.0108</span>,  <span class="number">0.0137</span>], device=<span class="string">&#x27;cuda:0&#x27;</span>, grad_fn=&lt;SliceBackward&gt;) </span><br><span class="line"></span><br><span class="line">Layer: linear_relu_stack<span class="number">.2</span>.weight | Size: torch.Size([<span class="number">512</span>, <span class="number">512</span>]) | Values : tensor([[ <span class="number">0.0048</span>,  <span class="number">0.0112</span>,  <span class="number">0.0430</span>,  ..., -<span class="number">0.0423</span>, -<span class="number">0.0438</span>,  <span class="number">0.0150</span>],</span><br><span class="line">        [-<span class="number">0.0213</span>, -<span class="number">0.0016</span>, -<span class="number">0.0128</span>,  ...,  <span class="number">0.0230</span>,  <span class="number">0.0200</span>, -<span class="number">0.0120</span>]],</span><br><span class="line">       device=<span class="string">&#x27;cuda:0&#x27;</span>, grad_fn=&lt;SliceBackward&gt;) </span><br><span class="line"></span><br><span class="line">Layer: linear_relu_stack<span class="number">.2</span>.bias | Size: torch.Size([<span class="number">512</span>]) | Values : tensor([<span class="number">0.0374</span>, <span class="number">0.0331</span>], device=<span class="string">&#x27;cuda:0&#x27;</span>, grad_fn=&lt;SliceBackward&gt;) </span><br><span class="line"></span><br><span class="line">Layer: linear_relu_stack<span class="number">.4</span>.weight | Size: torch.Size([<span class="number">10</span>, <span class="number">512</span>]) | Values : tensor([[ <span class="number">0.0318</span>, -<span class="number">0.0424</span>,  <span class="number">0.0038</span>,  ...,  <span class="number">0.0241</span>, -<span class="number">0.0187</span>, -<span class="number">0.0105</span>],</span><br><span class="line">        [ <span class="number">0.0004</span>,  <span class="number">0.0132</span>,  <span class="number">0.0343</span>,  ..., -<span class="number">0.0182</span>,  <span class="number">0.0013</span>, -<span class="number">0.0020</span>]],</span><br><span class="line">       device=<span class="string">&#x27;cuda:0&#x27;</span>, grad_fn=&lt;SliceBackward&gt;) </span><br><span class="line"></span><br><span class="line">Layer: linear_relu_stack<span class="number">.4</span>.bias | Size: torch.Size([<span class="number">10</span>]) | Values : tensor([<span class="number">0.0109</span>, <span class="number">0.0427</span>], device=<span class="string">&#x27;cuda:0&#x27;</span>, grad_fn=&lt;SliceBackward&gt;) </span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="五、使用torch-autograd进行自动微分"><a href="#五、使用torch-autograd进行自动微分" class="headerlink" title="五、使用torch.autograd进行自动微分"></a>五、使用torch.autograd进行自动微分</h2><p>在训练神经网络时，最常使用的算法是反向传播算法。在这种算法中，参数（模型权重）是根据损失函数相对于给定参数的梯度来调整的。</p><p>为了计算这些梯度，PyTorch有一个内置的微分引擎，叫做torch.autograd。它支持对任何计算图的梯度进行自动计算。</p><p>考虑最简单的单层神经网络，输入x，参数w和b，以及一些损失函数。它可以在PyTorch中以如下方式定义：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.ones(<span class="number">5</span>)  <span class="comment"># input tensor</span></span><br><span class="line">y = torch.zeros(<span class="number">3</span>)  <span class="comment"># expected output</span></span><br><span class="line">w = torch.randn(<span class="number">5</span>, <span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.randn(<span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">z = torch.matmul(x, w)+b</span><br><span class="line">loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)</span><br></pre></td></tr></table></figure><h3 id="1、tensor、函数和计算图"><a href="#1、tensor、函数和计算图" class="headerlink" title="1、tensor、函数和计算图"></a>1、tensor、函数和计算图</h3><p><img src="https://i.loli.net/2021/10/25/iulTGxK2vWg1V97.png" alt="计算图"></p><p>在这个网络中，w和b是参数，我们需要进行优化。因此，我们需要能够计算损失函数相对于这些变量的梯度。为了做到这一点，我们设置了这些tensor的 requires_grad 属性。</p><p>注意:你可以在创建tensor时设置requires_grad的值，或者在以后使用x.requires_grad_(True)方法来设置。</p><p>我们应用于tensor来构建计算图的函数实际上是一个Function类的对象。这个对象知道如何在forward方向上计算函数，也知道如何在后向传播步骤中计算其导数。对后向传播函数的引用被存储在tensor的grad_fn属性中。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Gradient function for z =&#x27;</span>, z.grad_fn)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Gradient function for loss =&#x27;</span>, loss.grad_fn)</span><br></pre></td></tr></table></figure><h3 id="2、计算梯度"><a href="#2、计算梯度" class="headerlink" title="2、计算梯度"></a>2、计算梯度</h3><p>为了优化神经网络中的参数权重，我们需要计算损失函数相对于参数的导数</p><p>为了计算这些导数，我们调用 loss.backward()，然后从 w.grad 和 b.grad 中获取数值。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loss.backward()</span><br><span class="line"><span class="built_in">print</span>(w.grad)</span><br><span class="line"><span class="built_in">print</span>(b.grad)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[0.3256, 0.2634, 0.1137],</span></span><br><span class="line"><span class="string">        [0.3256, 0.2634, 0.1137],</span></span><br><span class="line"><span class="string">        [0.3256, 0.2634, 0.1137],</span></span><br><span class="line"><span class="string">        [0.3256, 0.2634, 0.1137],</span></span><br><span class="line"><span class="string">        [0.3256, 0.2634, 0.1137]])</span></span><br><span class="line"><span class="string">tensor([0.3256, 0.2634, 0.1137])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><ul><li>我们只能获得计算图的叶子节点的grad属性，这些节点的requires_grad属性设置为True。对于我们图中的所有其他节点，梯度将不可用。</li><li>出于性能方面的考虑，我们只能在一个给定的图上使用一次backward来进行梯度计算。如果我们需要在同一个图上进行多次backward调用，我们需要在backward调用中传递 retain_graph=True。</li></ul><h3 id="3、禁止梯度跟踪"><a href="#3、禁止梯度跟踪" class="headerlink" title="3、禁止梯度跟踪"></a>3、禁止梯度跟踪</h3><p>默认情况下，所有带有require_grad=True的tensor都在跟踪它们的计算历史并支持梯度计算。然而，在某些情况下，我们不需要这样做，例如，当我们已经训练了模型，只是想把它应用于一些输入数据，也就是说，我们只想通过网络进行前向计算。我们可以通过用torch.no_grad()块包围我们的计算代码来停止跟踪计算。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">z = torch.matmul(x, w)+b</span><br><span class="line"><span class="built_in">print</span>(z.requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    z = torch.matmul(x, w)+b</span><br><span class="line"><span class="built_in">print</span>(z.requires_grad)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">True</span><br><span class="line">False</span><br></pre></td></tr></table></figure><p>另一种实现相同结果的方法是在tensor上使用detach()方法。</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">z = torch.matmul(x, w)+b</span><br><span class="line">z_det = z.detach()</span><br><span class="line">print(z_det.requires_grad)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">False</span><br></pre></td></tr></table></figure><p>你可能想禁用梯度跟踪，原因如下：</p><ul><li>将神经网络中的一些参数标记为冻结参数。这是对预训练的网络进行微调的一个非常常见的情况。</li><li>当你只做前向传递时，为了加快计算速度，对不跟踪梯度的tensor的计算会更有效率。</li></ul><h3 id="更多"><a href="#更多" class="headerlink" title="更多"></a>更多</h3><p>从概念上讲，autograd在一个由Function对象组成的有向无环图（DAG）中保存了数据（tensor）和所有执行的操作（以及产生的新tensor）的记录。在这个DAG中，叶子是输入tensor，根部是输出tensor。通过追踪这个图从根到叶，你可以使用链式规则自动计算梯度。</p><p>在一个前向传递中，autograd同时做两件事。</p><ul><li>运行请求的操作，计算出一个结果tensor。</li><li>在DAG中维护该操作的梯度函数。</li></ul><p>当在DAG根上调用.backward()时，后向传递开始了。</p><ul><li>计算每个.grad_fn的梯度。</li><li>将它们累积到各自tensor的 .grad 属性中</li><li>使用链式规则，一直传播到叶子tensor。</li></ul><p>注意:在PyTorch中，DAG是动态的。需要注意的是，图是从头开始重新创建的；在每次调用.backward()后，autograd开始填充一个新的图。这正是允许你在模型中使用控制流语句的原因；如果需要，你可以在每次迭代时改变形状、大小和操作。</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line">from torchvision import datasets</span><br><span class="line">from torchvision.transforms import ToTensor, Lambda</span><br><span class="line"></span><br><span class="line">training_data = datasets.FashionMNIST(</span><br><span class="line">    root=&quot;data&quot;,</span><br><span class="line">    train=True,</span><br><span class="line">    download=True,</span><br><span class="line">    transform=ToTensor()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">test_data = datasets.FashionMNIST(</span><br><span class="line">    root=&quot;data&quot;,</span><br><span class="line">    train=False,</span><br><span class="line">    download=True,</span><br><span class="line">    transform=ToTensor()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">train_dataloader = DataLoader(training_data, batch_size=64)</span><br><span class="line">test_dataloader = DataLoader(test_data, batch_size=64)</span><br><span class="line"></span><br><span class="line">class NeuralNetwork(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(NeuralNetwork, self).__init__()</span><br><span class="line">        self.flatten = nn.Flatten()</span><br><span class="line">        self.linear_relu_stack = nn.Sequential(</span><br><span class="line">            nn.Linear(28*28, 512),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(512, 512),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(512, 10),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        logits = self.linear_relu_stack(x)</span><br><span class="line">        return logits</span><br><span class="line"></span><br><span class="line">model = NeuralNetwork()</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;pytorch基础&quot;&gt;&lt;a href=&quot;#pytorch基础&quot; class=&quot;headerlink&quot; title=&quot;pytorch基础&quot;&gt;&lt;/a&gt;pytorch基础&lt;/h1&gt;&lt;h2 id=&quot;一、tensor&quot;&gt;&lt;a href=&quot;#一、tensor&quot; class=&quot;</summary>
      
    
    
    
    <category term="pytorch" scheme="http://example.com/categories/pytorch/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="pytorch" scheme="http://example.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch深度学习三</title>
    <link href="http://example.com/post/a0d5236b.html"/>
    <id>http://example.com/post/a0d5236b.html</id>
    <published>2021-10-25T11:00:09.000Z</published>
    <updated>2021-10-25T11:26:02.033Z</updated>
    
    <content type="html"><![CDATA[<h1 id="pytorch"><a href="#pytorch" class="headerlink" title="pytorch"></a>pytorch</h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">使用pytorch实现线性模型</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure><p>数据</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># x和y数据必须是矩阵，所以如[1.0]</span></span><br><span class="line">x_data = torch.Tensor([[<span class="number">1.0</span>], [<span class="number">2.0</span>], [<span class="number">3.0</span>]])</span><br><span class="line">y_data = torch.Tensor([[<span class="number">2.0</span>], [<span class="number">4.0</span>], [<span class="number">6.0</span>]])</span><br><span class="line">loss_list = []</span><br></pre></td></tr></table></figure><p>模型</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearModel</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(LinearModel, self).__init__()</span><br><span class="line">        <span class="comment"># 实例化一个linear对象</span></span><br><span class="line">        self.linear = nn.Linear(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># 可调用的对象</span></span><br><span class="line">        y_pred = self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line">    </span><br><span class="line">model = LinearModel()</span><br></pre></td></tr></table></figure><p>优化器和损失函数</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># criterion = torch.nn.MSELoss(size_average=False)</span></span><br><span class="line">criterion = torch.nn.MSELoss(reduction = <span class="string">&#x27;sum&#x27;</span>) <span class="comment">#求和</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr = <span class="number">0.01</span>) <span class="comment">#学习率lr = 0.01</span></span><br></pre></td></tr></table></figure><p>训练</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500</span>):</span><br><span class="line">    y_pred = model(x_data)</span><br><span class="line">    loss = criterion(y_pred, y_data)</span><br><span class="line">    <span class="comment"># 打印loss对象会自动调用__str__(),不会产生计算图</span></span><br><span class="line">    <span class="built_in">print</span>(epoch, loss.item())</span><br><span class="line">    loss_list.append(loss.item())</span><br><span class="line">    <span class="comment">#每次需要归零，不然梯度会积累</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="comment">#参数更新</span></span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure><p>结果：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span> <span class="number">8.456552505493164</span></span><br><span class="line"><span class="number">1</span> <span class="number">4.0267653465271</span></span><br><span class="line"><span class="number">2</span> <span class="number">2.050983428955078</span></span><br><span class="line"><span class="number">3</span> <span class="number">1.167707085609436</span></span><br><span class="line"><span class="number">4</span> <span class="number">0.7708380222320557</span></span><br><span class="line"></span><br><span class="line">...................</span><br><span class="line">...................</span><br><span class="line">...................</span><br><span class="line"></span><br><span class="line"><span class="number">491</span> <span class="number">0.0003971618425566703</span></span><br><span class="line"><span class="number">492</span> <span class="number">0.00039145682239905</span></span><br><span class="line"><span class="number">493</span> <span class="number">0.00038583995774388313</span></span><br><span class="line"><span class="number">494</span> <span class="number">0.0003802851715590805</span></span><br><span class="line"><span class="number">495</span> <span class="number">0.0003748224989976734</span></span><br><span class="line"><span class="number">496</span> <span class="number">0.00036944064777344465</span></span><br><span class="line"><span class="number">497</span> <span class="number">0.00036413324414752424</span></span><br><span class="line"><span class="number">498</span> <span class="number">0.0003588950785342604</span></span><br><span class="line"><span class="number">499</span> <span class="number">0.0003537364536896348</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 画图</span></span><br><span class="line">plt.plot(<span class="built_in">range</span>(<span class="number">500</span>), loss_list)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;epoch&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/10/25/cZYV6AWHdvORPFn.png" alt="image-20211025192214494"></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;w = &#x27;</span>, model.linear.weight.item())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;b = &#x27;</span>, model.linear.bias.item())</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">w =  <span class="number">1.9874792098999023</span></span><br><span class="line">b =  <span class="number">0.02846282161772251</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Test </span></span><br><span class="line"><span class="comment"># 输入是1×1矩阵，输出也是1×1矩阵</span></span><br><span class="line">x_test = torch.Tensor([[<span class="number">2.0</span>]]) </span><br><span class="line">y_test = model(x_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;y_pred = &#x27;</span>, y_test.data)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y_pred =  tensor([[<span class="number">4.0034</span>]])</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;pytorch&quot;&gt;&lt;a href=&quot;#pytorch&quot; class=&quot;headerlink&quot; title=&quot;pytorch&quot;&gt;&lt;/a&gt;pytorch&lt;/h1&gt;&lt;figure class=&quot;highlight plaintext&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td cl</summary>
      
    
    
    
    <category term="pytorch" scheme="http://example.com/categories/pytorch/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="pytorch" scheme="http://example.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch深度学习二</title>
    <link href="http://example.com/post/e289b566.html"/>
    <id>http://example.com/post/e289b566.html</id>
    <published>2021-10-25T10:23:44.000Z</published>
    <updated>2021-10-25T10:58:15.031Z</updated>
    
    <content type="html"><![CDATA[<h1 id="pytorch"><a href="#pytorch" class="headerlink" title="pytorch"></a>pytorch</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">使用梯度下降进行求解</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/10/25/8clrVKoF6udLbWs.png" alt="image-20211025185053738"></p><p><img src="https://i.loli.net/2021/10/25/9z8ayRL1d32GZeu.png" alt="image-20211025185107672"></p><p><img src="https://i.loli.net/2021/10/25/TLmfhDd1PsZw7JV.png" alt="image-20211025185123456"></p><p><img src="https://i.loli.net/2021/10/25/n3rb2o6QsMfCk78.png" alt="image-20211025185137466"></p><p><img src="https://i.loli.net/2021/10/25/jJsVux5qO2bkcaS.png" alt="image-20211025185150736"></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>]</span><br><span class="line">loss_list = []</span><br><span class="line">w = <span class="number">1.0</span> <span class="comment">#权重初始化为1</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 向前传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x * w</span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">x_data, y_data</span>):</span> <span class="comment">#x_data [1.0, 2.0, 3.0] y_data [2.0, 4.0, 6.0]</span></span><br><span class="line">    loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(x_data, y_data):</span><br><span class="line">        y_predict = forward(x)</span><br><span class="line">        loss += (y_predict - y) ** <span class="number">2</span>  <span class="comment">#总loss</span></span><br><span class="line">    <span class="keyword">return</span> loss / <span class="built_in">len</span>(x_data)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 求梯度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span>(<span class="params">x_data, y_data</span>):</span></span><br><span class="line">    grad = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> <span class="built_in">zip</span>(x_data, y_data):</span><br><span class="line">        <span class="comment"># 向前传播</span></span><br><span class="line">        temp = forward(x)</span><br><span class="line">        <span class="comment"># 求梯度</span></span><br><span class="line">        grad += <span class="number">2</span> * x *(temp - y)</span><br><span class="line">    <span class="keyword">return</span> grad / <span class="built_in">len</span>(x_data)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># train</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="comment"># 求损失值</span></span><br><span class="line">    loss_val = loss(x_data, y_data)</span><br><span class="line">    loss_list.append(loss_val)</span><br><span class="line">    <span class="comment"># 求梯度值</span></span><br><span class="line">    grad_val = gradient(x_data, y_data)</span><br><span class="line">    <span class="comment"># 更新参数w</span></span><br><span class="line">    w -= <span class="number">0.01</span> *grad_val</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Epoch: &quot;</span>, epoch, <span class="string">&quot;w = &quot;</span>, w, <span class="string">&quot;loss = &quot;</span>, loss_val)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 绘图</span></span><br><span class="line">plt.plot(<span class="built_in">range</span>(<span class="number">100</span>), loss_list[<span class="number">0</span>:<span class="number">100</span>])</span><br><span class="line">plt.ylabel(<span class="string">&quot;loss&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Epoch&quot;</span>)</span><br><span class="line">plt.show()  </span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/10/25/LxF4phI2AzJckHM.png" alt="image-20211025183650352"></p><p>预测</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y_predict = []</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> x_data:</span><br><span class="line">    temp = forward(x)</span><br><span class="line">    y_predict.append(temp)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(y_predict)</span><br><span class="line"><span class="comment"># 绘图</span></span><br><span class="line">plt.plot(x_data, y_predict,<span class="string">&#x27;o&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;y_predict&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;x_data&quot;</span>)</span><br><span class="line">plt.show()  </span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/10/25/MhnPQLv5yYDtfe4.png" alt="image-20211025184841418"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;pytorch&quot;&gt;&lt;a href=&quot;#pytorch&quot; class=&quot;headerlink&quot; title=&quot;pytorch&quot;&gt;&lt;/a&gt;pytorch&lt;/h1&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;</summary>
      
    
    
    
    <category term="pytorch" scheme="http://example.com/categories/pytorch/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="pytorch" scheme="http://example.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch深度学习一</title>
    <link href="http://example.com/post/d9099bcf.html"/>
    <id>http://example.com/post/d9099bcf.html</id>
    <published>2021-10-25T07:00:58.000Z</published>
    <updated>2021-10-25T09:27:35.179Z</updated>
    
    <content type="html"><![CDATA[<h1 id="pytorch"><a href="#pytorch" class="headerlink" title="pytorch"></a>pytorch</h1><h3 id="1、线性模型"><a href="#1、线性模型" class="headerlink" title="1、线性模型"></a>1、线性模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x * w</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    y_pred = forward(x)</span><br><span class="line">    <span class="keyword">return</span> (y_pred - y) * (y_pred - y)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 保存权重</span></span><br><span class="line">w_list = []</span><br><span class="line"><span class="comment"># 保存权重的损失函数值</span></span><br><span class="line">loss_list = []</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 损失函数MSE</span></span><br><span class="line"><span class="keyword">for</span> w <span class="keyword">in</span> np.arange(<span class="number">0.0</span>, <span class="number">4.1</span>, <span class="number">0.1</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;w = &#x27;</span>, w)</span><br><span class="line">    loss_sum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x_val, y_val <span class="keyword">in</span> <span class="built_in">zip</span>(x_data, y_data):</span><br><span class="line">        <span class="comment"># 为了打印y预测值，其实loss里也计算了</span></span><br><span class="line">        y_pred_val = forward(x_val)</span><br><span class="line">        loss_val = loss(x_val, y_val)</span><br><span class="line">        loss_sum += loss_val</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;\t&#x27;</span>, x_val, y_val,</span><br><span class="line">              y_pred_val, loss_val)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;loss = &#x27;</span>, loss_sum / <span class="number">3</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;☆&#x27;</span>*<span class="number">55</span>)</span><br><span class="line">    w_list.append(w)</span><br><span class="line">    loss_list.append(loss_sum / <span class="number">3</span>)</span><br></pre></td></tr></table></figure><p>结果：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">w =  0.0</span><br><span class="line"> 1.0 2.0 0.0 4.0</span><br><span class="line"> 2.0 4.0 0.0 16.0</span><br><span class="line"> 3.0 6.0 0.0 36.0</span><br><span class="line">loss =  18.666666666666668</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  0.1</span><br><span class="line"> 1.0 2.0 0.1 3.61</span><br><span class="line"> 2.0 4.0 0.2 14.44</span><br><span class="line"> 3.0 6.0 0.30000000000000004 32.49</span><br><span class="line">loss =  16.846666666666668</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  0.2</span><br><span class="line"> 1.0 2.0 0.2 3.24</span><br><span class="line"> 2.0 4.0 0.4 12.96</span><br><span class="line"> 3.0 6.0 0.6000000000000001 29.160000000000004</span><br><span class="line">loss =  15.120000000000003</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  0.30000000000000004</span><br><span class="line"> 1.0 2.0 0.30000000000000004 2.8899999999999997</span><br><span class="line"> 2.0 4.0 0.6000000000000001 11.559999999999999</span><br><span class="line"> 3.0 6.0 0.9000000000000001 26.009999999999998</span><br><span class="line">loss =  13.486666666666665</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  0.4</span><br><span class="line"> 1.0 2.0 0.4 2.5600000000000005</span><br><span class="line"> 2.0 4.0 0.8 10.240000000000002</span><br><span class="line"> 3.0 6.0 1.2000000000000002 23.04</span><br><span class="line">loss =  11.946666666666667</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  0.5</span><br><span class="line"> 1.0 2.0 0.5 2.25</span><br><span class="line"> 2.0 4.0 1.0 9.0</span><br><span class="line"> 3.0 6.0 1.5 20.25</span><br><span class="line">loss =  10.5</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  0.6000000000000001</span><br><span class="line"> 1.0 2.0 0.6000000000000001 1.9599999999999997</span><br><span class="line"> 2.0 4.0 1.2000000000000002 7.839999999999999</span><br><span class="line"> 3.0 6.0 1.8000000000000003 17.639999999999993</span><br><span class="line">loss =  9.146666666666663</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  0.7000000000000001</span><br><span class="line"> 1.0 2.0 0.7000000000000001 1.6899999999999995</span><br><span class="line"> 2.0 4.0 1.4000000000000001 6.759999999999998</span><br><span class="line"> 3.0 6.0 2.1 15.209999999999999</span><br><span class="line">loss =  7.886666666666666</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  0.8</span><br><span class="line"> 1.0 2.0 0.8 1.44</span><br><span class="line"> 2.0 4.0 1.6 5.76</span><br><span class="line"> 3.0 6.0 2.4000000000000004 12.959999999999997</span><br><span class="line">loss =  6.719999999999999</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  0.9</span><br><span class="line"> 1.0 2.0 0.9 1.2100000000000002</span><br><span class="line"> 2.0 4.0 1.8 4.840000000000001</span><br><span class="line"> 3.0 6.0 2.7 10.889999999999999</span><br><span class="line">loss =  5.646666666666666</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  1.0</span><br><span class="line"> 1.0 2.0 1.0 1.0</span><br><span class="line"> 2.0 4.0 2.0 4.0</span><br><span class="line"> 3.0 6.0 3.0 9.0</span><br><span class="line">loss =  4.666666666666667</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  1.1</span><br><span class="line"> 1.0 2.0 1.1 0.8099999999999998</span><br><span class="line"> 2.0 4.0 2.2 3.2399999999999993</span><br><span class="line"> 3.0 6.0 3.3000000000000003 7.289999999999998</span><br><span class="line">loss =  3.779999999999999</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  1.2000000000000002</span><br><span class="line"> 1.0 2.0 1.2000000000000002 0.6399999999999997</span><br><span class="line"> 2.0 4.0 2.4000000000000004 2.5599999999999987</span><br><span class="line"> 3.0 6.0 3.6000000000000005 5.759999999999997</span><br><span class="line">loss =  2.986666666666665</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  1.3</span><br><span class="line"> 1.0 2.0 1.3 0.48999999999999994</span><br><span class="line"> 2.0 4.0 2.6 1.9599999999999997</span><br><span class="line"> 3.0 6.0 3.9000000000000004 4.409999999999998</span><br><span class="line">loss =  2.2866666666666657</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  1.4000000000000001</span><br><span class="line"> 1.0 2.0 1.4000000000000001 0.3599999999999998</span><br><span class="line"> 2.0 4.0 2.8000000000000003 1.4399999999999993</span><br><span class="line"> 3.0 6.0 4.2 3.2399999999999993</span><br><span class="line">loss =  1.6799999999999995</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  1.5</span><br><span class="line"> 1.0 2.0 1.5 0.25</span><br><span class="line"> 2.0 4.0 3.0 1.0</span><br><span class="line"> 3.0 6.0 4.5 2.25</span><br><span class="line">loss =  1.1666666666666667</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  1.6</span><br><span class="line"> 1.0 2.0 1.6 0.15999999999999992</span><br><span class="line"> 2.0 4.0 3.2 0.6399999999999997</span><br><span class="line"> 3.0 6.0 4.800000000000001 1.4399999999999984</span><br><span class="line">loss =  0.746666666666666</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  1.7000000000000002</span><br><span class="line"> 1.0 2.0 1.7000000000000002 0.0899999999999999</span><br><span class="line"> 2.0 4.0 3.4000000000000004 0.3599999999999996</span><br><span class="line"> 3.0 6.0 5.1000000000000005 0.809999999999999</span><br><span class="line">loss =  0.4199999999999995</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  1.8</span><br><span class="line"> 1.0 2.0 1.8 0.03999999999999998</span><br><span class="line"> 2.0 4.0 3.6 0.15999999999999992</span><br><span class="line"> 3.0 6.0 5.4 0.3599999999999996</span><br><span class="line">loss =  0.1866666666666665</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  1.9000000000000001</span><br><span class="line"> 1.0 2.0 1.9000000000000001 0.009999999999999974</span><br><span class="line"> 2.0 4.0 3.8000000000000003 0.0399999999999999</span><br><span class="line"> 3.0 6.0 5.7 0.0899999999999999</span><br><span class="line">loss =  0.046666666666666586</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  2.0</span><br><span class="line"> 1.0 2.0 2.0 0.0</span><br><span class="line"> 2.0 4.0 4.0 0.0</span><br><span class="line"> 3.0 6.0 6.0 0.0</span><br><span class="line">loss =  0.0</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  2.1</span><br><span class="line"> 1.0 2.0 2.1 0.010000000000000018</span><br><span class="line"> 2.0 4.0 4.2 0.04000000000000007</span><br><span class="line"> 3.0 6.0 6.300000000000001 0.09000000000000043</span><br><span class="line">loss =  0.046666666666666835</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  2.2</span><br><span class="line"> 1.0 2.0 2.2 0.04000000000000007</span><br><span class="line"> 2.0 4.0 4.4 0.16000000000000028</span><br><span class="line"> 3.0 6.0 6.6000000000000005 0.36000000000000065</span><br><span class="line">loss =  0.18666666666666698</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  2.3000000000000003</span><br><span class="line"> 1.0 2.0 2.3000000000000003 0.09000000000000016</span><br><span class="line"> 2.0 4.0 4.6000000000000005 0.36000000000000065</span><br><span class="line"> 3.0 6.0 6.9 0.8100000000000006</span><br><span class="line">loss =  0.42000000000000054</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  2.4000000000000004</span><br><span class="line"> 1.0 2.0 2.4000000000000004 0.16000000000000028</span><br><span class="line"> 2.0 4.0 4.800000000000001 0.6400000000000011</span><br><span class="line"> 3.0 6.0 7.200000000000001 1.4400000000000026</span><br><span class="line">loss =  0.7466666666666679</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  2.5</span><br><span class="line"> 1.0 2.0 2.5 0.25</span><br><span class="line"> 2.0 4.0 5.0 1.0</span><br><span class="line"> 3.0 6.0 7.5 2.25</span><br><span class="line">loss =  1.1666666666666667</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  2.6</span><br><span class="line"> 1.0 2.0 2.6 0.3600000000000001</span><br><span class="line"> 2.0 4.0 5.2 1.4400000000000004</span><br><span class="line"> 3.0 6.0 7.800000000000001 3.2400000000000024</span><br><span class="line">loss =  1.6800000000000008</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  2.7</span><br><span class="line"> 1.0 2.0 2.7 0.49000000000000027</span><br><span class="line"> 2.0 4.0 5.4 1.960000000000001</span><br><span class="line"> 3.0 6.0 8.100000000000001 4.410000000000006</span><br><span class="line">loss =  2.2866666666666693</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  2.8000000000000003</span><br><span class="line"> 1.0 2.0 2.8000000000000003 0.6400000000000005</span><br><span class="line"> 2.0 4.0 5.6000000000000005 2.560000000000002</span><br><span class="line"> 3.0 6.0 8.4 5.760000000000002</span><br><span class="line">loss =  2.986666666666668</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  2.9000000000000004</span><br><span class="line"> 1.0 2.0 2.9000000000000004 0.8100000000000006</span><br><span class="line"> 2.0 4.0 5.800000000000001 3.2400000000000024</span><br><span class="line"> 3.0 6.0 8.700000000000001 7.290000000000005</span><br><span class="line">loss =  3.780000000000003</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  3.0</span><br><span class="line"> 1.0 2.0 3.0 1.0</span><br><span class="line"> 2.0 4.0 6.0 4.0</span><br><span class="line"> 3.0 6.0 9.0 9.0</span><br><span class="line">loss =  4.666666666666667</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  3.1</span><br><span class="line"> 1.0 2.0 3.1 1.2100000000000002</span><br><span class="line"> 2.0 4.0 6.2 4.840000000000001</span><br><span class="line"> 3.0 6.0 9.3 10.890000000000004</span><br><span class="line">loss =  5.646666666666668</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  3.2</span><br><span class="line"> 1.0 2.0 3.2 1.4400000000000004</span><br><span class="line"> 2.0 4.0 6.4 5.760000000000002</span><br><span class="line"> 3.0 6.0 9.600000000000001 12.96000000000001</span><br><span class="line">loss =  6.720000000000003</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  3.3000000000000003</span><br><span class="line"> 1.0 2.0 3.3000000000000003 1.6900000000000006</span><br><span class="line"> 2.0 4.0 6.6000000000000005 6.7600000000000025</span><br><span class="line"> 3.0 6.0 9.9 15.210000000000003</span><br><span class="line">loss =  7.886666666666668</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  3.4000000000000004</span><br><span class="line"> 1.0 2.0 3.4000000000000004 1.960000000000001</span><br><span class="line"> 2.0 4.0 6.800000000000001 7.840000000000004</span><br><span class="line"> 3.0 6.0 10.200000000000001 17.640000000000008</span><br><span class="line">loss =  9.14666666666667</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  3.5</span><br><span class="line"> 1.0 2.0 3.5 2.25</span><br><span class="line"> 2.0 4.0 7.0 9.0</span><br><span class="line"> 3.0 6.0 10.5 20.25</span><br><span class="line">loss =  10.5</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  3.6</span><br><span class="line"> 1.0 2.0 3.6 2.5600000000000005</span><br><span class="line"> 2.0 4.0 7.2 10.240000000000002</span><br><span class="line"> 3.0 6.0 10.8 23.040000000000006</span><br><span class="line">loss =  11.94666666666667</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  3.7</span><br><span class="line"> 1.0 2.0 3.7 2.8900000000000006</span><br><span class="line"> 2.0 4.0 7.4 11.560000000000002</span><br><span class="line"> 3.0 6.0 11.100000000000001 26.010000000000016</span><br><span class="line">loss =  13.486666666666673</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  3.8000000000000003</span><br><span class="line"> 1.0 2.0 3.8000000000000003 3.240000000000001</span><br><span class="line"> 2.0 4.0 7.6000000000000005 12.960000000000004</span><br><span class="line"> 3.0 6.0 11.4 29.160000000000004</span><br><span class="line">loss =  15.120000000000005</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  3.9000000000000004</span><br><span class="line"> 1.0 2.0 3.9000000000000004 3.610000000000001</span><br><span class="line"> 2.0 4.0 7.800000000000001 14.440000000000005</span><br><span class="line"> 3.0 6.0 11.700000000000001 32.49000000000001</span><br><span class="line">loss =  16.84666666666667</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br><span class="line">w =  4.0</span><br><span class="line"> 1.0 2.0 4.0 4.0</span><br><span class="line"> 2.0 4.0 8.0 16.0</span><br><span class="line"> 3.0 6.0 12.0 36.0</span><br><span class="line">loss =  18.666666666666668</span><br><span class="line">☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"> <span class="comment"># 绘loss变化图，横坐标是w，纵坐标是loss</span></span><br><span class="line">plt.plot(w_list, loss_list)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;w&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/10/25/ANq6IBCF9OEZnov.png" alt="image-20211025164754953"></p><p>预测</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 当w为2时间，损失最小</span></span><br><span class="line">y_pers = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> x_data:</span><br><span class="line">    y_per =<span class="number">2</span> * i</span><br><span class="line">    y_pers.append(y_per)</span><br><span class="line">    </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;预测结果:&quot;</span>,<span class="built_in">str</span>(y_pers))    </span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">预测结果: [<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.plot(x_data,y_data,<span class="string">&#x27;o&#x27;</span>,color=<span class="string">&#x27;red&#x27;</span> )</span><br><span class="line">plt.plot(x_data,y_pers)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;predict&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/10/25/XjHdxEn95cB1U84.png" alt="image-20211025164651345"></p><p><img src="https://i.loli.net/2021/10/25/dv7NYHnSXxLjVCK.jpg"></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">实现线性模型（y = w x  + b）并输出loss的3D图像</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> cm</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 线性模型 加入一个偏置b</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">x,w,b</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x * w + b</span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失函数 mse损失</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">x, y, w, b</span>):</span></span><br><span class="line">    y_pred = forward(x, w, b)</span><br><span class="line">    <span class="keyword">return</span> (y_pred - y) * (y_pred - y)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#计算每个x，b对应的损失loss</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_loss</span>(<span class="params">w,b</span>):</span></span><br><span class="line">    l_sum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x_val, y_val <span class="keyword">in</span> <span class="built_in">zip</span>(x_data, y_data):</span><br><span class="line">        y_pred_val = forward(x_val,w,b)</span><br><span class="line">        loss_val = loss(x_val, y_val,w,b)</span><br><span class="line">        l_sum += loss_val</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;\t&#x27;</span>, x_val, y_val, y_pred_val, loss_val)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;loss=&#x27;</span>, l_sum / <span class="number">3</span>)</span><br><span class="line">    <span class="keyword">return</span>  l_sum/<span class="number">3</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.定义网格化数据</span></span><br><span class="line">b_list=np.arange(-<span class="number">30</span>,<span class="number">30</span>,<span class="number">0.1</span>)</span><br><span class="line">w_list=np.arange(-<span class="number">30</span>,<span class="number">30</span>,<span class="number">0.1</span>);</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 2.生成网格化数据</span></span><br><span class="line">xx, yy = np.meshgrid(b_list, w_list, sparse=<span class="literal">False</span>, indexing=<span class="string">&#x27;xy&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.每个点的对应高度 loss</span></span><br><span class="line">zz=get_loss(xx,yy)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"> 1.0 2.0 [[-6.00000000e+01 -5.99000000e+01 -5.98000000e+01 ... -3.00000000e-01</span><br><span class="line">  -2.00000000e-01 -1.00000000e-01]</span><br><span class="line"> [-5.99000000e+01 -5.98000000e+01 -5.97000000e+01 ... -2.00000000e-01</span><br><span class="line">  -1.00000000e-01  8.52651283e-13]</span><br><span class="line"> [-5.98000000e+01 -5.97000000e+01 -5.96000000e+01 ... -1.00000000e-01</span><br><span class="line">   8.52651283e-13  1.00000000e-01]</span><br><span class="line"> ...</span><br><span class="line"> [-3.00000000e-01 -2.00000000e-01 -1.00000000e-01 ...  5.94000000e+01</span><br><span class="line">   5.95000000e+01  5.96000000e+01]</span><br><span class="line"> [-2.00000000e-01 -1.00000000e-01  8.52651283e-13 ...  5.95000000e+01</span><br><span class="line">   5.96000000e+01  5.97000000e+01]</span><br><span class="line"> [-1.00000000e-01  8.52651283e-13  1.00000000e-01 ...  5.96000000e+01</span><br><span class="line">   5.97000000e+01  5.98000000e+01]] [[3.84400e+03 3.83161e+03 3.81924e+03 ... 5.29000e+00 4.84000e+00</span><br><span class="line">  4.41000e+00]</span><br><span class="line"> [3.83161e+03 3.81924e+03 3.80689e+03 ... 4.84000e+00 4.41000e+00</span><br><span class="line">  4.00000e+00]</span><br><span class="line"> [3.81924e+03 3.80689e+03 3.79456e+03 ... 4.41000e+00 4.00000e+00</span><br><span class="line">  3.61000e+00]</span><br><span class="line"> ...</span><br><span class="line"> [5.29000e+00 4.84000e+00 4.41000e+00 ... 3.29476e+03 3.30625e+03</span><br><span class="line">  3.31776e+03]</span><br><span class="line"> [4.84000e+00 4.41000e+00 4.00000e+00 ... 3.30625e+03 3.31776e+03</span><br><span class="line">  3.32929e+03]</span><br><span class="line"> [4.41000e+00 4.00000e+00 3.61000e+00 ... 3.31776e+03 3.32929e+03</span><br><span class="line">  3.34084e+03]]</span><br><span class="line"> 2.0 4.0 [[-90.  -89.8 -89.6 ...  29.4  29.6  29.8]</span><br><span class="line"> [-89.9 -89.7 -89.5 ...  29.5  29.7  29.9]</span><br><span class="line"> [-89.8 -89.6 -89.4 ...  29.6  29.8  30. ]</span><br><span class="line"> ...</span><br><span class="line"> [-30.3 -30.1 -29.9 ...  89.1  89.3  89.5]</span><br><span class="line"> [-30.2 -30.  -29.8 ...  89.2  89.4  89.6]</span><br><span class="line"> [-30.1 -29.9 -29.7 ...  89.3  89.5  89.7]] [[8836.   8798.44 8760.96 ...  645.16  655.36  665.64]</span><br><span class="line"> [8817.21 8779.69 8742.25 ...  650.25  660.49  670.81]</span><br><span class="line"> [8798.44 8760.96 8723.56 ...  655.36  665.64  676.  ]</span><br><span class="line"> ...</span><br><span class="line"> [1176.49 1162.81 1149.21 ... 7242.01 7276.09 7310.25]</span><br><span class="line"> [1169.64 1156.   1142.44 ... 7259.04 7293.16 7327.36]</span><br><span class="line"> [1162.81 1149.21 1135.69 ... 7276.09 7310.25 7344.49]]</span><br><span class="line"> 3.0 6.0 [[-120.  -119.7 -119.4 ...   59.1   59.4   59.7]</span><br><span class="line"> [-119.9 -119.6 -119.3 ...   59.2   59.5   59.8]</span><br><span class="line"> [-119.8 -119.5 -119.2 ...   59.3   59.6   59.9]</span><br><span class="line"> ...</span><br><span class="line"> [ -60.3  -60.   -59.7 ...  118.8  119.1  119.4]</span><br><span class="line"> [ -60.2  -59.9  -59.6 ...  118.9  119.2  119.5]</span><br><span class="line"> [ -60.1  -59.8  -59.5 ...  119.   119.3  119.6]] [[15876.   15800.49 15725.16 ...  2819.61  2851.56  2883.69]</span><br><span class="line"> [15850.81 15775.36 15700.09 ...  2830.24  2862.25  2894.44]</span><br><span class="line"> [15825.64 15750.25 15675.04 ...  2840.89  2872.96  2905.21]</span><br><span class="line"> ...</span><br><span class="line"> [ 4395.69  4356.    4316.49 ... 12723.84 12791.61 12859.56]</span><br><span class="line"> [ 4382.44  4342.81  4303.36 ... 12746.41 12814.24 12882.25]</span><br><span class="line"> [ 4369.21  4329.64  4290.25 ... 12769.   12836.89 12904.96]]</span><br><span class="line">loss= [[9518.66666667 9476.84666667 9435.12       ... 1156.68666667</span><br><span class="line">  1170.58666667 1184.58      ]</span><br><span class="line"> [9499.87666667 9458.09666667 9416.41       ... 1161.77666667</span><br><span class="line">  1175.71666667 1189.75      ]</span><br><span class="line"> [9481.10666667 9439.36666667 9397.72       ... 1166.88666667</span><br><span class="line">  1180.86666667 1194.94      ]</span><br><span class="line"> ...</span><br><span class="line"> [1859.15666667 1841.21666667 1823.37       ... 7753.53666667</span><br><span class="line">  7791.31666667 7829.19      ]</span><br><span class="line"> [1852.30666667 1834.40666667 1816.6        ... 7770.56666667</span><br><span class="line">  7808.38666667 7846.3       ]</span><br><span class="line"> [1845.47666667 1827.61666667 1809.85       ... 7787.61666667</span><br><span class="line">  7825.47666667 7863.43      ]]</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">fig = plt.figure()</span><br><span class="line">ax = Axes3D(fig)</span><br><span class="line">plt.title(&quot;3D&quot;)</span><br><span class="line">ax.plot_surface(xx, yy, zz, cmap=plt.get_cmap(&#x27;rainbow&#x27;)) # 设置曲面的颜色</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/10/25/gj2lr6tU3s9qZhk.png" alt="image-20211025172320588"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;pytorch&quot;&gt;&lt;a href=&quot;#pytorch&quot; class=&quot;headerlink&quot; title=&quot;pytorch&quot;&gt;&lt;/a&gt;pytorch&lt;/h1&gt;&lt;h3 id=&quot;1、线性模型&quot;&gt;&lt;a href=&quot;#1、线性模型&quot; class=&quot;headerlink&quot; </summary>
      
    
    
    
    <category term="pytorch" scheme="http://example.com/categories/pytorch/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="pytorch" scheme="http://example.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>注意力机制</title>
    <link href="http://example.com/post/141d1667.html"/>
    <id>http://example.com/post/141d1667.html</id>
    <published>2021-10-24T07:06:02.000Z</published>
    <updated>2021-10-24T17:41:34.111Z</updated>
    
    <content type="html"><![CDATA[<h1 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">注意力机制的核心在于通过计算一个注意力map，来强调最相关的特征，并避免不相关特征的干扰。</span><br><span class="line">获取注意力map的方法可分为两类：无参数、有参数，如图6所示，主要的区别在于注意图中的重要性权重是否可学习：</span><br><span class="line">注意力机制为深度网络提供了突出给定图像中最重要区域的能力</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1D 通道注意力(任务)</span><br><span class="line">2D 空间注意力</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">对于1D数据来说，在注意力方面，SE仅关注了通道注意力，没考虑空间方面的注意力。</span><br></pre></td></tr></table></figure><h3 id="Encoder-Decoder"><a href="#Encoder-Decoder" class="headerlink" title="Encoder-Decoder"></a>Encoder-Decoder</h3><h3 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h3><h3 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h3>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;注意力机制&quot;&gt;&lt;a href=&quot;#注意力机制&quot; class=&quot;headerlink&quot; title=&quot;注意力机制&quot;&gt;&lt;/a&gt;注意力机制&lt;/h1&gt;&lt;figure class=&quot;highlight plaintext&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;cod</summary>
      
    
    
    
    <category term="深度学习" scheme="http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="人工智能" scheme="http://example.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="注意力机制" scheme="http://example.com/tags/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"/>
    
  </entry>
  
  <entry>
    <title>人脸识别一</title>
    <link href="http://example.com/post/937cd58.html"/>
    <id>http://example.com/post/937cd58.html</id>
    <published>2021-10-24T05:57:51.000Z</published>
    <updated>2021-10-24T17:41:34.109Z</updated>
    
    
    
    
    
  </entry>
  
  <entry>
    <title>目标检测一</title>
    <link href="http://example.com/post/f9b95e7d.html"/>
    <id>http://example.com/post/f9b95e7d.html</id>
    <published>2021-10-24T05:57:22.000Z</published>
    <updated>2021-10-24T17:41:34.112Z</updated>
    
    <content type="html"><![CDATA[<h3 id="YOLO"><a href="#YOLO" class="headerlink" title="YOLO"></a>YOLO</h3><h3 id="YOLO的改进"><a href="#YOLO的改进" class="headerlink" title="YOLO的改进"></a>YOLO的改进</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1、去掉FPN</span><br><span class="line">yolo-f</span><br><span class="line"></span><br><span class="line">2、anchor Free</span><br><span class="line">yolo-x</span><br><span class="line">将每个位置预测个数从3减少到1，并直接预测四个值（即：到网格的左上角的偏移量和box的高宽）；同时，将中心点设为正样本，并预设了一个尺度范围为每个对象指定FPN级别。</span><br><span class="line"></span><br><span class="line">3、NMS-Free</span><br><span class="line">OneNet</span><br><span class="line">4、</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;YOLO&quot;&gt;&lt;a href=&quot;#YOLO&quot; class=&quot;headerlink&quot; title=&quot;YOLO&quot;&gt;&lt;/a&gt;YOLO&lt;/h3&gt;&lt;h3 id=&quot;YOLO的改进&quot;&gt;&lt;a href=&quot;#YOLO的改进&quot; class=&quot;headerlink&quot; title=&quot;YOL</summary>
      
    
    
    
    <category term="目标检测" scheme="http://example.com/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
    
    <category term="人工智能" scheme="http://example.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="目标检测" scheme="http://example.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>深度学习之生成网络cyclegan</title>
    <link href="http://example.com/post/8b28088e.html"/>
    <id>http://example.com/post/8b28088e.html</id>
    <published>2021-10-23T16:43:40.000Z</published>
    <updated>2021-10-23T19:39:41.301Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Cyclegan"><a href="#Cyclegan" class="headerlink" title="Cyclegan"></a>Cyclegan</h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">贡献：</span><br><span class="line">提出了循环一致性损失</span><br><span class="line">使用非对称的数据就可以进行风格转换</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">总共使用了两队生成器和判别器。</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/10/24/Bdo5CKXvTqup4jJ.png"></p><p><img src="https://i.loli.net/2021/10/24/XCteayNqGjv46ZW.png"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">循环一致性损失：相当于一张图片风格为A，通过G_AB 生成风格为B的假图 -&gt; fake_B,</span><br><span class="line">然后通过 G_BA 生成风格为A假图 fake_A,最终得到的fake_A能够骗过D_A，反之亦然。</span><br><span class="line">A -G_AB&gt; fake_B -G_BA&gt; fake_A</span><br><span class="line">为什么循环一致性损失能够保证在非对称的数据下，图像的内容信息不会改变？</span><br><span class="line"></span><br><span class="line">简单来说：判别器是判断图像风格的，如果没有循环一致性损失时，从源域（风格A）到目标域（风格B）中任意一个或许是最简单的方法，</span><br><span class="line">但是当加入了循环一致性损失后，从源域到目标域的变化中，最简单的方法是只变化风格，而不变化内容，从而约束了生成器的生成。</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Cyclegan&quot;&gt;&lt;a href=&quot;#Cyclegan&quot; class=&quot;headerlink&quot; title=&quot;Cyclegan&quot;&gt;&lt;/a&gt;Cyclegan&lt;/h1&gt;&lt;figure class=&quot;highlight plaintext&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;t</summary>
      
    
    
    
    <category term="生成网络" scheme="http://example.com/categories/%E7%94%9F%E6%88%90%E7%BD%91%E7%BB%9C/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="人工智能" scheme="http://example.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="生成网络" scheme="http://example.com/tags/%E7%94%9F%E6%88%90%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>深度学习之生成网络gan</title>
    <link href="http://example.com/post/f5c1817e.html"/>
    <id>http://example.com/post/f5c1817e.html</id>
    <published>2021-10-23T16:43:16.000Z</published>
    <updated>2021-10-23T17:04:46.423Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Gan"><a href="#Gan" class="headerlink" title="Gan"></a>Gan</h1><h1 id="Gan的改进"><a href="#Gan的改进" class="headerlink" title="Gan的改进"></a>Gan的改进</h1>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Gan&quot;&gt;&lt;a href=&quot;#Gan&quot; class=&quot;headerlink&quot; title=&quot;Gan&quot;&gt;&lt;/a&gt;Gan&lt;/h1&gt;&lt;h1 id=&quot;Gan的改进&quot;&gt;&lt;a href=&quot;#Gan的改进&quot; class=&quot;headerlink&quot; title=&quot;Gan的改进&quot;&gt;&lt;</summary>
      
    
    
    
    <category term="生成网络" scheme="http://example.com/categories/%E7%94%9F%E6%88%90%E7%BD%91%E7%BB%9C/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="人工智能" scheme="http://example.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="生成网络" scheme="http://example.com/tags/%E7%94%9F%E6%88%90%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>深度学习基础一</title>
    <link href="http://example.com/post/9f799b3c.html"/>
    <id>http://example.com/post/9f799b3c.html</id>
    <published>2021-10-23T16:20:02.000Z</published>
    <updated>2021-10-24T17:41:10.521Z</updated>
    
    <content type="html"><![CDATA[<h1 id="卷积-cnn"><a href="#卷积-cnn" class="headerlink" title="卷积(cnn)"></a>卷积(cnn)</h1><p><img src="https://i.loli.net/2021/10/24/YqCZOxnU51ELi8Q.png"></p><p><img src="https://i.loli.net/2021/10/24/7n9Si4u1d6l3oIN.gif"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">卷积核：简单来说就可以看做一个特征提取器。</span><br><span class="line"></span><br><span class="line">步长stride：每次卷积核在特征图上滑动的步长，默认只滑动一个像素。</span><br><span class="line">当滑动的步长越大，输出的特征图(feature map)就越小，相当于下面说的，池化操作。</span><br><span class="line"></span><br><span class="line">padding操作：填充，对于图像边缘的像素来说，只进行了一次卷积操作，而内部的卷积则进行了至少两次卷积操作，</span><br><span class="line">作用有两个：</span><br><span class="line">1、通过填充后，可以使得卷积后的feature map 和原来的feature map一样的大小,</span><br><span class="line">可以用来控制卷积层输出的特征图的大小。</span><br><span class="line">2、通过对边缘向外填充像素点，来使得边缘像素点，也来充分提取特征。</span><br><span class="line"></span><br><span class="line">感受野：由于卷积操作后，假如是3*3的卷积核，对应于原来的feature map，3*3区域大小的像素</span><br><span class="line">就变为了一个像素区域，因此输出的feature map就相当于原来的3*3的大小，随着卷积层的</span><br><span class="line">加深，感受野也会越来越大。</span><br></pre></td></tr></table></figure><p>当只有一个卷积核时学习到的特征有限，因此需要多个卷积核：</p><p><img src="https://i.loli.net/2021/10/24/upofgIP5DrBdbXe.png"></p><p><img src="https://i.loli.net/2021/10/24/in7tUKYS4jWOywx.png" alt="0005"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">一组多个卷积核就能学到多个特征。</span><br><span class="line">对于k个卷积核中的一个卷积核，每张特征图上对应一个卷积核，</span><br><span class="line">在卷积的时候，每张特征图上卷积核不是同一个卷积核，</span><br><span class="line">也就是说对于3通道来说，卷积核就变为了3D，也就是3个卷积核。</span><br><span class="line">卷积完后，然后相加，就变为了一个feature map</span><br><span class="line">对于k个卷积核，最终会得到k个feature map。</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/10/24/qO12soTD6GRlpc3.png"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">通过上面的例子再说一下：</span><br><span class="line">1、上面图上有两个卷积核，都是3*3的，每个卷积核上的值叫做权重  Bias b是偏置，可以不用管</span><br><span class="line">2、3*3的卷积核，因为上一层feature map是3个，也就是通道数是3，卷积核就会变为3*3*3</span><br><span class="line">3、且每个卷积核不是同一个卷积核，然后每个卷积核和对应的卷积层进行卷积，得到的值，</span><br><span class="line">然后相加，最终变为一个值，图中就是5.</span><br></pre></td></tr></table></figure><h3 id="1、输入层"><a href="#1、输入层" class="headerlink" title="1、输入层"></a>1、输入层</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">一般是一张图像 H*W*C</span><br><span class="line">H：图像高 W ：图像宽 c：图像通道数，刚开始图像通道数为3，RGB</span><br></pre></td></tr></table></figure><h3 id="2、卷积层"><a href="#2、卷积层" class="headerlink" title="2、卷积层"></a>2、卷积层</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">就是我们刚才说的卷积操作。</span><br><span class="line"></span><br><span class="line">有一点需要注意，每层的卷积核是权重共享的。</span><br><span class="line"></span><br><span class="line">为什么要多层卷积？？？？</span><br><span class="line">卷积一直在做的一件事就是对特征提取。</span><br><span class="line">多层卷积相当于做了多次特征提取，随着网络的加深提取到的特征越来越特殊，</span><br><span class="line">越来越能代表这种物体的特征。</span><br></pre></td></tr></table></figure><h3 id="3、池化层"><a href="#3、池化层" class="headerlink" title="3、池化层"></a>3、池化层</h3><p><img src="https://i.loli.net/2021/10/24/uGerRDV4a5oOZv2.png" alt="0006"></p><p><img src="https://i.loli.net/2021/10/24/SJ1AoQqvflEc7eZ.png" alt="0007"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">特征选择作用：采样层实际上就是一个特征选择的过程，挑选出对我们最有用的信息，去除多余的，冗余的信息</span><br><span class="line">1、最大池化</span><br><span class="line">和卷积核大小类似，加入说我们选2*2的大小，进行池化</span><br><span class="line">最大池化就是说从这2*2 ，也就是4个点中，选择最大的那个，其他舍弃</span><br><span class="line">池化可以一定程度提高空间不变性，比如旋转后，和旋转前，池化是一样的，但是局部有限区域内。</span><br><span class="line"></span><br><span class="line">2、平均池化</span><br><span class="line">就是把四个数加起来除4，当做特征值</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">可以看到，池化说到底还是一个特征选择，信息过滤的过程，我们会损失了一部分信息，来减小参数，计算量</span><br><span class="line">但是现在有的地方也不用池化层了。</span><br></pre></td></tr></table></figure><h3 id="4、激活函数-加入非线性"><a href="#4、激活函数-加入非线性" class="headerlink" title="4、激活函数(加入非线性)"></a>4、激活函数(加入非线性)</h3><p><img src="https://i.loli.net/2021/10/24/hLZuNxep4nrMyzf.png" alt="000"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">对于原有的卷积层，只能处理线性问题，对于非线性分类问题解决不了。</span><br><span class="line">那么无论神经网络的层数有多少还是在解决线性函数问题，因为两个线性函数的组合还是线性的。</span><br><span class="line"></span><br><span class="line">因此需要引入非线性函数。</span><br><span class="line">sigmoid函数：</span><br><span class="line">在反向传播时，容易出现梯度消失</span><br><span class="line">计算量大</span><br><span class="line">tanh函数：</span><br><span class="line">在反向传播时，容易出现梯度消失</span><br><span class="line">计算量大</span><br><span class="line">Relu函数：</span><br><span class="line">优点：避免梯度消失的问题，通过使得部分神经元为0，抑制作用，避免过拟合问题</span><br><span class="line">缺点：当值为负数的时候，就会变为0，神经元完全不起作用。</span><br><span class="line"></span><br><span class="line">注：现在一般都会使用Relu函数和其改进版本，如ELU、PRelu等，从某种程度上避免了使部分神经元死掉的问题。</span><br></pre></td></tr></table></figure><h3 id="5、标准化层"><a href="#5、标准化层" class="headerlink" title="5、标准化层"></a>5、标准化层</h3><p><img src="https://i.loli.net/2021/10/24/uBCUmf89LDoShkN.png"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">什么是标准化？？？</span><br><span class="line">数据的标准化是指将数据按照比例缩放,使之落入一个特定的区间.</span><br><span class="line"></span><br><span class="line">为什么要做标准化？？？</span><br><span class="line">1. 加快网络的训练和收敛的速度</span><br><span class="line">将数据分布在均值为零，方差为1状态下，使得梯度稳定，容易收敛。</span><br><span class="line"></span><br><span class="line">2. 控制梯度爆炸和防止梯度消失</span><br><span class="line">控制数据集中分布在0值附近，有两个好处，</span><br><span class="line">例如sigmoid函数：</span><br><span class="line">1、在经过sigmoid函数将值约束到0附近，梯度不会消失。</span><br><span class="line">因此通常会加在全连接和激励函数之间。</span><br><span class="line">2、如果不使用标准化可能初始loss过大，梯度反向传播就会积累，前面几层会变得非常大，产生梯度爆炸的问题。</span><br><span class="line">而使用标准化后权值就不会很大了。</span><br><span class="line"></span><br><span class="line">3. 防止过拟合</span><br><span class="line">标准化层在使用过程中，通常会以考虑整个batch的数据进行标准化，</span><br><span class="line">考虑整体比只考虑单个肯定能够一定程度上解决过拟合问题</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="6、全连接层"><a href="#6、全连接层" class="headerlink" title="6、全连接层"></a>6、全连接层</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">为什么需要全连接层？</span><br><span class="line">卷积层我们会发现，有一个天生致命的缺点，那就是没有全局，</span><br><span class="line">因此我们需要一个来把握全局的，那就是全连接层。</span><br><span class="line">相当于把前面，每个学习到的特征进行一个汇总操作，使得我们能够把握到整体。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">全连接层如果加深层数，增加神经元输量，网络能力会得到提升，但是也有可能出现过拟合问题。</span><br></pre></td></tr></table></figure><h3 id="7、输出层"><a href="#7、输出层" class="headerlink" title="7、输出层"></a>7、输出层</h3><p><img src="https://i.loli.net/2021/10/25/hDmXqrZFkHx2lBe.png"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">对于分类任务来说：</span><br><span class="line">FC+Softmax+Cross-entropy loss</span><br><span class="line">1、当输入为X, 预测类别为j 的概率为P</span><br><span class="line">2、所有预测类别概率和为1</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">一句话说就是：卷积层负责提取特征，池化负责特征选择，激活函数增加非线性能力，标准化层用来约束数据分布，全连接层负责分类</span><br></pre></td></tr></table></figure><h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><h3 id="分类损失"><a href="#分类损失" class="headerlink" title="分类损失"></a>分类损失</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1、交叉熵损失CE</span><br><span class="line">用于度量两个函数的分布.</span><br><span class="line"></span><br><span class="line">信息熵:去掉冗余信息后的平均信息量。衡量不确定性，不确定性大，信息熵就越大。</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">相对熵(KL散度): 下图</span><br><span class="line">KL散度用于估计两个分布的相似性</span><br><span class="line">其中l(p,p)是分布p的熵，而l(p,q)就是p和q的交叉熵。</span><br><span class="line">假如p是一个已知的分布，则熵是一个常数。</span><br><span class="line">此时KL与l(p,q)也就是交叉熵只有一个常数的差异，两者是等价的。</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/10/24/ae6Rd5IFBGs7Myw.jpg"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">交叉熵CE:</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/10/24/kSmxX8PlvJoji9E.jpg"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">由于交叉熵正负样本平衡，对于正负样本不平衡网络就不能很好的学习</span><br><span class="line">2、平衡交叉熵损失（正负样本不平衡问题）</span><br><span class="line">3、专注难样本 Focal loss（难易样本）</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">在工程上，经常和softmax loss一起使用：</span><br></pre></td></tr></table></figure><p>$$<br>softmax<br>$$</p><p><img src="https://i.loli.net/2021/10/25/ZVeDqaf5v4EnKm9.png" alt="image-20211025004211016"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">假如</span><br><span class="line"></span><br><span class="line">输出为 [0.3 0.3 0.4] 目标是 [0 0 1]</span><br><span class="line">则：</span><br><span class="line">交叉熵为：- (ln(0.3) * 0 + ln(0.3) * 0 + ln(0.4) * 1 ) = - ln4</span><br></pre></td></tr></table></figure><h3 id="对于softmax的改进："><a href="#对于softmax的改进：" class="headerlink" title="对于softmax的改进："></a>对于softmax的改进：</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1、Large-Margin Softmax Loss L-Softmax loss</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/10/25/KIV7MAyW6j8sdSB.png" alt="image-20211025005021695"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">这张图显示的是不同softmax loss和L-Softmax loss学习到的特征分布。</span><br><span class="line">第一列就是softmax，第2列是L-Softmax </span><br><span class="line">loss在参数m取不同值时的分布。通过可视化特征可知学习到的类间的特征是比较明显的，但是类内比较散。</span><br><span class="line"></span><br><span class="line">也就是说使得不同类别之间的夹角增大，同时同类分布也更为紧凑。</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/10/25/aIugtWSJpTvlKxC.png" alt="image-20211025011546484"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">因为最后是全连接层的输出：</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/10/25/aWOm7K2qAoDjd3S.png" alt="image-20211025011903423"></p><p><img src="https://i.loli.net/2021/10/25/5dRUQVgxm8vTjnA.png" alt="image-20211025011941901"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">因为cos在[0,π]上是减函数，</span><br><span class="line">当f1 &gt; f2 时，也就是说θ1&lt;θ2,样本将被分类为类别1,</span><br><span class="line">当f1 &lt; f2 时，也就是说θ1&gt;θ2,样本将被分类为类别2,</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/10/25/uBvEb68Dyoh5TZn.png"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">L-Softmax损失函数中对角度施加了更为强烈的约束,m&gt;=1</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/10/25/59VNlbKSinOh3zr.png" alt="image-20211025012636114"></p><p><img src="https://i.loli.net/2021/10/25/3L9CA2Qxe4GdlJM.png" alt="image-20211025012807781"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">2、Angular Softmax Loss A-Softmax</span><br><span class="line">Angular Softmax Loss(简称A-Softmax loss)与L-Softmax思想类似，主要区别是进一步加入了一个权重约束。</span><br><span class="line">使得||w|| = 1，就变成了</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/10/25/3GcBTQE9bgL4sVW.png" alt="image-20211025005437525"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">使得类别的判断将只依赖于样本与类别权重的夹角。</span><br></pre></td></tr></table></figure><h3 id="回归损失"><a href="#回归损失" class="headerlink" title="回归损失"></a>回归损失</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1、L1损失</span><br><span class="line">Mean absolute loss(MAE)也被称为L1 Loss，是以绝对误差作为距离</span><br><span class="line">由于L1 loss具有稀疏性，为了惩罚较大的值，因此常常将其作为正则项添加到其他loss中作为约束。</span><br><span class="line">L1 loss的最大问题是梯度在零点不平滑，导致会跳过极小值。</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/10/24/53QFT6aUDu4GqrX.png"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">2、L2损失</span><br><span class="line">Mean Squared Loss/ Quadratic Loss(MSE loss)也被称为L2 loss，或欧氏距离，它以误差的平方和作为距离：</span><br><span class="line">L2 loss也常常作为正则项。当预测值与目标值相差很大时, 梯度容易爆炸，因为梯度里包含了x−t。</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/10/24/oFuUOi58LA6ItaT.png"></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">3、L1 loss与L2 loss的改进</span><br><span class="line"></span><br><span class="line">原始的L1 loss和L2 loss都有缺陷，比如L1 loss的最大问题是梯度不平滑，而L2 loss的最大问题是容易梯度爆炸，所以研究者们对其提出了很多的改进。</span><br><span class="line"></span><br><span class="line">在faster rcnn框架中，使用了smooth L1 loss来综合L1与L2 loss的优点。</span><br><span class="line"></span><br><span class="line">在x比较小时，上式等价于L2 loss，保持平滑。在x比较大时，上式等价于L1 loss，可以限制数值的大小。</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/10/24/NiLIMr5eu2FDfk8.jpg"></p><p><img src="https://i.loli.net/2021/10/24/cumGWvkODPKZeqr.png" alt="image-20211024235947407"></p><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;卷积-cnn&quot;&gt;&lt;a href=&quot;#卷积-cnn&quot; class=&quot;headerlink&quot; title=&quot;卷积(cnn)&quot;&gt;&lt;/a&gt;卷积(cnn)&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/10/24/YqCZOxnU51EL</summary>
      
    
    
    
    <category term="深度学习基础" scheme="http://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"/>
    
    
    <category term="深度学习" scheme="http://example.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="人工智能" scheme="http://example.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
  </entry>
  
  <entry>
    <title>英语单词一</title>
    <link href="http://example.com/post/afe428e5.html"/>
    <id>http://example.com/post/afe428e5.html</id>
    <published>2021-10-23T16:17:38.000Z</published>
    <updated>2021-10-23T16:38:04.481Z</updated>
    
    <content type="html"><![CDATA[<h1 id="A"><a href="#A" class="headerlink" title="A"></a>A</h1>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;A&quot;&gt;&lt;a href=&quot;#A&quot; class=&quot;headerlink&quot; title=&quot;A&quot;&gt;&lt;/a&gt;A&lt;/h1&gt;</summary>
      
    
    
    
    <category term="英语" scheme="http://example.com/categories/%E8%8B%B1%E8%AF%AD/"/>
    
    
    <category term="英语" scheme="http://example.com/tags/%E8%8B%B1%E8%AF%AD/"/>
    
  </entry>
  
  <entry>
    <title>test</title>
    <link href="http://example.com/post/d87f7e0c.html"/>
    <id>http://example.com/post/d87f7e0c.html</id>
    <published>2021-10-23T15:15:36.000Z</published>
    <updated>2021-10-23T15:17:50.455Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">test文件</span><br><span class="line">撒很难过船</span><br><span class="line"></span><br><span class="line">大萨达</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2021/10/23/eKFWfmxuVjSndR6.jpg"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;figure class=&quot;highlight plaintext&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;test文件&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;撒很难过船&lt;/span&gt;&lt;br&gt;</summary>
      
    
    
    
    <category term="人工智能" scheme="http://example.com/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    
    <category term="人工智能" scheme="http://example.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="python" scheme="http://example.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://example.com/post/16107.html"/>
    <id>http://example.com/post/16107.html</id>
    <published>2021-10-23T06:47:10.453Z</published>
    <updated>2021-10-23T11:16:13.161Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for</summary>
      
    
    
    
    
  </entry>
  
</feed>
