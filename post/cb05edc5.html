<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>pytorch基础一 | blog</title><meta name="keywords" content="深度学习,pytorch"><meta name="author" content="祎熵"><meta name="copyright" content="祎熵"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="pytorch基础一、tensortensor是一种专有的数据结构，与数组和矩阵非常相似。在PyTorch中，我们使用tensor来编码一个模型的输入和输出，以及模型的参数。tensor类似于NumPy的ndarrays，只是tensor可以在GPU或其他硬件加速器上运行。事实上，tensor和NumPy数组通常可以共享相同的底层内存，不需要复制数据。tensor还为自动微分进行了优化  impo">
<meta property="og:type" content="article">
<meta property="og:title" content="pytorch基础一">
<meta property="og:url" content="http://example.com/post/cb05edc5.html">
<meta property="og:site_name" content="blog">
<meta property="og:description" content="pytorch基础一、tensortensor是一种专有的数据结构，与数组和矩阵非常相似。在PyTorch中，我们使用tensor来编码一个模型的输入和输出，以及模型的参数。tensor类似于NumPy的ndarrays，只是tensor可以在GPU或其他硬件加速器上运行。事实上，tensor和NumPy数组通常可以共享相同的底层内存，不需要复制数据。tensor还为自动微分进行了优化  impo">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.loli.net/2021/10/24/AjI5GkrsQcpdTNF.jpg">
<meta property="article:published_time" content="2021-10-25T11:29:36.000Z">
<meta property="article:modified_time" content="2021-10-25T15:04:53.289Z">
<meta property="article:author" content="祎熵">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="pytorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2021/10/24/AjI5GkrsQcpdTNF.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/post/cb05edc5"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#121212","position":"top-right"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'pytorch基础一',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-10-25 23:04:53'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/mycss.css"><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="blog" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/touxiang.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">15</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">8</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/timeline/"><i class="fa-fw fa fa-bell"></i><span> 日志</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://i.loli.net/2021/10/24/AjI5GkrsQcpdTNF.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">blog</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/timeline/"><i class="fa-fw fa fa-bell"></i><span> 日志</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">pytorch基础一</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-10-25T11:29:36.000Z" title="发表于 2021-10-25 19:29:36">2021-10-25</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-10-25T15:04:53.289Z" title="更新于 2021-10-25 23:04:53">2021-10-25</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/pytorch/">pytorch</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="pytorch基础一"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="pytorch基础"><a href="#pytorch基础" class="headerlink" title="pytorch基础"></a>pytorch基础</h1><h2 id="一、tensor"><a href="#一、tensor" class="headerlink" title="一、tensor"></a>一、tensor</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor是一种专有的数据结构，与数组和矩阵非常相似。在PyTorch中，我们使用tensor来编码一个模型的输入和输出，以及模型的参数。</span><br><span class="line">tensor类似于NumPy的ndarrays，只是tensor可以在GPU或其他硬件加速器上运行。事实上，tensor和NumPy数组通常可以共享相同的底层内存，不需要复制数据。tensor还为自动微分进行了优化</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import torch</span><br><span class="line">import numpy as np</span><br></pre></td></tr></table></figure>

<p>tensor可以通过各种方式进行初始化。</p>
<h3 id="1、初始化tensor"><a href="#1、初始化tensor" class="headerlink" title="1、初始化tensor"></a>1、初始化tensor</h3><h4 id="1-1、直接初始化"><a href="#1-1、直接初始化" class="headerlink" title="1.1、直接初始化"></a>1.1、直接初始化</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor可以直接从数据中创建。数据类型是自动推断出来的。</span><br><span class="line">data = [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]]</span><br><span class="line">x_data = torch.tensor(data)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[1, 2],</span></span><br><span class="line"><span class="string">        [3, 4]])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>



<h4 id="1-2、从Numpy数组中初始化"><a href="#1-2、从Numpy数组中初始化" class="headerlink" title="1.2、从Numpy数组中初始化"></a>1.2、从Numpy数组中初始化</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">可以从Numpy中创建tensor</span><br><span class="line">np_array = np.array(data)</span><br><span class="line">x_np = torch.from_numpy(np_array)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot; </span></span><br><span class="line"><span class="string">tensor([[1, 2],</span></span><br><span class="line"><span class="string">        [3, 4]], dtype=torch.int32)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>



<h4 id="1-3、从其他tensor中初始化"><a href="#1-3、从其他tensor中初始化" class="headerlink" title="1.3、从其他tensor中初始化"></a>1.3、从其他tensor中初始化</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">新的tensor保留了参数tensor的属性（形状、数据类型），除非明确重写。</span><br><span class="line">x_ones = torch.ones_like(x_data) <span class="comment"># retains the properties of x_data</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Ones Tensor: \n <span class="subst">&#123;x_ones&#125;</span> \n&quot;</span>)</span><br><span class="line">x_rand = torch.rand_like(x_data, dtype=torch.<span class="built_in">float</span>) <span class="comment"># overrides the datatype of x_data</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Random Tensor: \n <span class="subst">&#123;x_rand&#125;</span> \n&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot; </span></span><br><span class="line"><span class="string">Ones Tensor: </span></span><br><span class="line"><span class="string"> tensor([[1, 1],</span></span><br><span class="line"><span class="string">        [1, 1]]) </span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">Random Tensor: </span></span><br><span class="line"><span class="string"> tensor([[0.4452, 0.7225],</span></span><br><span class="line"><span class="string">        [0.6876, 0.3488]]) </span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>



<h4 id="1-4、使用随机数和常数初始化"><a href="#1-4、使用随机数和常数初始化" class="headerlink" title="1.4、使用随机数和常数初始化"></a>1.4、使用随机数和常数初始化</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">shape是一个tensor的元组。在下面的代码中，它决定了输出tensor的维度。</span><br><span class="line">shape = (<span class="number">2</span>,<span class="number">3</span>,)</span><br><span class="line">rand_tensor = torch.rand(shape)</span><br><span class="line">ones_tensor = torch.ones(shape)</span><br><span class="line">zeros_tensor = torch.zeros(shape)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Random Tensor: \n <span class="subst">&#123;rand_tensor&#125;</span> \n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Ones Tensor: \n <span class="subst">&#123;ones_tensor&#125;</span> \n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Zeros Tensor: \n <span class="subst">&#123;zeros_tensor&#125;</span>&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Random Tensor: </span></span><br><span class="line"><span class="string"> tensor([[0.5200, 0.8270, 0.3728],</span></span><br><span class="line"><span class="string">        [0.7114, 0.4883, 0.0574]]) </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Ones Tensor: </span></span><br><span class="line"><span class="string"> tensor([[1., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 1.]]) </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Zeros Tensor: </span></span><br><span class="line"><span class="string"> tensor([[0., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 0., 0.]])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>



<h3 id="2、tensor的性质"><a href="#2、tensor的性质" class="headerlink" title="2、tensor的性质"></a>2、tensor的性质</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tensor属性描述了它们的形状、数据类型以及存储它们的设备。</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor = torch.rand(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Shape of tensor: <span class="subst">&#123;tensor.shape&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Datatype of tensor: <span class="subst">&#123;tensor.dtype&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Device tensor is stored on: <span class="subst">&#123;tensor.device&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Shape of tensor: torch.Size([3, 4])</span></span><br><span class="line"><span class="string">Datatype of tensor: torch.float32</span></span><br><span class="line"><span class="string">Device tensor is stored on: cpu</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<h3 id="3、-tensor的操作"><a href="#3、-tensor的操作" class="headerlink" title="3、 tensor的操作"></a>3、 tensor的操作</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tensor操作，包括算术、线性代数、矩阵操作（转置、索引、切片）、采样等。</span><br><span class="line">默认情况下，tensor是在CPU上创建的。我们需要使用.to方法明确地将tensor移动到GPU上</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 如果有的话，我们把我们的tensor移到GPU上</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    tensor = tensor.to(<span class="string">&#x27;cuda&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h4 id="3-1、索引和切分"><a href="#3-1、索引和切分" class="headerlink" title="3.1、索引和切分"></a>3.1、索引和切分</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor = torch.ones(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;第一行: &#x27;</span>, tensor[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;第一列：&#x27;</span>, tensor[:, <span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;最后一列：&#x27;</span>, tensor[..., -<span class="number">1</span>])</span><br><span class="line">tensor[:,<span class="number">1</span>] = <span class="number">0</span></span><br><span class="line"><span class="built_in">print</span>(tensor)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">第一行:  tensor([1., 1., 1., 1.])</span></span><br><span class="line"><span class="string">第一列： tensor([1., 1., 1., 1.])</span></span><br><span class="line"><span class="string">最后一列： tensor([1., 1., 1., 1.])</span></span><br><span class="line"><span class="string">tensor([[1., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 0., 1., 1.]])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>



<h4 id="3-2、tensor拼接"><a href="#3-2、tensor拼接" class="headerlink" title="3.2、tensor拼接"></a>3.2、tensor拼接</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t1 = torch.cat([tensor, tensor, tensor], dim=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(t1)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>



<h4 id="3-3、算术运算"><a href="#3-3、算术运算" class="headerlink" title="3.3、算术运算"></a>3.3、算术运算</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 这将计算两个tensor之间的矩阵乘法，y1, y2, y3将有相同的值</span></span><br><span class="line">y1 = tensor @ tensor.T</span><br><span class="line">y2 = tensor.matmul(tensor.T)</span><br><span class="line"></span><br><span class="line">y3 = torch.rand_like(tensor)</span><br><span class="line">torch.matmul(tensor, tensor.T, out=y3)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[3., 3., 3., 3.],</span></span><br><span class="line"><span class="string">        [3., 3., 3., 3.],</span></span><br><span class="line"><span class="string">        [3., 3., 3., 3.],</span></span><br><span class="line"><span class="string">        [3., 3., 3., 3.]])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 这将计算出元素相乘的结果。z1，z2, z3有相同的值</span></span><br><span class="line">z1 = tensor * tensor</span><br><span class="line">z2 = tensor.mul(tensor)</span><br><span class="line"></span><br><span class="line">z3 = torch.rand_like(tensor)</span><br><span class="line">torch.mul(tensor, tensor, out=z3)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[1., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 0., 1., 1.]])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<p><strong>单元素tensor</strong>:如果你有一个单元素tensor，例如将一个tensor的所有值总计成一个值，你可以使用item()将其变换为Python数值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">agg = tensor.<span class="built_in">sum</span>()</span><br><span class="line">agg_item = agg.item()</span><br><span class="line"><span class="built_in">print</span>(agg_item, <span class="built_in">type</span>(agg_item))</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">12.0 &lt;class &#x27;float&#x27;&gt;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<p><strong>原位操作</strong>将结果存储到操作数中的操作被称为原位操作。它们用后缀<em>来表示。例如：x.copy</em>(y), x.t_(), 将改变x。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(tensor, <span class="string">&quot;\n&quot;</span>)</span><br><span class="line">tensor.add_(<span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(tensor)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[1., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 0., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 0., 1., 1.]]) </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">tensor([[6., 5., 6., 6.],</span></span><br><span class="line"><span class="string">        [6., 5., 6., 6.],</span></span><br><span class="line"><span class="string">        [6., 5., 6., 6.],</span></span><br><span class="line"><span class="string">        [6., 5., 6., 6.]])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<h3 id="4、和Numpy转换"><a href="#4、和Numpy转换" class="headerlink" title="4、和Numpy转换"></a>4、和Numpy转换</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CPU上的张量和NumPy数组可以共享它们的底层内存位置，改变一个将改变另一个。</span><br></pre></td></tr></table></figure>

<h4 id="4-1、tensor转换为NumPy数组"><a href="#4-1、tensor转换为NumPy数组" class="headerlink" title="4.1、tensor转换为NumPy数组"></a>4.1、tensor转换为NumPy数组</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t = torch.ones(<span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;t: <span class="subst">&#123;t&#125;</span>&quot;</span>)</span><br><span class="line">n = t.numpy()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;n: <span class="subst">&#123;n&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">t: tensor([1., 1., 1., 1., 1.])</span></span><br><span class="line"><span class="string">n: [1. 1. 1. 1. 1.]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<p>tensor的变化反映在NumPy数组中。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t.add_(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;t: <span class="subst">&#123;t&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;n: <span class="subst">&#123;n&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">t: tensor([2., 2., 2., 2., 2.])</span></span><br><span class="line"><span class="string">n: [2. 2. 2. 2. 2.]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<h4 id="4-2、Numpy数组转换为tensor"><a href="#4-2、Numpy数组转换为tensor" class="headerlink" title="4.2、Numpy数组转换为tensor"></a>4.2、Numpy数组转换为tensor</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">n = np.ones(5)</span><br><span class="line">t = torch.from_numpy(n)</span><br><span class="line">np.add(n, 1, out=n)</span><br><span class="line">print(f&quot;t: &#123;t&#125;&quot;)</span><br><span class="line">print(f&quot;n: &#123;n&#125;&quot;)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)</span></span><br><span class="line"><span class="string">n: [2. 2. 2. 2. 2.]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>





<h2 id="二、数据集和数据载入器"><a href="#二、数据集和数据载入器" class="headerlink" title="二、数据集和数据载入器"></a>二、数据集和数据载入器</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PyTorch提供了两个数据模块：torch.utils.data.DataLoader和torch.utils.data.Dataset，允许你使用预先加载的数据集以及你自己的数据。Dataset存储了样本及其相应的标签，而DataLoader对Dataset包裹了一个可迭代的数据集，以便能够方便地访问这些样本。</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PyTorch库提供了一些预加载的数据集（如FashionMNIST），这些数据集是torch.utils.data.Dataset的子类，并实现了针对特定数据的功能。它们可以用来为你的模型建立原型和基准。</span><br></pre></td></tr></table></figure>

<h3 id="1、加载数据集"><a href="#1、加载数据集" class="headerlink" title="1、加载数据集"></a>1、加载数据集</h3><p>从TorchVision加载Fashion-MNIST数据集的例子</p>
<p>Fashion-MNIST是一个由60,000个训练实例和10,000个测试实例组成的Zalando杂志中的图像数据集。每个例子包括一个28×28的灰度图像和这个图像的标签，标签是10类中的一个类别。</p>
<p>我们用以下参数加载FashionMNIST数据集。</p>
<ul>
<li>root是存储训练/测试数据的路径。</li>
<li>train指定训练或测试数据集。</li>
<li>download=True如果root目录下没有数据，则从互联网上下载数据。</li>
<li>transform和target_transform指定特征和标签的变换。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> ToTensor</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">training_data = datasets.FashionMNIST( root=<span class="string">&quot;data&quot;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=ToTensor() )</span><br><span class="line"></span><br><span class="line">test_data = datasets.FashionMNIST( root=<span class="string">&quot;data&quot;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=ToTensor() )</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.loli.net/2021/10/25/xwHnTrWKPsgJ82d.png" alt="image-20211025204306243"></p>
<h3 id="2、数据集的迭代和可视化"><a href="#2、数据集的迭代和可视化" class="headerlink" title="2、数据集的迭代和可视化"></a>2、数据集的迭代和可视化</h3><p>我们可以像列表一样手动索引数据集：<code>training_data[index]</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">labels_map = &#123;</span><br><span class="line">    <span class="number">0</span>: <span class="string">&quot;T-Shirt&quot;</span>,</span><br><span class="line">    <span class="number">1</span>: <span class="string">&quot;Trouser&quot;</span>,</span><br><span class="line">    <span class="number">2</span>: <span class="string">&quot;Pullover&quot;</span>,</span><br><span class="line">    <span class="number">3</span>: <span class="string">&quot;Dress&quot;</span>,</span><br><span class="line">    <span class="number">4</span>: <span class="string">&quot;Coat&quot;</span>,</span><br><span class="line">    <span class="number">5</span>: <span class="string">&quot;Sandal&quot;</span>,</span><br><span class="line">    <span class="number">6</span>: <span class="string">&quot;Shirt&quot;</span>,</span><br><span class="line">    <span class="number">7</span>: <span class="string">&quot;Sneaker&quot;</span>,</span><br><span class="line">    <span class="number">8</span>: <span class="string">&quot;Bag&quot;</span>,</span><br><span class="line">    <span class="number">9</span>: <span class="string">&quot;Ankle Boot&quot;</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">figure = plt.figure(figsize=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line">cols, rows = <span class="number">3</span>, <span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, cols * rows + <span class="number">1</span>):</span><br><span class="line">    sample_idx = torch.randint(<span class="built_in">len</span>(training_data), size=(<span class="number">1</span>,)).item()</span><br><span class="line">    img, label = training_data[sample_idx]</span><br><span class="line">    figure.add_subplot(rows, cols, i)</span><br><span class="line">    plt.title(labels_map[label])</span><br><span class="line">    plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line">    plt.imshow(img.squeeze(), cmap=<span class="string">&quot;gray&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.loli.net/2021/10/25/6AIMenYh7UqzX4t.png" alt="image-20211025204330890"></p>
<h3 id="3、为你的文件创建一个自定义数据集"><a href="#3、为你的文件创建一个自定义数据集" class="headerlink" title="3、为你的文件创建一个自定义数据集"></a>3、为你的文件创建一个自定义数据集</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">一个自定义数据集类必须实现三个函数：__init__, __len__, 和 __getitem__。</span><br></pre></td></tr></table></figure>

<p>FashionMNIST的图片被存储在一个目录img_dir中，它们的标签被分别存储在一个CSV文件annotations_file中。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> torchvision.io <span class="keyword">import</span> read_image</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomImageDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, annotations_file, img_dir, transform=<span class="literal">None</span>, target_transform=<span class="literal">None</span></span>):</span></span><br><span class="line">        self.img_labels = pd.read_csv(annotations_file)</span><br><span class="line">        self.img_dir = img_dir</span><br><span class="line">        self.transform = transform</span><br><span class="line">        self.target_transform = target_transform</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.img_labels)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, idx</span>):</span></span><br><span class="line">        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, <span class="number">0</span>])</span><br><span class="line">        image = read_image(img_path)</span><br><span class="line">        label = self.img_labels.iloc[idx, <span class="number">1</span>]</span><br><span class="line">        <span class="keyword">if</span> self.transform:</span><br><span class="line">            image = self.transform(image)</span><br><span class="line">        <span class="keyword">if</span> self.target_transform:</span><br><span class="line">            label = self.target_transform(label)</span><br><span class="line">        <span class="keyword">return</span> image, label</span><br></pre></td></tr></table></figure>

<h4 id="3-1、init"><a href="#3-1、init" class="headerlink" title="3.1、init"></a>3.1、init</h4><p>在实例化数据集对象时，__init__函数运行一次。我们初始化包含图像目录、标注文件目录和变换（在下一节有更详细的介绍）。</p>
<p>labels.csv文件:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tshirt1.jpg, <span class="number">0</span></span><br><span class="line">tshirt2.jpg, <span class="number">0</span></span><br><span class="line">......</span><br><span class="line">ankleboot999.jpg, <span class="number">9</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, annotations_file, img_dir, transform=<span class="literal">None</span>, target_transform=<span class="literal">None</span></span>):</span></span><br><span class="line">    self.img_labels = pd.read_csv(annotations_file)</span><br><span class="line">    self.img_dir = img_dir</span><br><span class="line">    self.transform = transform</span><br><span class="line">    self.target_transform = target_transform</span><br></pre></td></tr></table></figure>

<h4 id="3-2、len"><a href="#3-2、len" class="headerlink" title="3.2、len"></a>3.2、len</h4><p>函数 <code>__len__</code> 返回我们数据集中的样本数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">len</span>(self.img_labels)</span><br></pre></td></tr></table></figure>

<h4 id="3-3、getitem"><a href="#3-3、getitem" class="headerlink" title="3.3、getitem"></a>3.3、getitem</h4><p>函数 <code>__getitem__</code> 在给定的索引idx处加载并返回数据集中的一个样本。基于索引，它确定图像在磁盘上的位置，使用read_image将其变换为tensor，从self.img_labels中的csv数据中获取相应的标签，对其调用变换函数（如果适用），并在一个元组中返回tensor图像和相应标签。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, idx</span>):</span></span><br><span class="line">    img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, <span class="number">0</span>])</span><br><span class="line">    image = read_image(img_path)</span><br><span class="line">    label = self.img_labels.iloc[idx, <span class="number">1</span>]</span><br><span class="line">    <span class="keyword">if</span> self.transform:</span><br><span class="line">        image = self.transform(image)</span><br><span class="line">    <span class="keyword">if</span> self.target_transform:</span><br><span class="line">        label = self.target_transform(label)</span><br><span class="line">    <span class="keyword">return</span> image, label</span><br></pre></td></tr></table></figure>

<h3 id="4、用DataLoaders准备你的数据进行训练"><a href="#4、用DataLoaders准备你的数据进行训练" class="headerlink" title="4、用DataLoaders准备你的数据进行训练"></a>4、用DataLoaders准备你的数据进行训练</h3><p>数据集每次都会检索我们的数据集的特征和标签。在训练模型时，我们通常希望以 “小批 “的形式传递样本，在每个epoch中重新洗牌以减少模型的过拟合，并使用Python的multiprocessing来加快数据的检索速度。</p>
<p>DataLoader是一个可迭代的对象，它用一个简单的API为我们抽象了这种复杂性。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">train_dataloader = DataLoader(training_data, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_dataloader = DataLoader(test_data, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h3 id="5、通过DataLoader进行迭代"><a href="#5、通过DataLoader进行迭代" class="headerlink" title="5、通过DataLoader进行迭代"></a>5、通过DataLoader进行迭代</h3><p>我们已经将该数据集加载到DataLoader中，可以根据需要迭代数据集。下面的每次迭代都会返回一批train_features和train_labels（分别包含batch_size=64的特征和标签）。因为我们指定了shuffle=True，所以在我们迭代完所有的批次后，数据会被洗牌。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Display image and label.</span></span><br><span class="line">train_features, train_labels = <span class="built_in">next</span>(<span class="built_in">iter</span>(train_dataloader))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Feature batch shape: <span class="subst">&#123;train_features.size()&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Labels batch shape: <span class="subst">&#123;train_labels.size()&#125;</span>&quot;</span>)</span><br><span class="line">img = train_features[<span class="number">0</span>].squeeze()</span><br><span class="line">label = train_labels[<span class="number">0</span>]</span><br><span class="line">plt.imshow(img, cmap=<span class="string">&quot;gray&quot;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Label: <span class="subst">&#123;label&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.loli.net/2021/10/25/iQXM9DvkZs6HU1c.png" alt="image-20211025210207793.png"></p>
<h2 id="三、变换"><a href="#三、变换" class="headerlink" title="三、变换"></a>三、变换</h2><p>数据并不总是以训练机器学习算法所需的最终处理形式出现。我们使用变换来对数据进行一些操作，使其适合训练。</p>
<p>所有的TorchVision数据集都有两个参数–用于修改特征的transform和用于修改标签的target_transform–它们接受包含变换逻辑的调用语句。torchvision.transforms模块提供了几个常用的变换，开箱即用。</p>
<p>FashionMNIST的特征是PIL图像格式的，标签是整数。对于训练，我们需要将特征作为归一化的tensor，将标签作为one-hot编码的tensor。为了进行这些变换，我们使用ToTensor和Lambda。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> ToTensor, Lambda</span><br><span class="line"></span><br><span class="line">ds = datasets.FashionMNIST(</span><br><span class="line">    root=<span class="string">&quot;data&quot;</span>,</span><br><span class="line">    train=<span class="literal">True</span>,</span><br><span class="line">    download=<span class="literal">True</span>,</span><br><span class="line">    transform=ToTensor(),</span><br><span class="line">    target_transform=Lambda(<span class="keyword">lambda</span> y: torch.zeros(<span class="number">10</span>, dtype=torch.<span class="built_in">float</span>).scatter_(<span class="number">0</span>, torch.tensor(y), value=<span class="number">1</span>))</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p><strong>ToTensor()</strong></p>
<p>ToTensor将PIL图像或NumPy的ndarray变换为FloatTensor，并将图像的像素亮度值按[0., 1.]的范围进行缩放。</p>
<p><strong>Lambda变换</strong></p>
<p>Lambda变换应用任何用户定义的Lambda函数。在这里，我们定义了一个函数，把整数变成一个one-hot的tensor。它首先创建一个大小为10（我们数据集中的标签数量），值为0的tensor，并调用scatter_，在标签y给出的索引上分配一个value=1。</p>
<h2 id="四、搭建神经网络"><a href="#四、搭建神经网络" class="headerlink" title="四、搭建神经网络"></a>四、搭建神经网络</h2><p>神经网络由对数据进行操作的层/模块组成。torch.nn命名空间提供了您构建自己的神经网络所需的所有组件。PyTorch中的每个模块都是nn.Module的子类。一个神经网络本身就是一个由其他模块（层）组成的模块。这种嵌套结构允许轻松构建和管理复杂的架构。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br></pre></td></tr></table></figure>

<h3 id="1、获取训练的设备"><a href="#1、获取训练的设备" class="headerlink" title="1、获取训练的设备"></a>1、获取训练的设备</h3><p>我们希望能够在像GPU这样的硬件加速器上训练我们的模型，如果它是可用的。让我们检查一下torch.cuda是否可用，否则我们继续使用CPU。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">device = <span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Using &#123;&#125; device&#x27;</span>.<span class="built_in">format</span>(device))</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Using cuda device</span><br></pre></td></tr></table></figure>

<h3 id="2、定义模型类"><a href="#2、定义模型类" class="headerlink" title="2、定义模型类"></a>2、定义模型类</h3><p>我们通过子类化 nn.Module 来定义我们的神经网络，并在 <code>__init__</code>中初始化神经网络层。每个 nn.Module 子类都在 forward 方法中实现了对输入数据的操作。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NeuralNetwork</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(NeuralNetwork, self).__init__()</span><br><span class="line">        self.flatten = nn.Flatten()</span><br><span class="line">        self.linear_relu_stack = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">28</span>*<span class="number">28</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, <span class="number">10</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        logits = self.linear_relu_stack(x)</span><br><span class="line">        <span class="keyword">return</span> logits</span><br></pre></td></tr></table></figure>

<p>我们创建一个NeuralNetwork的实例，并将其移动到设备上，并打印其结构。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = NeuralNetwork().to(device)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">NeuralNetwork(</span></span><br><span class="line"><span class="string">  (flatten): Flatten(start_dim=1, end_dim=-1)</span></span><br><span class="line"><span class="string">  (linear_relu_stack): Sequential(</span></span><br><span class="line"><span class="string">    (0): Linear(in_features=784, out_features=512, bias=True)</span></span><br><span class="line"><span class="string">    (1): ReLU()</span></span><br><span class="line"><span class="string">    (2): Linear(in_features=512, out_features=512, bias=True)</span></span><br><span class="line"><span class="string">    (3): ReLU()</span></span><br><span class="line"><span class="string">    (4): Linear(in_features=512, out_features=10, bias=True)</span></span><br><span class="line"><span class="string">  )</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<p>为了使用这个模型，我们把输入数据传给它。这就执行了模型的forward函数，以及一些后台操作。</p>
<p>在输入数据上调用模型会返回一个10维的tensor，其中包含每个类别的原始预测值。我们通过nn.Softmax模块的一个实例来获得预测概率。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.rand(<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>, device=device)</span><br><span class="line">logits = model(X)</span><br><span class="line">pred_probab = nn.Softmax(dim=<span class="number">1</span>)(logits)</span><br><span class="line">y_pred = pred_probab.argmax(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Predicted class: <span class="subst">&#123;y_pred&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Predicted class: tensor([1], device=&#x27;cuda:0&#x27;)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<h3 id="3、模型层"><a href="#3、模型层" class="headerlink" title="3、模型层"></a>3、模型层</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">input_image = torch.rand(<span class="number">3</span>,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line"><span class="built_in">print</span>(input_image.size())</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">torch.Size([3, 28, 28])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<p><strong>nn.Flatten</strong></p>
<p>我们初始化nn.Flatten层，将每个28x28的二维图像变换为784个像素值的连续数组（minibatch的维度（dim=0）被保持）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">flatten = nn.Flatten()</span><br><span class="line">flat_image = flatten(input_image)</span><br><span class="line"><span class="built_in">print</span>(flat_image.size())</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">torch.Size([3, 784])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<p><strong>nn.Linear</strong></p>
<p>线性层是一个模块，使用其存储的权重和偏置对输入进行线性变换。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">layer1 = nn.Linear(in_features=<span class="number">28</span>*<span class="number">28</span>, out_features=<span class="number">20</span>)</span><br><span class="line">hidden1 = layer1(flat_image)</span><br><span class="line"><span class="built_in">print</span>(hidden1.size())</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">torch.Size([3, 20])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<p><strong>nn.ReLU</strong></p>
<p>非线性激活是在模型的输入和输出之间建立复杂的映射关系。它们被应用在线性变换之后，以引入非线性，帮助神经网络学习各种各样的函数。</p>
<p>在这个模型中，我们在线性层之间使用了nn.ReLU，但还有其他激活函数可以在你的模型中引入非线性。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Before ReLU: <span class="subst">&#123;hidden1&#125;</span>\n\n&quot;</span>)</span><br><span class="line">hidden1 = nn.ReLU()(hidden1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;After ReLU: <span class="subst">&#123;hidden1&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Before ReLU: tensor([[-0.2050, -0.2515, -0.1101, -0.5910,  0.0241, -0.4991, -0.0064,  0.2330,</span></span><br><span class="line"><span class="string">          0.2104, -0.2930, -0.4654, -0.6682,  0.0789,  0.2525,  0.3306, -0.4441,</span></span><br><span class="line"><span class="string">         -0.1403,  0.2946,  0.2446, -0.6398],</span></span><br><span class="line"><span class="string">        [-0.2331, -0.3806, -0.2077, -0.7201, -0.2562, -0.4168, -0.0570,  0.0775,</span></span><br><span class="line"><span class="string">          0.1734, -0.0644, -0.2212, -0.4178, -0.1430,  0.3815,  0.3207, -0.4322,</span></span><br><span class="line"><span class="string">         -0.2514, -0.0818,  0.0162, -0.7603],</span></span><br><span class="line"><span class="string">        [ 0.1153, -0.3066,  0.6950, -0.4477,  0.0225, -0.3306,  0.2582, -0.0583,</span></span><br><span class="line"><span class="string">          0.3550, -0.1699, -0.5302, -0.6426, -0.3060,  0.2715,  0.0820, -0.2693,</span></span><br><span class="line"><span class="string">         -0.3574,  0.1241,  0.3639, -0.9418]], grad_fn=&lt;AddmmBackward&gt;)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">After ReLU: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0241, 0.0000, 0.0000, 0.2330, 0.2104,</span></span><br><span class="line"><span class="string">         0.0000, 0.0000, 0.0000, 0.0789, 0.2525, 0.3306, 0.0000, 0.0000, 0.2946,</span></span><br><span class="line"><span class="string">         0.2446, 0.0000],</span></span><br><span class="line"><span class="string">        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0775, 0.1734,</span></span><br><span class="line"><span class="string">         0.0000, 0.0000, 0.0000, 0.0000, 0.3815, 0.3207, 0.0000, 0.0000, 0.0000,</span></span><br><span class="line"><span class="string">         0.0162, 0.0000],</span></span><br><span class="line"><span class="string">        [0.1153, 0.0000, 0.6950, 0.0000, 0.0225, 0.0000, 0.2582, 0.0000, 0.3550,</span></span><br><span class="line"><span class="string">         0.0000, 0.0000, 0.0000, 0.0000, 0.2715, 0.0820, 0.0000, 0.0000, 0.1241,</span></span><br><span class="line"><span class="string">         0.3639, 0.0000]], grad_fn=&lt;ReluBackward0&gt;)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<p><strong>nn.Sequential</strong></p>
<p>nn.Sequential是一个有序模块的容器。数据以定义的顺序通过所有的模块。你可以使用 序列容器来组建一个快速的网络，如seq_modules：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">seq_modules = nn.Sequential(</span><br><span class="line">    flatten,</span><br><span class="line">    layer1,</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">20</span>, <span class="number">10</span>)</span><br><span class="line">)</span><br><span class="line">input_image = torch.rand(<span class="number">3</span>,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line">logits = seq_modules(input_image)</span><br></pre></td></tr></table></figure>



<p><strong>nn.Softmax</strong></p>
<p>神经网络的最后一个线性层返回logits–[-infty, infty]中的原始值–并传递给nn.Softmax模块。对数被缩放到数值区间[0, 1]，代表模型对每个类别的预测概率。 dim参数表示数值必须和为1的维度。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">softmax = nn.Softmax(dim=<span class="number">1</span>)</span><br><span class="line">pred_probab = softmax(logits)</span><br></pre></td></tr></table></figure>

<h3 id="4、模型参数"><a href="#4、模型参数" class="headerlink" title="4、模型参数"></a>4、模型参数</h3><p>神经网络中的许多层都是参数化的，也就是说，层相关的权重和偏置在训练中被优化。nn.Module的子类会自动跟踪你的模型对象中定义的所有字段，并使用你的模型的 parameters() 或 named_parameters() 方法访问所有参数。</p>
<p>在这个例子中，我们遍历每个参数，并打印其大小和预览其值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Model structure: &quot;</span>, model, <span class="string">&quot;\n\n&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Layer: <span class="subst">&#123;name&#125;</span> | Size: <span class="subst">&#123;param.size()&#125;</span> | Values : <span class="subst">&#123;param[:<span class="number">2</span>]&#125;</span> \n&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Model structure:  NeuralNetwork(</span><br><span class="line">  (flatten): Flatten(start_dim=<span class="number">1</span>, end_dim=-<span class="number">1</span>)</span><br><span class="line">  (linear_relu_stack): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Linear(in_features=<span class="number">784</span>, out_features=<span class="number">512</span>, bias=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">1</span>): ReLU()</span><br><span class="line">    (<span class="number">2</span>): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">512</span>, bias=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">3</span>): ReLU()</span><br><span class="line">    (<span class="number">4</span>): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">10</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  )</span><br><span class="line">) </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Layer: linear_relu_stack<span class="number">.0</span>.weight | Size: torch.Size([<span class="number">512</span>, <span class="number">784</span>]) | Values : tensor([[-<span class="number">0.0331</span>,  <span class="number">0.0108</span>, -<span class="number">0.0115</span>,  ...,  <span class="number">0.0196</span>,  <span class="number">0.0255</span>,  <span class="number">0.0289</span>],</span><br><span class="line">        [-<span class="number">0.0168</span>,  <span class="number">0.0280</span>, -<span class="number">0.0132</span>,  ..., -<span class="number">0.0103</span>, -<span class="number">0.0099</span>, -<span class="number">0.0121</span>]],</span><br><span class="line">       device=<span class="string">&#x27;cuda:0&#x27;</span>, grad_fn=&lt;SliceBackward&gt;) </span><br><span class="line"></span><br><span class="line">Layer: linear_relu_stack<span class="number">.0</span>.bias | Size: torch.Size([<span class="number">512</span>]) | Values : tensor([-<span class="number">0.0108</span>,  <span class="number">0.0137</span>], device=<span class="string">&#x27;cuda:0&#x27;</span>, grad_fn=&lt;SliceBackward&gt;) </span><br><span class="line"></span><br><span class="line">Layer: linear_relu_stack<span class="number">.2</span>.weight | Size: torch.Size([<span class="number">512</span>, <span class="number">512</span>]) | Values : tensor([[ <span class="number">0.0048</span>,  <span class="number">0.0112</span>,  <span class="number">0.0430</span>,  ..., -<span class="number">0.0423</span>, -<span class="number">0.0438</span>,  <span class="number">0.0150</span>],</span><br><span class="line">        [-<span class="number">0.0213</span>, -<span class="number">0.0016</span>, -<span class="number">0.0128</span>,  ...,  <span class="number">0.0230</span>,  <span class="number">0.0200</span>, -<span class="number">0.0120</span>]],</span><br><span class="line">       device=<span class="string">&#x27;cuda:0&#x27;</span>, grad_fn=&lt;SliceBackward&gt;) </span><br><span class="line"></span><br><span class="line">Layer: linear_relu_stack<span class="number">.2</span>.bias | Size: torch.Size([<span class="number">512</span>]) | Values : tensor([<span class="number">0.0374</span>, <span class="number">0.0331</span>], device=<span class="string">&#x27;cuda:0&#x27;</span>, grad_fn=&lt;SliceBackward&gt;) </span><br><span class="line"></span><br><span class="line">Layer: linear_relu_stack<span class="number">.4</span>.weight | Size: torch.Size([<span class="number">10</span>, <span class="number">512</span>]) | Values : tensor([[ <span class="number">0.0318</span>, -<span class="number">0.0424</span>,  <span class="number">0.0038</span>,  ...,  <span class="number">0.0241</span>, -<span class="number">0.0187</span>, -<span class="number">0.0105</span>],</span><br><span class="line">        [ <span class="number">0.0004</span>,  <span class="number">0.0132</span>,  <span class="number">0.0343</span>,  ..., -<span class="number">0.0182</span>,  <span class="number">0.0013</span>, -<span class="number">0.0020</span>]],</span><br><span class="line">       device=<span class="string">&#x27;cuda:0&#x27;</span>, grad_fn=&lt;SliceBackward&gt;) </span><br><span class="line"></span><br><span class="line">Layer: linear_relu_stack<span class="number">.4</span>.bias | Size: torch.Size([<span class="number">10</span>]) | Values : tensor([<span class="number">0.0109</span>, <span class="number">0.0427</span>], device=<span class="string">&#x27;cuda:0&#x27;</span>, grad_fn=&lt;SliceBackward&gt;) </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="五、使用torch-autograd进行自动微分"><a href="#五、使用torch-autograd进行自动微分" class="headerlink" title="五、使用torch.autograd进行自动微分"></a>五、使用torch.autograd进行自动微分</h2><p>在训练神经网络时，最常使用的算法是反向传播算法。在这种算法中，参数（模型权重）是根据损失函数相对于给定参数的梯度来调整的。</p>
<p>为了计算这些梯度，PyTorch有一个内置的微分引擎，叫做torch.autograd。它支持对任何计算图的梯度进行自动计算。</p>
<p>考虑最简单的单层神经网络，输入x，参数w和b，以及一些损失函数。它可以在PyTorch中以如下方式定义：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.ones(<span class="number">5</span>)  <span class="comment"># input tensor</span></span><br><span class="line">y = torch.zeros(<span class="number">3</span>)  <span class="comment"># expected output</span></span><br><span class="line">w = torch.randn(<span class="number">5</span>, <span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.randn(<span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">z = torch.matmul(x, w)+b</span><br><span class="line">loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)</span><br></pre></td></tr></table></figure>

<h3 id="1、tensor、函数和计算图"><a href="#1、tensor、函数和计算图" class="headerlink" title="1、tensor、函数和计算图"></a>1、tensor、函数和计算图</h3><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.loli.net/2021/10/25/iulTGxK2vWg1V97.png" alt="计算图"></p>
<p>在这个网络中，w和b是参数，我们需要进行优化。因此，我们需要能够计算损失函数相对于这些变量的梯度。为了做到这一点，我们设置了这些tensor的 requires_grad 属性。</p>
<p>注意:你可以在创建tensor时设置requires_grad的值，或者在以后使用x.requires_grad_(True)方法来设置。</p>
<p>我们应用于tensor来构建计算图的函数实际上是一个Function类的对象。这个对象知道如何在forward方向上计算函数，也知道如何在后向传播步骤中计算其导数。对后向传播函数的引用被存储在tensor的grad_fn属性中。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Gradient function for z =&#x27;</span>, z.grad_fn)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Gradient function for loss =&#x27;</span>, loss.grad_fn)</span><br></pre></td></tr></table></figure>

<h3 id="2、计算梯度"><a href="#2、计算梯度" class="headerlink" title="2、计算梯度"></a>2、计算梯度</h3><p>为了优化神经网络中的参数权重，我们需要计算损失函数相对于参数的导数</p>
<p>为了计算这些导数，我们调用 loss.backward()，然后从 w.grad 和 b.grad 中获取数值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loss.backward()</span><br><span class="line"><span class="built_in">print</span>(w.grad)</span><br><span class="line"><span class="built_in">print</span>(b.grad)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[0.3256, 0.2634, 0.1137],</span></span><br><span class="line"><span class="string">        [0.3256, 0.2634, 0.1137],</span></span><br><span class="line"><span class="string">        [0.3256, 0.2634, 0.1137],</span></span><br><span class="line"><span class="string">        [0.3256, 0.2634, 0.1137],</span></span><br><span class="line"><span class="string">        [0.3256, 0.2634, 0.1137]])</span></span><br><span class="line"><span class="string">tensor([0.3256, 0.2634, 0.1137])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<ul>
<li>我们只能获得计算图的叶子节点的grad属性，这些节点的requires_grad属性设置为True。对于我们图中的所有其他节点，梯度将不可用。</li>
<li>出于性能方面的考虑，我们只能在一个给定的图上使用一次backward来进行梯度计算。如果我们需要在同一个图上进行多次backward调用，我们需要在backward调用中传递 retain_graph=True。</li>
</ul>
<h3 id="3、禁止梯度跟踪"><a href="#3、禁止梯度跟踪" class="headerlink" title="3、禁止梯度跟踪"></a>3、禁止梯度跟踪</h3><p>默认情况下，所有带有require_grad=True的tensor都在跟踪它们的计算历史并支持梯度计算。然而，在某些情况下，我们不需要这样做，例如，当我们已经训练了模型，只是想把它应用于一些输入数据，也就是说，我们只想通过网络进行前向计算。我们可以通过用torch.no_grad()块包围我们的计算代码来停止跟踪计算。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">z = torch.matmul(x, w)+b</span><br><span class="line"><span class="built_in">print</span>(z.requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    z = torch.matmul(x, w)+b</span><br><span class="line"><span class="built_in">print</span>(z.requires_grad)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">True</span><br><span class="line">False</span><br></pre></td></tr></table></figure>

<p>另一种实现相同结果的方法是在tensor上使用detach()方法。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">z = torch.matmul(x, w)+b</span><br><span class="line">z_det = z.detach()</span><br><span class="line">print(z_det.requires_grad)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">False</span><br></pre></td></tr></table></figure>



<p>你可能想禁用梯度跟踪，原因如下：</p>
<ul>
<li>将神经网络中的一些参数标记为冻结参数。这是对预训练的网络进行微调的一个非常常见的情况。</li>
<li>当你只做前向传递时，为了加快计算速度，对不跟踪梯度的tensor的计算会更有效率。</li>
</ul>
<h3 id="更多"><a href="#更多" class="headerlink" title="更多"></a>更多</h3><p>从概念上讲，autograd在一个由Function对象组成的有向无环图（DAG）中保存了数据（tensor）和所有执行的操作（以及产生的新tensor）的记录。在这个DAG中，叶子是输入tensor，根部是输出tensor。通过追踪这个图从根到叶，你可以使用链式规则自动计算梯度。</p>
<p>在一个前向传递中，autograd同时做两件事。</p>
<ul>
<li>运行请求的操作，计算出一个结果tensor。</li>
<li>在DAG中维护该操作的梯度函数。</li>
</ul>
<p>当在DAG根上调用.backward()时，后向传递开始了。</p>
<ul>
<li>计算每个.grad_fn的梯度。</li>
<li>将它们累积到各自tensor的 .grad 属性中</li>
<li>使用链式规则，一直传播到叶子tensor。</li>
</ul>
<p>注意:在PyTorch中，DAG是动态的。需要注意的是，图是从头开始重新创建的；在每次调用.backward()后，autograd开始填充一个新的图。这正是允许你在模型中使用控制流语句的原因；如果需要，你可以在每次迭代时改变形状、大小和操作。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line">from torchvision import datasets</span><br><span class="line">from torchvision.transforms import ToTensor, Lambda</span><br><span class="line"></span><br><span class="line">training_data = datasets.FashionMNIST(</span><br><span class="line">    root=&quot;data&quot;,</span><br><span class="line">    train=True,</span><br><span class="line">    download=True,</span><br><span class="line">    transform=ToTensor()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">test_data = datasets.FashionMNIST(</span><br><span class="line">    root=&quot;data&quot;,</span><br><span class="line">    train=False,</span><br><span class="line">    download=True,</span><br><span class="line">    transform=ToTensor()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">train_dataloader = DataLoader(training_data, batch_size=64)</span><br><span class="line">test_dataloader = DataLoader(test_data, batch_size=64)</span><br><span class="line"></span><br><span class="line">class NeuralNetwork(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(NeuralNetwork, self).__init__()</span><br><span class="line">        self.flatten = nn.Flatten()</span><br><span class="line">        self.linear_relu_stack = nn.Sequential(</span><br><span class="line">            nn.Linear(28*28, 512),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(512, 512),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(512, 10),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        logits = self.linear_relu_stack(x)</span><br><span class="line">        return logits</span><br><span class="line"></span><br><span class="line">model = NeuralNetwork()</span><br></pre></td></tr></table></figure>

</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">祎熵</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/post/cb05edc5.html">http://example.com/post/cb05edc5.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/pytorch/">pytorch</a></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2021/10/24/AjI5GkrsQcpdTNF.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/post/a7500519.html"><img class="prev-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.loli.net/2021/10/24/SyHYctCU8TpleuO.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">pytorch基础二</div></div></a></div><div class="next-post pull-right"><a href="/post/a0d5236b.html"><img class="next-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.loli.net/2021/10/24/lytaJWDRzsKPjYQ.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">pytorch深度学习三</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/post/e50c9314.html" title="pytorch基础三"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.loli.net/2021/10/24/ePFMHpcdy47SxJN.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-10-25</div><div class="title">pytorch基础三</div></div></a></div><div><a href="/post/a7500519.html" title="pytorch基础二"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.loli.net/2021/10/24/SyHYctCU8TpleuO.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-10-25</div><div class="title">pytorch基础二</div></div></a></div><div><a href="/post/d9099bcf.html" title="pytorch深度学习一"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.loli.net/2021/10/23/3ZWQCPlgvVaxcEm.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-10-25</div><div class="title">pytorch深度学习一</div></div></a></div><div><a href="/post/a0d5236b.html" title="pytorch深度学习三"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.loli.net/2021/10/24/lytaJWDRzsKPjYQ.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-10-25</div><div class="title">pytorch深度学习三</div></div></a></div><div><a href="/post/e289b566.html" title="pytorch深度学习二"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.loli.net/2021/10/24/m2NKABrGxusckq7.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-10-25</div><div class="title">pytorch深度学习二</div></div></a></div><div><a href="/post/141d1667.html" title="注意力机制"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.loli.net/2021/10/23/3ZWQCPlgvVaxcEm.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-10-24</div><div class="title">注意力机制</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#pytorch%E5%9F%BA%E7%A1%80"><span class="toc-number">1.</span> <span class="toc-text">pytorch基础</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81tensor"><span class="toc-number">1.1.</span> <span class="toc-text">一、tensor</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E3%80%81%E5%88%9D%E5%A7%8B%E5%8C%96tensor"><span class="toc-number">1.1.1.</span> <span class="toc-text">1、初始化tensor</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1%E3%80%81%E7%9B%B4%E6%8E%A5%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">1.1.1.1.</span> <span class="toc-text">1.1、直接初始化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2%E3%80%81%E4%BB%8ENumpy%E6%95%B0%E7%BB%84%E4%B8%AD%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">1.1.1.2.</span> <span class="toc-text">1.2、从Numpy数组中初始化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3%E3%80%81%E4%BB%8E%E5%85%B6%E4%BB%96tensor%E4%B8%AD%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">1.1.1.3.</span> <span class="toc-text">1.3、从其他tensor中初始化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-4%E3%80%81%E4%BD%BF%E7%94%A8%E9%9A%8F%E6%9C%BA%E6%95%B0%E5%92%8C%E5%B8%B8%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">1.1.1.4.</span> <span class="toc-text">1.4、使用随机数和常数初始化</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E3%80%81tensor%E7%9A%84%E6%80%A7%E8%B4%A8"><span class="toc-number">1.1.2.</span> <span class="toc-text">2、tensor的性质</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3%E3%80%81-tensor%E7%9A%84%E6%93%8D%E4%BD%9C"><span class="toc-number">1.1.3.</span> <span class="toc-text">3、 tensor的操作</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1%E3%80%81%E7%B4%A2%E5%BC%95%E5%92%8C%E5%88%87%E5%88%86"><span class="toc-number">1.1.3.1.</span> <span class="toc-text">3.1、索引和切分</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2%E3%80%81tensor%E6%8B%BC%E6%8E%A5"><span class="toc-number">1.1.3.2.</span> <span class="toc-text">3.2、tensor拼接</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3%E3%80%81%E7%AE%97%E6%9C%AF%E8%BF%90%E7%AE%97"><span class="toc-number">1.1.3.3.</span> <span class="toc-text">3.3、算术运算</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4%E3%80%81%E5%92%8CNumpy%E8%BD%AC%E6%8D%A2"><span class="toc-number">1.1.4.</span> <span class="toc-text">4、和Numpy转换</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1%E3%80%81tensor%E8%BD%AC%E6%8D%A2%E4%B8%BANumPy%E6%95%B0%E7%BB%84"><span class="toc-number">1.1.4.1.</span> <span class="toc-text">4.1、tensor转换为NumPy数组</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2%E3%80%81Numpy%E6%95%B0%E7%BB%84%E8%BD%AC%E6%8D%A2%E4%B8%BAtensor"><span class="toc-number">1.1.4.2.</span> <span class="toc-text">4.2、Numpy数组转换为tensor</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E6%95%B0%E6%8D%AE%E8%BD%BD%E5%85%A5%E5%99%A8"><span class="toc-number">1.2.</span> <span class="toc-text">二、数据集和数据载入器</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E3%80%81%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">1.2.1.</span> <span class="toc-text">1、加载数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E3%80%81%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E8%BF%AD%E4%BB%A3%E5%92%8C%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-number">1.2.2.</span> <span class="toc-text">2、数据集的迭代和可视化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3%E3%80%81%E4%B8%BA%E4%BD%A0%E7%9A%84%E6%96%87%E4%BB%B6%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E8%87%AA%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">1.2.3.</span> <span class="toc-text">3、为你的文件创建一个自定义数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1%E3%80%81init"><span class="toc-number">1.2.3.1.</span> <span class="toc-text">3.1、init</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2%E3%80%81len"><span class="toc-number">1.2.3.2.</span> <span class="toc-text">3.2、len</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3%E3%80%81getitem"><span class="toc-number">1.2.3.3.</span> <span class="toc-text">3.3、getitem</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4%E3%80%81%E7%94%A8DataLoaders%E5%87%86%E5%A4%87%E4%BD%A0%E7%9A%84%E6%95%B0%E6%8D%AE%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83"><span class="toc-number">1.2.4.</span> <span class="toc-text">4、用DataLoaders准备你的数据进行训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5%E3%80%81%E9%80%9A%E8%BF%87DataLoader%E8%BF%9B%E8%A1%8C%E8%BF%AD%E4%BB%A3"><span class="toc-number">1.2.5.</span> <span class="toc-text">5、通过DataLoader进行迭代</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E5%8F%98%E6%8D%A2"><span class="toc-number">1.3.</span> <span class="toc-text">三、变换</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E6%90%AD%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.4.</span> <span class="toc-text">四、搭建神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E3%80%81%E8%8E%B7%E5%8F%96%E8%AE%AD%E7%BB%83%E7%9A%84%E8%AE%BE%E5%A4%87"><span class="toc-number">1.4.1.</span> <span class="toc-text">1、获取训练的设备</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E3%80%81%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9E%8B%E7%B1%BB"><span class="toc-number">1.4.2.</span> <span class="toc-text">2、定义模型类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3%E3%80%81%E6%A8%A1%E5%9E%8B%E5%B1%82"><span class="toc-number">1.4.3.</span> <span class="toc-text">3、模型层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4%E3%80%81%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0"><span class="toc-number">1.4.4.</span> <span class="toc-text">4、模型参数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E4%BD%BF%E7%94%A8torch-autograd%E8%BF%9B%E8%A1%8C%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86"><span class="toc-number">1.5.</span> <span class="toc-text">五、使用torch.autograd进行自动微分</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E3%80%81tensor%E3%80%81%E5%87%BD%E6%95%B0%E5%92%8C%E8%AE%A1%E7%AE%97%E5%9B%BE"><span class="toc-number">1.5.1.</span> <span class="toc-text">1、tensor、函数和计算图</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E3%80%81%E8%AE%A1%E7%AE%97%E6%A2%AF%E5%BA%A6"><span class="toc-number">1.5.2.</span> <span class="toc-text">2、计算梯度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3%E3%80%81%E7%A6%81%E6%AD%A2%E6%A2%AF%E5%BA%A6%E8%B7%9F%E8%B8%AA"><span class="toc-number">1.5.3.</span> <span class="toc-text">3、禁止梯度跟踪</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9B%B4%E5%A4%9A"><span class="toc-number">1.5.4.</span> <span class="toc-text">更多</span></a></li></ol></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2021 By 祎熵</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"></div><canvas class="fireworks" mobile="true"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/fireworks.min.js"></script><script defer="defer" id="fluttering_ribbon" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-fluttering-ribbon.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>